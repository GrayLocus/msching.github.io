<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Category: iOS | 码农人生]]></title>
  <link href="http://msching.github.io/blog/categories/ios/atom.xml" rel="self"/>
  <link href="http://msching.github.io/"/>
  <updated>2014-07-12T16:00:22+08:00</updated>
  <id>http://msching.github.io/</id>
  <author>
    <name><![CDATA[ChengYinZju]]></name>
    
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[iOS音频播放 (三)：AudioFileStream]]></title>
    <link href="http://msching.github.io/blog/2014/07/09/audio-in-ios-3/"/>
    <updated>2014-07-09T11:31:28+08:00</updated>
    <id>http://msching.github.io/blog/2014/07/09/audio-in-ios-3</id>
    <content type="html"><![CDATA[<h1>前言</h1>

<p>本来说好是要在第三篇中讲<code>AudioFileStream</code>和<code>AudioQueue</code>，但写着写着发现光<code>AudioFileStream</code>就好多内容，最后还是决定分篇介绍，这篇先来说一下<code>AudioFileStream</code>，下一篇计划说一下和<code>AudioFileStream</code>类似的<code>AudioFile</code>，下下篇再来说<code>AudioQueue</code>。</p>

<p>在本片那种将会提到计算音频时长duration和音频seek的方法，这些方法对于CBR编码形式的音频文件可以做到比较精确而对于VBR编码形式的会存在较大的误差（关于CBR和VBR，请看本系列的<a href="blog/2014/07/07/audio-in-ios/">第一篇</a>），具体讲到duration和seek时会再进行说明。</p>

<hr />

<h1>AudioFileStream介绍</h1>

<p>在<a href="blog/2014/07/07/audio-in-ios/">第一篇</a>中说到<code>AudioFileStreamer</code>时提到它的作用是用来读取采样率、码率、时长等基本信息以及分离音频帧。那么在<a href="https://developer.apple.com/library/ios/documentation/audiovideo/conceptual/multimediapg/usingaudio/usingaudio.html#//apple_ref/doc/uid/TP40009767-CH2-SW28">官方文档</a>中Apple是这样描述的：</p>

<p><code>To play streamed audio content, such as from a network connection, use Audio File Stream Services in concert with Audio Queue Services. Audio File Stream Services parses audio packets and metadata from common audio file container formats in a network bitstream. You can also use it to parse packets and metadata from on-disk files</code></p>

<p>根据Apple的描述<code>AudioFileStreamer</code>用在流播放中，当然不仅限于网络流，本地文件同样可以用它来读取信息和分离音频帧。<code>AudioFileStreamer</code>的主要数据是文件数据而不是文件路径，所以数据的读取需要使用者自行实现，</p>

<p>支持的文件格式有：</p>

<ul>
<li>MPEG-1 Audio Layer 3, used for .mp3 files</li>
<li>MPEG-2 ADTS, used for the .aac audio data format</li>
<li>AIFC</li>
<li>AIFF</li>
<li>CAF</li>
<li>MPEG-4, used for .m4a, .mp4, and .3gp files</li>
<li>NeXT</li>
<li>WAVE</li>
</ul>


<p>上述格式是iOS、MacOSX所支持的音频格式，这类格式可以被系统提供的API解码，如果想要解码其他的音频格式（如OGG、APE、FLAC）就需要自己实现解码器了。</p>

<hr />

<h1>初始化AudioFileStream</h1>

<p>第一步，自然是要生成一个<code>AudioFileStream</code>的实例：
```objc
extern OSStatus AudioFileStreamOpen (void * inClientData,</p>

<pre><code>                                   AudioFileStream_PropertyListenerProc inPropertyListenerProc,
                                   AudioFileStream_PacketsProc inPacketsProc,
                                   AudioFileTypeID inFileTypeHint,
                                   AudioFileStreamID * outAudioFileStream);
</code></pre>

<p>```
第一个参数和之前的AudioSession的初始化方法一样是一个上下文对象；</p>

<p>第二个参数<code>AudioFileStream_PropertyListenerProc</code>是歌曲信息解析的回调，每解析出一个歌曲信息都会进行一次回调；</p>

<p>第三个参数<code>AudioFileStream_PacketsProc</code>是分离帧的回调，每解析出一部分帧就会进行一次回调；</p>

<p>第四个参数<code>AudioFileTypeID</code>是文件类型的提示，这个参数来帮助<code>AudioFileStream</code>对文件格式进行解析。这个参数在文件信息不完整（例如信息有缺陷）时尤其有用，它可以给与<code>AudioFileStream</code>一定的提示，帮助其绕过文件中的错误或者缺失从而成功解析文件。所以在确定文件类型的情况下建议各位还是填上这个参数，如果无法确定可以传入0（原理上应该和<a href="/blog/2014/05/04/secret-of-avaudioplayer/">这篇博文</a>近似）；</p>

<p>```objc
//AudioFileTypeID枚举
enum {</p>

<pre><code>    kAudioFileAIFFType              = 'AIFF',
    kAudioFileAIFCType              = 'AIFC',
    kAudioFileWAVEType              = 'WAVE',
    kAudioFileSoundDesigner2Type    = 'Sd2f',
    kAudioFileNextType              = 'NeXT',
    kAudioFileMP3Type               = 'MPG3',   // mpeg layer 3
    kAudioFileMP2Type               = 'MPG2',   // mpeg layer 2
    kAudioFileMP1Type               = 'MPG1',   // mpeg layer 1
      kAudioFileAC3Type             = 'ac-3',
    kAudioFileAAC_ADTSType          = 'adts',
    kAudioFileMPEG4Type            = 'mp4f',
    kAudioFileM4AType              = 'm4af',
    kAudioFileM4BType              = 'm4bf',
      kAudioFileCAFType             = 'caff',
      kAudioFile3GPType             = '3gpp',
      kAudioFile3GP2Type                = '3gp2',       
      kAudioFileAMRType             = 'amrf'        
</code></pre>

<p>};
```</p>

<p>第五个参数是返回的AudioFileStream实例对应的<code>AudioFileStreamID</code>，这个ID需要保存起来作为后续一些方法的参数使用；</p>

<p>返回值用来判断是否成功初始化（OSSStatus == noErr）。</p>

<hr />

<h1>解析数据</h1>

<p>在初始化完成之后，只要拿到文件数据就可以进行解析了。解析时调用方法：</p>

<p>```objc
extern OSStatus AudioFileStreamParseBytes(AudioFileStreamID inAudioFileStream,</p>

<pre><code>                                        UInt32 inDataByteSize,
                                        const void* inData,
                                        UInt32 inFlags);
</code></pre>

<p><code>``
第一个参数</code>AudioFileStreamID`，即初始化时返回的ID；</p>

<p>第二个参数inDataByteSize，本次解析的数据长度；</p>

<p>第三个参数inData，本次解析的数据；</p>

<p>第四个参数是说本次的解析和上一次解析是否是连续的关系，如果是连续的传入0，否则传入<code>kAudioFileStreamParseFlag_Discontinuity</code>。</p>

<p>这里需要插入解释一下何谓“连续”。在第一篇中我们提到过形如MP3的数据都以帧的形式存在的，解析时也需要以帧为单位解析。但在解码之前我们不可能知道每个帧的边界在第几个字节，所以就会出现这样的情况：我们传给AudioFileStreamParseBytes的数据在解析完成之后会有一部分数据余下来，这部分数据是接下去那一帧的前半部分，如果再次有数据输入需要继续解析时就必须要用到前一次解析余下来的数据才能保证帧数据完整，所以在正常播放的情况下传入0即可。目前知道的需要传入<code>kAudioFileStreamParseFlag_Discontinuity</code>的情况有两个，一个是在<strong>seek完毕之后</strong>显然seek后的数据和之前的数据完全无关；另一个是开源播放器<a href="https://github.com/mattgallagher/AudioStreamer">AudioStreamer</a>的作者@Matt Gallagher曾在自己的<a href="http://www.cocoawithlove.com/2008/09/streaming-and-playing-live-mp3-stream.html">blog</a>中提到过的：</p>

<p><code>the Audio File Stream Services hit me with a nasty bug: AudioFileStreamParseBytes will crash when trying to parse a streaming MP3.</code></p>

<p><code>In this case, if we pass the kAudioFileStreamParseFlag_Discontinuity flag to AudioFileStreamParseBytes on every invocation between receiving kAudioFileStreamProperty_ReadyToProducePackets and the first successful call to MyPacketsProc, then AudioFileStreamParseBytes will be extra cautious in its approach and won't crash.</code></p>

<p>Matt发布这篇blog是在2008年，这个Bug年代相当久远了，而且原因未知，究竟是否修复也不得而知，而且由于环境不同（比如测试用的mp3文件和所处的iOS系统）无法重现这个问题，所以我个人觉得还是按照Matt的work around在回调得到<code>kAudioFileStreamProperty_ReadyToProducePackets</code>之后，在正常解析第一帧之前都传入<code>kAudioFileStreamParseFlag_Discontinuity</code>比较好。</p>

<p>回到之前的内容，<code>AudioFileStreamParseBytes</code>方法的返回值表示当前的数据是否被正常解析，如果OSStatus的值不是noErr则表示解析不成功，其中错误码包括：</p>

<p>```objc
enum
{</p>

<pre><code>kAudioFileStreamError_UnsupportedFileType       = 'typ?',
kAudioFileStreamError_UnsupportedDataFormat     = 'fmt?',
kAudioFileStreamError_UnsupportedProperty       = 'pty?',
kAudioFileStreamError_BadPropertySize           = '!siz',
kAudioFileStreamError_NotOptimized              = 'optm',
kAudioFileStreamError_InvalidPacketOffset       = 'pck?',
kAudioFileStreamError_InvalidFile               = 'dta?',
kAudioFileStreamError_ValueUnknown              = 'unk?',
kAudioFileStreamError_DataUnavailable           = 'more',
kAudioFileStreamError_IllegalOperation          = 'nope',
kAudioFileStreamError_UnspecifiedError          = 'wht?',
kAudioFileStreamError_DiscontinuityCantRecover  = 'dsc!'
</code></pre>

<p>};</p>

<p><code>``
大多数都可以从字面上理解，需要提一下的是</code>kAudioFileStreamError_NotOptimized`，文档上是这么说的：</p>

<p><code>It is not possible to produce output packets because the file's packet table or other defining info is either not present or is after the audio data.</code></p>

<p>它的含义是说这个音频文件的文件头不存在或者说文件头可能在文件的末尾，当前无法正常Parse，换句话说就是这个文件需要全部下载完才能播放，无法流播。</p>

<p><strong>注意<code>AudioFileStreamParseBytes</code>方法每一次调用都应该注意返回值，一旦出现错误就可以不必继续Parse了。</strong></p>

<hr />

<h1>解析文件格式信息</h1>

<p>在调用<code>AudioFileStreamParseBytes</code>方法进行解析时会首先读取格式信息，并同步的进入<code>AudioFileStream_PropertyListenerProc</code>回调方法</p>

<p><img src="/images/iOS-audio/audiofilestreamParse-1.jpg" alt="" /></p>

<p>来看一下这个回调方法的定义
```objc
typedef void (*AudioFileStream_PropertyListenerProc)(void * inClientData,</p>

<pre><code>                                                   AudioFileStreamID inAudioFileStream,
                                                   AudioFileStreamPropertyID inPropertyID,
                                                   UInt32 * ioFlags);
</code></pre>

<p>```</p>

<p>回调的第一个参数是Open方法中的上下文对象；</p>

<p>第二个参数inAudioFileStream是和Open方法中第四个返回参数<code>AudioFileStreamID</code>一样，表示当前FileStream的ID；</p>

<p>第三个参数是此次回调解析的信息ID。表示当前PropertyID对应的信息已经解析完成信息（例如数据格式、音频数据的偏移量等等），使用者可以通过<code>AudioFileStreamGetProperty</code>接口获取PropertyID对应的值或者数据结构；</p>

<p>```objc
extern OSStatus AudioFileStreamGetProperty(AudioFileStreamID inAudioFileStream,</p>

<pre><code>                                         AudioFileStreamPropertyID inPropertyID,
                                         UInt32 * ioPropertyDataSize,
                                         void * outPropertyData);
</code></pre>

<p>```</p>

<p>第四个参数ioFlags是一个返回参数，表示这个property是否需要被缓存，如果需要赋值<code>kAudioFileStreamPropertyFlag_PropertyIsCached</code>否则不赋值（这个参数我也不知道应该在啥场景下使用。。一直都没去理他）；</p>

<p>这个回调会进来多次，但并不是每一次都需要进行处理，可以根据需求处理需要的PropertyID进行处理（PropertyID列表如下）。
```objc
//AudioFileStreamProperty枚举
enum
{</p>

<pre><code>kAudioFileStreamProperty_ReadyToProducePackets          =   'redy',
kAudioFileStreamProperty_FileFormat                     =   'ffmt',
kAudioFileStreamProperty_DataFormat                     =   'dfmt',
kAudioFileStreamProperty_FormatList                     =   'flst',
kAudioFileStreamProperty_MagicCookieData                    =   'mgic',
kAudioFileStreamProperty_AudioDataByteCount             =   'bcnt',
kAudioFileStreamProperty_AudioDataPacketCount           =   'pcnt',
kAudioFileStreamProperty_MaximumPacketSize              =   'psze',
kAudioFileStreamProperty_DataOffset                     =   'doff',
kAudioFileStreamProperty_ChannelLayout                  =   'cmap',
kAudioFileStreamProperty_PacketToFrame                  =   'pkfr',
kAudioFileStreamProperty_FrameToPacket                  =   'frpk',
kAudioFileStreamProperty_PacketToByte                   =   'pkby',
kAudioFileStreamProperty_ByteToPacket                   =   'bypk',
kAudioFileStreamProperty_PacketTableInfo                    =   'pnfo',
kAudioFileStreamProperty_PacketSizeUpperBound           =   'pkub',
kAudioFileStreamProperty_AverageBytesPerPacket          =   'abpp',
kAudioFileStreamProperty_BitRate                            =   'brat',
</code></pre>

<p>  kAudioFileStreamProperty_InfoDictionary                  =    &lsquo;info&rsquo;
};
```</p>

<p>这里列几个我认为比较重要的PropertyID：</p>

<p>1、<code>kAudioFileStreamProperty_BitRate</code>：</p>

<p>表示音频数据的码率，获取这个Property是为了计算音频的总时长Duration（因为AudioFileStream没有这样的接口。。）。</p>

<p>```objc
UInt32 bitRate;
UInt32 bitRateSize = sizeof(bitRate);
OSStatus status = AudioFileStreamGetProperty(inAudioFileStream, kAudioFileStreamProperty_BitRate, &amp;bitRateSize, &amp;bitRate);
if (status != noErr)
{</p>

<pre><code>//错误处理
</code></pre>

<p>}
```</p>

<p>2、<code>kAudioFileStreamProperty_DataOffset</code>：</p>

<p>表示音频数据在整个音频文件中的offset（因为大多数音频文件都会有一个文件头之后才使真正的音频数据），这个值在seek时会发挥比较大的作用，音频的seek并不是直接seek文件位置而seek时间（比如seek到2分10秒的位置），seek时会根据时间计算出音频数据的字节offset然后需要再加上音频数据的offset才能得到在文件中的真正offset。
```objc
SInt64 dataOffset;
UInt32 offsetSize = sizeof(dataOffset);
OSStatus status = AudioFileStreamGetProperty(inAudioFileStream, kAudioFileStreamProperty_DataOffset, &amp;offsetSize, &amp;dataOffset);
if (status != noErr)
{</p>

<pre><code>//错误处理
</code></pre>

<p>}
```</p>

<p>3、<code>kAudioFileStreamProperty_DataFormat</code></p>

<p>表示音频文件结构信息，是一个AudioStreamBasicDescription的结构</p>

<p>```objc
struct AudioStreamBasicDescription
{</p>

<pre><code>Float64 mSampleRate;
UInt32  mFormatID;
UInt32  mFormatFlags;
UInt32  mBytesPerPacket;
UInt32  mFramesPerPacket;
UInt32  mBytesPerFrame;
UInt32  mChannelsPerFrame;
UInt32  mBitsPerChannel;
UInt32  mReserved;
</code></pre>

<p>};</p>

<p>AudioStreamBasicDescription asbd;
UInt32 asbdSize = sizeof(asbd);
OSStatus status = AudioFileStreamGetProperty(inAudioFileStream, kAudioFileStreamProperty_DataFormat, &amp;asbdSize, &amp;asbd);
if (status != noErr)
{</p>

<pre><code>//错误处理
</code></pre>

<p>} <br/>
```</p>

<p>4、<code>kAudioFileStreamProperty_FormatList</code></p>

<p>作用和<code>kAudioFileStreamProperty_DataFormat</code>是一样的，区别在于用这个PropertyID获取到是一个AudioStreamBasicDescription的数组，这个参数是用来支持AAC SBR这样的包含多个文件类型的银屏格式。</p>

<p>```objc
Boolean outWriteable;
UInt32 formatListSize;
OSStatus status = AudioFileStreamGetPropertyInfo(inAudioFileStream, kAudioFileStreamProperty_FormatList, &amp;formatListSize, &amp;outWriteable);
if (status != noErr)
{</p>

<pre><code>//错误处理
</code></pre>

<p>}</p>

<p>AudioFormatListItem *formatList = malloc(formatListSize);
OSStatus status = AudioFileStreamGetProperty(inAudioFileStream, kAudioFileStreamProperty_FormatList, &amp;formatListSize, formatList);
if (status != noErr)
{</p>

<pre><code>//错误处理
</code></pre>

<p>}</p>

<p>for (int i = 0; i * sizeof(AudioFormatListItem) &lt; formatListSize; i += sizeof(AudioFormatListItem))
{</p>

<pre><code>AudioStreamBasicDescription pasbd = formatList[i].mASBD;
//选择需要的格式。。                             
</code></pre>

<p>}
free(formatList);
```</p>

<p>5、<code>kAudioFileStreamProperty_AudioDataByteCount</code></p>

<p>顾名思义，音频文件中音频数据的总量。这个Property的作用一是用来计算音频的总时长，二是可以在seek时用来计算时间对应的字节offset。</p>

<p>```objc
UInt64 audioDataByteCount;
UInt32 byteCountSize = sizeof(audioDataByteCount);
OSStatus status = AudioFileStreamGetProperty(inAudioFileStream, kAudioFileStreamProperty_AudioDataByteCount, &amp;byteCountSize, &amp;audioDataByteCount);
if (status != noErr)
{</p>

<pre><code>//错误处理
</code></pre>

<p>}
```</p>

<p>5、<code>kAudioFileStreamProperty_ReadyToProducePackets</code></p>

<p>这个PropertyID可以不必获取对应的值，一旦回调中这个PropertyID出现就代表解析完成，接下来可以对音频数据进行帧分离了。</p>

<hr />

<h1>计算时长Duration</h1>

<p>获取时长的最佳方法是从ID3信息中去读取，那样是最准确的。如果ID3信息中没有存，那就依赖于文件头中的信息去计算了。</p>

<p>计算duration的公式如下：</p>

<p><code>
double duration = (audioDataByteCount * 8) / bitRate
</code></p>

<p>音频数据的字节总量audioDataByteCount可以通过<code>kAudioFileStreamProperty_AudioDataByteCount</code>获取，码率bitRate可以通过<code>kAudioFileStreamProperty_BitRate</code>获取也可以通过Parse一部分数据后计算平均码率来得到。</p>

<p>对于CBR数据来说用这样的计算方法的duration会比较准确，对于VBR数据就不好说了。所以对于VBR数据来说，最好是能够从ID3信息中获取到duration，获取不到再想办法通过计算平均码率的途径来计算duration。</p>

<hr />

<h1>分离音频帧</h1>

<p>读取格式信息完成之后继续调用<code>AudioFileStreamParseBytes</code>方法可以对帧进行分离，并同步的进入<code>AudioFileStream_PacketsProc</code>回调方法。</p>

<p><img src="/images/iOS-audio/audiofilestreamParse-2.jpg" alt="" /></p>

<p>回调的定义：</p>

<p>```objc
typedef void (*AudioFileStream_PacketsProc)(void * inClientData,</p>

<pre><code>                                          UInt32 inNumberBytes,
                                          UInt32 inNumberPackets,
                                          const void * inInputData,
                                          AudioStreamPacketDescription * inPacketDescriptions);
</code></pre>

<p>```
第一个参数，一如既往的上下文对象；</p>

<p>第二个参数，本次处理的数据大小；</p>

<p>第三个参数，本次总共处理了多少帧（即代码里的Packet）；</p>

<p>第四个参数，本次处理的所有数据；</p>

<p>第五个参数，<code>AudioStreamPacketDescription</code>数组，存储了每一帧数据是从第几个字节开始的，这一帧总共多少字节。</p>

<p>```objc
//AudioStreamPacketDescription结构
//这里的mVariableFramesInPacket是指实际的数据帧只有VBR的数据才能用到（像MP3这样的压缩数据一个帧里会有好几个数据帧）
struct  AudioStreamPacketDescription
{</p>

<pre><code>SInt64  mStartOffset;
UInt32  mVariableFramesInPacket;
UInt32  mDataByteSize;
</code></pre>

<p>};
```</p>

<p>下面是我按照自己的理解实现的回调方法片段：</p>

<p>```
static void MyAudioFileStreamPacketsCallBack(void *inClientData,</p>

<pre><code>                                         UInt32 inNumberBytes,
                                         UInt32 inNumberPackets,
                                         const void *inInputData,
                                         AudioStreamPacketDescription   *inPacketDescriptions)
</code></pre>

<p>{</p>

<pre><code>//处理discontinuous..

if (numberOfBytes == 0 || numberOfPackets == 0)
{
    return;
}

BOOL deletePackDesc = NO;
if (packetDescriptioins == NULL)
{
    //如果packetDescriptioins不存在，就按照CBR处理，平均每一帧的数据后生成packetDescriptioins
    deletePackDesc = YES;
    UInt32 packetSize = numberOfBytes / numberOfPackets;
    packetDescriptioins = (AudioStreamPacketDescription *)malloc(sizeof(AudioStreamPacketDescription) * numberOfPackets);

    for (int i = 0; i &lt; numberOfPackets; i++)
    {
        UInt32 packetOffset = packetSize * i;
        descriptions[i].mStartOffset = packetOffset;
        descriptions[i].mVariableFramesInPacket = 0;
        if (i == numberOfPackets - 1)
        {
            packetDescriptioins[i].mDataByteSize = numberOfBytes - packetOffset;
        }
        else
        {
            packetDescriptioins[i].mDataByteSize = packetSize;
        }
    }
}

for (int i = 0; i &lt; numberOfPackets; ++i)
{
    SInt64 packetOffset = packetDescriptioins[i].mStartOffset;
    UInt32 packetSize   = packetDescriptioins[i].mDataByteSize;

    //把解析出来的帧数据放进自己的buffer中
    ...
}

if (deletePackDesc)
{
    free(packetDescriptioins);
}
</code></pre>

<p>}
```
inPacketDescriptions这个字段为空时需要按CBR的数据处理。但其实在解析CBR数据时inPacketDescriptions一般也会有返回，因为即使是CBR数据帧的大小也不是恒定不变的，例如CBR的MP3会在每一帧的数据后放1 byte的填充位，这个填充位也并非时时刻刻存在，所以帧的大小会有1 byte的浮动。（比如采样率44.1KHZ，码率160kbps的CBR MP3文件每一帧的大小在522字节和523字节浮动。）</p>

<hr />

<h1>Seek</h1>

<p>就音频的角度来seek功能描述为“我要拖到xx分xx秒”，而实际操作时我们需要操作的是文件，所以我们需要知道的是“我要拖到xx分xx秒”这个操作对应到文件上是要从第几个字节开始读取音频数据。</p>

<p>对于原始的PCM数据来说每一个PCM帧都是固定长度的，对应的播放时长也是固定的，但一旦转换成压缩后的音频数据就会因为编码形式的不同而不同了。对于CBR而言每个帧中所包含的PCM数据帧是恒定的，所以每一帧对应的播放时长也是恒定的；而VBR则不同，为了保证数据最优并且文件大小最小，VBR的每一帧中所包含的PCM数据帧是不固定的，这就导致在流播放的情况下VBR的数据想要做seek并不容易。这里我们也只讨论CBR下的seek。</p>

<p>CBR数据的seek一般是这样实现的（参考并修改自<a href="http://www.cocoawithlove.com/2010/03/streaming-mp3aac-audio-again.html">matt的blog</a>）：</p>

<p>1、近似地计算应该seek到哪个字节</p>

<p>```objc
double seekToTime = &hellip;; //需要seek到哪个时间，秒为单位
UInt64 audioDataByteCount = &hellip;; //通过kAudioFileStreamProperty_AudioDataByteCount获取的值
SInt64 dataOffset = &hellip;; //通过kAudioFileStreamProperty_DataOffset获取的值
double durtion = &hellip;; //通过公式(AudioDataByteCount * 8) / BitRate计算得到的时长</p>

<p>//近似seekOffset = 数据便宜 + seekToTime对应的近似字节数
SInt64 approximateSeekOffset = dataOffset + (seekToTime / duration) * audioDataByteCount;
```</p>

<p>2、计算seekToTime对应的是第几个帧（Packet）</p>

<p>我们可以利用之前Parse得到的音频格式信息来计算PacketDuration。<em>audioItem.fileFormat.mFramesPerPacket / </em>audioItem.fileFormat.mSampleRate;</p>

<p>```objc
//首先需要计算每个packet对应的时长
AudioStreamBasicDescription asbd = &hellip;; ////通过kAudioFileStreamProperty_DataFormat或者kAudioFileStreamProperty_FormatList获取的值
double packetDuration = asbd.mFramesPerPacket / asbd.mSampleRate</p>

<p>//然后计算packet位置
SInt64 seekToPacket = floor(seekToTime / packetDuration);
```</p>

<p>3、使用<code>AudioFileStreamSeek</code>计算精确的字节偏移和时间</p>

<p><code>AudioFileStreamSeek</code>可以用来寻找某一个帧（Packet）对应的字节偏移（byte offset）：</p>

<ul>
<li>如果找到了就会把ioFlags加上kAudioFileStreamSeekFlag_OffsetIsEstimated，并且给outDataByteOffset赋值，outDataByteOffset就是输入的seekToPacket对应的字节偏移量，我们可以根据outDataByteOffset来计算出精确的seekOffset和seekToTime；</li>
<li>如果没找到那么还是应该用第1步计算出来的approximateSeekOffset来做seek；</li>
</ul>


<p>```
SInt64 seekByteOffset;
UInt32 ioFlags = 0;
SInt64 outDataByteOffset;
OSStatus status = AudioFileStreamSeek(audioFileStreamID, seekToPacket, &amp;outDataByteOffset, &amp;ioFlags);
if (status == noErr &amp;&amp; !(ioFlags &amp; kAudioFileStreamSeekFlag_OffsetIsEstimated))
{</p>

<pre><code>//如果AudioFileStreamSeek方法找到了帧的字节偏移，需要修正一下时间
seekToTime -= ((seekByteOffset - dataOffset) - outDataByteOffset) * 8.0 / bitRate;
seekByteOffset = outDataByteOffset + dataOffset;
</code></pre>

<p>}
else
{</p>

<pre><code>seekByteOffset = approximateSeekOffset;
</code></pre>

<p>}
<code>``
4、按照seekByteOffset读取对应的数据继续使用</code>AudioFileStreamParseByte`进行解析</p>

<p>如果是网络流可以通过设置range头来获取字节，本地文件的话直接seek就好了。调用<code>AudioFileStreamParseByte</code>时注意刚seek完第一次Parse数据需要加参数<code>kAudioFileStreamParseFlag_Discontinuity</code>。</p>

<hr />

<h1>关闭AudioFileStream</h1>

<p><code>AudioFileStream</code>使用完毕后需要调用<code>AudioFileStreamClose</code>进行关闭，没啥特别需要注意的。</p>

<p><code>objc
extern OSStatus AudioFileStreamClose(AudioFileStreamID inAudioFileStream);  
</code></p>

<hr />

<h1>小结</h1>

<p>本篇关于<code>AudioFileStream</code>做了详细介绍，小结一下：</p>

<ul>
<li><p>使用<code>AudioFileStream</code>首先需要调用<code>AudioFileStreamOpen</code>，需要注意的是尽量提供inFileTypeHint参数帮助<code>AudioFileStream</code>解析数据，调用完成后记录<code>AudioFileStreamID</code>；</p></li>
<li><p>当有数据时调用<code>AudioFileStreamParseBytes</code>进行解析，每一次解析都需要注意返回值，返回值一旦出现noErr意外的值就代表Parse出错，其中<code>kAudioFileStreamError_NotOptimized</code>代表该文件缺少头信息或者其头信息在文件尾部不适合流播放；</p></li>
<li><p>使用<code>AudioFileStreamParseBytes</code>需要注意第四个参数在需要合适的时候传入<code>kAudioFileStreamParseFlag_Discontinuity</code>；</p></li>
<li><p>调用<code>AudioFileStreamParseBytes</code>后会首先同步进入<code>AudioFileStream_PropertyListenerProc</code>回调来解析文件格式信息，如果回调得到<code>kAudioFileStreamProperty_ReadyToProducePackets</code>表示解析格式信息完成；</p></li>
<li><p>解析格式信息完成后继续调用<code>AudioFileStreamParseBytes</code>会进入<code>MyAudioFileStreamPacketsCallBack</code>回调来分离音频帧，在回调中应该将分离出来的帧信息保存到自己的buffer中</p></li>
<li><p>seek时需要先近似的计算seekTime对应的seekByteOffset，然后利用<code>AudioFileStreamSeek</code>计算精确的offset，如果能得到精确的offset就修正一下seektime，如果无法得到精确的offset就用之前的近似结果</p></li>
<li><p><code>AudioFileStream</code>使用完毕后需要调用<code>AudioFileStreamClose</code>进行关闭；</p></li>
</ul>


<hr />

<h1>示例代码</h1>

<p><a href="https://github.com/mattgallagher/AudioStreamer">AudioStreamer</a>和<a href="https://github.com/muhku/FreeStreamer">FreeStreamer</a>这两份优秀的开源播放器都用到<code>AudioFileStream</code>大家也可以借鉴。我自己也写了一个<a href="https://github.com/msching/MCAudioFileStream">简单的AudioFileStream封装</a>。</p>

<hr />

<h1>下篇预告</h1>

<p>下一篇将讲述如何使用<code>AudioFile</code>。</p>

<hr />

<h1>参考资料</h1>

<p><a href="https://developer.apple.com/library/ios/documentation/audiovideo/conceptual/multimediapg/usingaudio/usingaudio.html#//apple_ref/doc/uid/TP40009767-CH2-SW28">Using Audio</a></p>

<p><a href="http://www.cocoawithlove.com/2008/09/streaming-and-playing-live-mp3-stream.html">Streaming and playing an MP3 stream</a></p>

<p><a href="http://www.cocoawithlove.com/2010/03/streaming-mp3aac-audio-again.html">Streaming MP3/AAC audio again</a></p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[iOS音频播放 (二)：AudioSession]]></title>
    <link href="http://msching.github.io/blog/2014/07/08/audio-in-ios-2/"/>
    <updated>2014-07-08T13:58:27+08:00</updated>
    <id>http://msching.github.io/blog/2014/07/08/audio-in-ios-2</id>
    <content type="html"><![CDATA[<h1>前言</h1>

<p>本篇为《iOS音频播放》系列的第二篇。</p>

<p>在实施<a href="blog/2014/07/07/audio-in-ios/">前一篇</a>中所述的7个步骤步之前还必须面对一个麻烦的问题，AudioSession。</p>

<hr />

<h1>AudioSession简介</h1>

<p>AudioSession这个玩意的主要功能包括以下几点（图片来自<a href="https://developer.apple.com/library/ios/documentation/Audio/Conceptual/AudioSessionProgrammingGuide/Introduction/Introduction.html#//apple_ref/doc/uid/TP40007875">官方文档</a>）：</p>

<ol>
<li>确定你的app如何使用音频（是播放？还是录音？）</li>
<li>为你的app选择合适的输入输出设备（比如输入用的麦克风，输出是耳机、手机功放或者airplay）</li>
<li>协调你的app的音频播放和系统以及其他app行为（例如有电话时需要打断，电话结束时需要恢复，按下静音按钮时是否歌曲也要静音等）</li>
</ol>


<p><img src="/images/iOS-audio/audiosession.jpg" alt="AudioSession" /></p>

<p>AudioSession相关的类有两个：</p>

<ol>
<li><code>AudioToolBox</code>中的<code>AudioSession</code></li>
<li><code>AVFoundation</code>中的<code>AVAudioSession</code></li>
</ol>


<p>其中AudioSession在SDK 7中已经被标注为depracated，而AVAudioSession这个类虽然iOS 3开始就已经存在了，但其中很多方法和变量都是在iOS 6以后甚至是iOS 7才有的。所以各位可以依照以下标准选择：</p>

<ul>
<li>如果最低版本支持iOS 5，请使用<code>AudioSession</code></li>
<li>如果最低版本支持iOS 6及以上，可以考虑使用<code>AVAudioSession</code></li>
<li>如果最低版本支持iOS 7及以上，请使用<code>AVAudioSession</code></li>
</ul>


<p>下面以<code>AudioSession</code>类为例来讲述AudioSession相关功能的使用（很不幸我需要支持iOS 5。。T-T，使用<code>AVAudioSession</code>的同学可以在其头文件中寻找对应的方法使用即可，需要注意的点我会加以说明）.</p>

<p><strong>注意：在使用AVAudioPlayer/AVPlayer时可以不用关心AudioSession的相关问题，Apple已经把AudioSession的处理过程封装了，但音乐打断后的响应还是要做的（比如打断后音乐暂停了UI状态也要变化，这个应该通过KVO就可以搞定了吧。。我没试过瞎猜的>_&lt;）。</strong></p>

<hr />

<h1>初始化AudioSession</h1>

<p>使用<code>AudioSession</code>类首先需要调用初始化方法：</p>

<p>```objc
extern OSStatus AudioSessionInitialize(CFRunLoopRef inRunLoop,</p>

<pre><code>                                     CFStringRef inRunLoopMode, 
                                     AudioSessionInterruptionListener inInterruptionListener,
                                     void *inClientData);
</code></pre>

<p>```</p>

<p>前两个参数一般填<code>NULL</code>表示AudioSession运行在主线程上（但并不代表音频的相关处理运行在主线程上，只是AudioSession），第三个参数需要传入一个一个<code>AudioSessionInterruptionListener</code>类型的方法，作为AudioSession被打断时的回调，第四个参数则是代表打断回调时需要附带的对象（即回到方法中的inClientData，如下所示，可以理解为UIView animation中的context）。</p>

<p><code>objc
typedef void (*AudioSessionInterruptionListener)(void * inClientData, UInt32 inInterruptionState);
</code></p>

<p>这才刚开始，坑就来了。这里会有两个问题：</p>

<p>第一，AudioSessionInitialize方法只能被有效运行一次，也就是说<code>AudioSessionInterruptionListener</code>只能被设置一次，这就意味着这个打断回调方法是一个静态方法，一旦初始化成功以后所有的打断都会回调到这个方法，即便下一次再次调用AudioSessionInitialize并且把另一个静态方法作为参数传入，当打断到来时还是会回调到第一次设置的方法上。</p>

<p>这种场景并不少见，例如你的app既需要播放歌曲又需要录音，当然你不可能知道用户会先调用哪个功能，所以你必须在播放和录音的模块中都调用AudioSessionInitialize注册打断方法，但最终打断回调只会作用在先注册的那个模块中，很蛋疼吧。。。所以对于AudioSession的使用最好的方法是生成一个类单独进行管理，统一接收打断回调并发送自定义的打断通知，在需要用到AudioSession的模块中接收通知并做相应的操作。</p>

<p>Apple也察觉到了这一点，所以在AVAudioSession中首先取消了Initialize方法，改为了单例方法<code>sharedInstance</code>。在iOS 6上所有的打断都需要通过设置<code>id&lt;AVAudioSessionDelegate&gt; delegate</code>并实现回调方法来实现，这同样会有上述的问题，所以在iOS6下仍然需要一个单独管理AudioSession的类存在。在iOS 7以后Apple终于把打断改成了通知的形式。。这下科学了。</p>

<p>第二，AudioSessionInitialize方法的第四个参数inClientData，也就是回调方法的第一个参数。上面已经说了打断回调是一个静态方法，而这个参数的目的是为了能让回调时拿到context（上下文信息），所以这个inClientData需要是一个有足够长生命周期的对象（当然前提是你确实需要用到这个参数），如果这个对象被dealloc了，那么回调时拿到的inClientData会是一个野指针。就这一点来说构造一个单独管理AudioSession的类也是有必要的，因为这个类的生命周期和AudioSession一样长，我们可以把context保存在这个类中。</p>

<hr />

<h1>监听RouteChange事件</h1>

<p>如果想要实现类似于“拔掉耳机就把歌曲暂停”的功能就需要监听RouteChange事件：</p>

<p>```objc
extern OSStatus AudioSessionAddPropertyListener(AudioSessionPropertyID inID,</p>

<pre><code>                                              AudioSessionPropertyListener inProc,
                                              void *inClientData);
</code></pre>

<p>typedef void (*AudioSessionPropertyListener)(void * inClientData,</p>

<pre><code>                                           AudioSessionPropertyID inID,
                                           UInt32 inDataSize,
                                           const void * inData);
</code></pre>

<p>```</p>

<p>调用上述方法，AudioSessionPropertyID参数传<code>kAudioSessionProperty_AudioRouteChange</code>，AudioSessionPropertyListener参数传对应的回调方法。inClientData参数同AudioSessionInitialize方法。</p>

<p>同样作为静态回调方法还是需要统一管理，接到回调时可以把第一个参数inData转换成<code>CFDictionaryRef</code>并从中获取kAudioSession_AudioRouteChangeKey_Reason键值对应的value（应该是一个CFNumberRef），得到这些信息后就可以发送自定义通知给其他模块进行相应操作(例如<code>kAudioSessionRouteChangeReason_OldDeviceUnavailable</code>就可以用来做“拔掉耳机就把歌曲暂停”)。</p>

<p>```objc
//AudioSession的AudioRouteChangeReason枚举
enum {</p>

<pre><code>    kAudioSessionRouteChangeReason_Unknown = 0,
    kAudioSessionRouteChangeReason_NewDeviceAvailable = 1,
    kAudioSessionRouteChangeReason_OldDeviceUnavailable = 2,
    kAudioSessionRouteChangeReason_CategoryChange = 3,
    kAudioSessionRouteChangeReason_Override = 4,
    kAudioSessionRouteChangeReason_WakeFromSleep = 6,
    kAudioSessionRouteChangeReason_NoSuitableRouteForCategory = 7,
    kAudioSessionRouteChangeReason_RouteConfigurationChange = 8
};
</code></pre>

<p>```</p>

<p>```objc
//AVAudioSession的AudioRouteChangeReason枚举
typedef NS_ENUM(NSUInteger, AVAudioSessionRouteChangeReason)
{</p>

<pre><code>AVAudioSessionRouteChangeReasonUnknown = 0,
AVAudioSessionRouteChangeReasonNewDeviceAvailable = 1,
AVAudioSessionRouteChangeReasonOldDeviceUnavailable = 2,
AVAudioSessionRouteChangeReasonCategoryChange = 3,
AVAudioSessionRouteChangeReasonOverride = 4,
AVAudioSessionRouteChangeReasonWakeFromSleep = 6,
AVAudioSessionRouteChangeReasonNoSuitableRouteForCategory = 7,
AVAudioSessionRouteChangeReasonRouteConfigurationChange NS_ENUM_AVAILABLE_IOS(7_0) = 8
</code></pre>

<p>}
```</p>

<p><strong>注意：iOS 6下如果使用了<code>AVAudioSession</code>由于<code>AVAudioSessionDelegate</code>中并没有定义相关的方法，还是需要用这个方法来实现监听。iOS 7下直接监听AVAudioSession的通知就可以了。</strong></p>

<hr />

<p>这里附带两个方法的实现，都是基于<code>AudioSession</code>类的（使用<code>AVAudioSession</code>的同学帮不到你们啦）。</p>

<p>1、判断是否插了耳机：</p>

<p>```objc
+ (BOOL)usingHeadset
{</p>

<h1>if TARGET_IPHONE_SIMULATOR</h1>

<pre><code>return NO;
</code></pre>

<h1>endif</h1>

<pre><code>CFStringRef route;
UInt32 propertySize = sizeof(CFStringRef);
AudioSessionGetProperty(kAudioSessionProperty_AudioRoute, &amp;propertySize, &amp;route);

BOOL hasHeadset = NO;
if((route == NULL) || (CFStringGetLength(route) == 0))
{
    // Silent Mode
}
else
{
    /* Known values of route:
     * "Headset"
     * "Headphone"
     * "Speaker"
     * "SpeakerAndMicrophone"
     * "HeadphonesAndMicrophone"
     * "HeadsetInOut"
     * "ReceiverAndMicrophone"
     * "Lineout"
     */
    NSString* routeStr = (__bridge NSString*)route;
    NSRange headphoneRange = [routeStr rangeOfString : @"Headphone"];
    NSRange headsetRange = [routeStr rangeOfString : @"Headset"];

    if (headphoneRange.location != NSNotFound)
    {
        hasHeadset = YES;
    }
    else if(headsetRange.location != NSNotFound)
    {
        hasHeadset = YES;
    }
}

if (route)
{
    CFRelease(route);
}

return hasHeadset;
</code></pre>

<p>}</p>

<p>```</p>

<p>2、判断是否开了Airplay(来自<a href="http://stackoverflow.com/questions/13044894/get-name-of-airplay-device-using-avplayer">StackOverflow</a>)：</p>

<p>```objc
+ (BOOL)isAirplayActived
{</p>

<pre><code>CFDictionaryRef currentRouteDescriptionDictionary = nil;
UInt32 dataSize = sizeof(currentRouteDescriptionDictionary);
AudioSessionGetProperty(kAudioSessionProperty_AudioRouteDescription, &amp;dataSize, &amp;currentRouteDescriptionDictionary);

BOOL airplayActived = NO;
if (currentRouteDescriptionDictionary)
{
    CFArrayRef outputs = CFDictionaryGetValue(currentRouteDescriptionDictionary, kAudioSession_AudioRouteKey_Outputs);
    if(outputs != NULL &amp;&amp; CFArrayGetCount(outputs) &gt; 0)
    {
        CFDictionaryRef currentOutput = CFArrayGetValueAtIndex(outputs, 0);
        //Get the output type (will show airplay / hdmi etc
        CFStringRef outputType = CFDictionaryGetValue(currentOutput, kAudioSession_AudioRouteKey_Type);

        airplayActived = (CFStringCompare(outputType, kAudioSessionOutputRoute_AirPlay, 0) == kCFCompareEqualTo);
    }
    CFRelease(currentRouteDescriptionDictionary);                
}
return airplayActived;
</code></pre>

<p>}</p>

<p>```</p>

<hr />

<h1>设置类别</h1>

<p>下一步要设置AudioSession的Category，使用<code>AudioSession</code>时调用下面的接口</p>

<p>```objc
extern OSStatus AudioSessionSetProperty(AudioSessionPropertyID inID,</p>

<pre><code>                                      UInt32 inDataSize,
                                      const void *inData);
</code></pre>

<p>```</p>

<p>如果我需要的功能是播放，执行如下代码</p>

<p>```objc
UInt32 sessionCategory = kAudioSessionCategory_MediaPlayback;
AudioSessionSetProperty (kAudioSessionProperty_AudioCategory,</p>

<pre><code>                       sizeof(sessionCategory),
                       &amp;sessionCategory);
</code></pre>

<p>```</p>

<p>使用<code>AVAudioSession</code>时调用下面的接口</p>

<p><code>objc
/* set session category */
- (BOOL)setCategory:(NSString *)category error:(NSError **)outError;
/* set session category with options */
- (BOOL)setCategory:(NSString *)category withOptions: (AVAudioSessionCategoryOptions)options error:(NSError **)outError NS_AVAILABLE_IOS(6_0);
</code></p>

<p>至于Category的类型在<a href="https://developer.apple.com/library/ios/documentation/Audio/Conceptual/AudioSessionProgrammingGuide/AudioSessionBasics/AudioSessionBasics.html#//apple_ref/doc/uid/TP40007875-CH3-SW1">官方文档</a>中都有介绍，我这里也只罗列一下具体就不赘述了，各位在使用时可以依照自己需要的功能设置Category。</p>

<p>```objc
//AudioSession的AudioSessionCategory枚举
enum {</p>

<pre><code>    kAudioSessionCategory_AmbientSound               = 'ambi',
    kAudioSessionCategory_SoloAmbientSound           = 'solo',
    kAudioSessionCategory_MediaPlayback              = 'medi',
    kAudioSessionCategory_RecordAudio                = 'reca',
    kAudioSessionCategory_PlayAndRecord              = 'plar',
    kAudioSessionCategory_AudioProcessing            = 'proc'
};
</code></pre>

<p>```</p>

<p>```objc
//AudioSession的AudioSessionCategory字符串
/<em>  Use this category for background sounds such as rain, car engine noise, etc.<br/>
 Mixes with other music. </em>/
AVF_EXPORT NSString *const AVAudioSessionCategoryAmbient;</p>

<p>/<em>  Use this category for background sounds.  Other music will stop playing. </em>/
AVF_EXPORT NSString *const AVAudioSessionCategorySoloAmbient;</p>

<p>/<em> Use this category for music tracks.</em>/
AVF_EXPORT NSString *const AVAudioSessionCategoryPlayback;</p>

<p>/<em>  Use this category when recording audio. </em>/
AVF_EXPORT NSString *const AVAudioSessionCategoryRecord;</p>

<p>/<em>  Use this category when recording and playing back audio. </em>/
AVF_EXPORT NSString *const AVAudioSessionCategoryPlayAndRecord;</p>

<p>/<em>  Use this category when using a hardware codec or signal processor while
 not playing or recording audio. </em>/
AVF_EXPORT NSString *const AVAudioSessionCategoryAudioProcessing;</p>

<p>```</p>

<hr />

<h1>启用</h1>

<p>有了Category就可以启动AudioSession了，启动方法：</p>

<p>```objc
//AudioSession的启动方法
extern OSStatus AudioSessionSetActive(Boolean active);
extern OSStatus AudioSessionSetActiveWithFlags(Boolean active, UInt32 inFlags);</p>

<p>//AVAudioSession的启动方法
&ndash; (BOOL)setActive:(BOOL)active error:(NSError <strong>)outError;
&ndash; (BOOL)setActive:(BOOL)active withOptions:(AVAudioSessionSetActiveOptions)options error:(NSError </strong>)outError NS_AVAILABLE_IOS(6_0);
```
启动方法调用后必须要判断是否启动成功，启动不成功的情况经常存在，例如一个前台的app正在播放，你的app正在后台想要启动AudioSession那就会返回失败。</p>

<p>一般情况下我们在启动和停止AudioSession调用第一个方法就可以了。但如果你正在做一个即时语音通讯app的话（类似于微信、易信）就需要注意在deactive AudioSession的时候需要使用第二个方法，inFlags参数传入<code>kAudioSessionSetActiveFlag_NotifyOthersOnDeactivation</code>（<code>AVAudioSession</code>给options参数传入<code>AVAudioSessionSetActiveOptionNotifyOthersOnDeactivation</code>）。当你的app deactive自己的AudioSession时系统会通知上一个被打断播放app打断结束（就是上面说到的打断回调），如果你的app在deactive时传入了NotifyOthersOnDeactivation参数，那么其他app在接到打断结束回调时会多得到一个参数<code>kAudioSessionInterruptionType_ShouldResume</code>否则就是ShouldNotResume（<code>AVAudioSessionInterruptionOptionShouldResume</code>），根据参数的值可以决定是否继续播放。</p>

<p>大概流程是这样的：</p>

<ol>
<li>一个音乐软件A正在播放；</li>
<li>用户打开你的软件播放对话语音，AudioSession active；</li>
<li>音乐软件A音乐被打断并收到InterruptBegin事件；</li>
<li>对话语音播放结束，AudioSession deactive并且传入NotifyOthersOnDeactivation参数；</li>
<li>音乐软件A收到InterruptEnd事件，查看Resume参数，如果是ShouldResume控制音频继续播放，如果是ShouldNotResume就维持打断状态；</li>
</ol>


<p><a href="https://developer.apple.com/library/ios/documentation/Audio/Conceptual/AudioSessionProgrammingGuide/ConfiguringanAudioSession/ConfiguringanAudioSession.html#//apple_ref/doc/uid/TP40007875-CH2-SW1">官方文档</a>中有一张很形象的图来阐述这个现象：</p>

<p><img src="/images/iOS-audio/audiosession-active.jpg" alt="" /></p>

<p>然而现在某些语音通讯软件和某些音乐软件却无视了<code>NotifyOthersOnDeactivation</code>和<code>ShouldResume</code>的正确用法，导致我们经常接到这样的用户反馈：</p>

<pre><code>你们的app在使用xx语音软件听了一段话后就不会继续播放了，但xx音乐软件可以继续播放啊。
</code></pre>

<p>好吧，上面只是吐槽一下。请无视我吧。</p>

<hr />

<h1>打断处理</h1>

<p>正常启动AudioSession之后就可以播放音频了，下面要讲的是对于打断的处理。之前我们说到打断的回调在iOS5、6下需要统一管理，在收到打断开始和结束时需要发送自定义的通知。</p>

<p>使用<code>AudioSession</code>时打断回调应该首先获取<code>kAudioSessionProperty_InterruptionType</code>，然后发送一个自定义的通知并带上对应的参数。</p>

<p>```objc
static void MyAudioSessionInterruptionListener(void *inClientData, UInt32 inInterruptionState)
{</p>

<pre><code>AudioSessionInterruptionType interruptionType = kAudioSessionInterruptionType_ShouldNotResume;
UInt32 interruptionTypeSize = sizeof(interruptionType);
AudioSessionGetProperty(kAudioSessionProperty_InterruptionType,
                        &amp;interruptionTypeSize,
                        &amp;interruptionType);

NSDictionary *userInfo = @{MyAudioInterruptionStateKey:@(inInterruptionState),
                             MyAudioInterruptionTypeKey:@(interruptionType)};

[[NSNotificationCenter defaultCenter] postNotificationName:MyAudioInterruptionNotification object:nil userInfo:userInfo];
</code></pre>

<p>}
```</p>

<p>收到通知后的处理方法如下（注意ShouldResume参数）：</p>

<p>```objc
&ndash; (void)interruptionNotificationReceived:(NSNotification *)notification
{</p>

<pre><code>UInt32 interruptionState = [notification.userInfo[MyAudioInterruptionStateKey] unsignedIntValue];
AudioSessionInterruptionType interruptionType = [notification.userInfo[MyAudioInterruptionTypeKey] unsignedIntValue];
[self handleAudioSessionInterruptionWithState:interruptionState type:interruptionType];
</code></pre>

<p>}</p>

<ul>
<li>(void)handleAudioSessionInterruptionWithState:(UInt32)interruptionState type:(AudioSessionInterruptionType)interruptionType
{
  if (interruptionState == kAudioSessionBeginInterruption)
  {
      //控制UI，暂停播放
  }
  else if (interruptionState == kAudioSessionEndInterruption)
  {
      if (interruptionType == kAudioSessionInterruptionType_ShouldResume)
      {
          OSStatus status = AudioSessionSetActive(true);
          if (status == noErr)
          {
              //控制UI，继续播放
          }
      }
  }
}
```</li>
</ul>


<hr />

<h1>小结</h1>

<p>关于AudioSession的话题到此结束（码字果然很累。。）。小结一下：</p>

<ul>
<li>如果最低版本支持iOS 5，请使用<code>AudioSession</code>，需要有一个类统一管理AudioSession的所有回调，在接到回调后发送对应的自定义通知；</li>
<li>如果最低版本支持iOS 6及以上，可以考虑使用<code>AVAudioSession</code>，仍然需要统一管理AudioSession的回调，其中打断回调可以使用<code>AVAudioSessionDelegate</code>的方法进行监听；</li>
<li>如果最低版本支持iOS 7及以上，请使用<code>AVAudioSession</code>，不用统一管理，接AVAudioSession的通知即可；</li>
<li>根据app的应用场景合理选择<code>Category</code>；</li>
<li>在deactive时需要注意app的应用场景来合理的选择是否使用<code>NotifyOthersOnDeactivation</code>参数；</li>
<li>在处理InterruptEnd事件时需要注意<code>ShouldResume</code>的值。</li>
</ul>


<hr />

<h1>示例代码</h1>

<p><a href="https://github.com/msching/MCAudioSession">这里</a>有我自己写的<code>AudioSession</code>的封装，如果各位需要支持iOS 5和6的话可以使用一下。</p>

<hr />

<h1>下篇预告</h1>

<p><del>下一篇将讲述如何使用<code>AudioFileStreamer</code>分离音频帧，以及如何使用<code>AudioQueue</code>进行播放。</del></p>

<p>下一篇将讲述如何使用<code>AudioFileStreamer</code>提取音频文件格式信息和分离音频帧。</p>

<hr />

<h1>参考资料</h1>

<p><a href="https://developer.apple.com/library/ios/documentation/Audio/Conceptual/AudioSessionProgrammingGuide/Introduction/Introduction.html#//apple_ref/doc/uid/TP40007875">AudioSession</a></p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[iOS音频播放 (一)：概述]]></title>
    <link href="http://msching.github.io/blog/2014/07/07/audio-in-ios/"/>
    <updated>2014-07-07T14:40:42+08:00</updated>
    <id>http://msching.github.io/blog/2014/07/07/audio-in-ios</id>
    <content type="html"><![CDATA[<h1>前言</h1>

<p>从事音乐相关的app开发也已经有一段时日了，在这过程中app的播放器几经修改我也因此对于iOS下的音频播放实现有了一定的研究。写这个系列的博客目的一方面希望能够抛砖引玉，另一方面也是希望能帮助国内其他的iOS开发者和爱好者少走弯路（我自己就遇到了不少的坑=。=）。</p>

<p>本篇为《iOS音频播放》系列的第一篇，主要将对iOS下实现音频播放的方法进行概述。</p>

<hr />

<h1>基础</h1>

<p>先来简单了解一下一些基础的音频知识。</p>

<p>目前我们在计算机上进行音频播放都需要依赖于音频文件，音频文件的生成过程是将声音信息采样、量化和编码产生的数字信号的过程，人耳所能听到的声音，最低的频率是从20Hz起一直到最高频率20KHZ，因此音频文件格式的最大带宽是20KHZ。根据<a href="http://zh.wikipedia.org/wiki/%E5%A5%88%E5%A5%8E%E6%96%AF%E7%89%B9%E9%A2%91%E7%8E%87">奈奎斯特</a>的理论，只有采样频率高于声音信号最高频率的两倍时，才能把数字信号表示的声音还原成为原来的声音，所以音频文件的采样率一般在40~50KHZ，比如最常见的CD音质采样率44.1KHZ。</p>

<p>对声音进行采样、量化过程被称为<a href="http://zh.wikipedia.org/wiki/%E8%84%88%E8%A1%9D%E7%B7%A8%E8%99%9F%E8%AA%BF%E8%AE%8A">脉冲编码调制</a>（Pulse Code Modulation），简称<code>PCM</code>。PCM数据是最原始的音频数据完全无损，所以PCM数据虽然音质优秀但体积庞大，为了解决这个问题先后诞生了一系列的音频格式，这些音频格式运用不同的方法对音频数据进行压缩，其中有无损压缩（ALAC、APE、FLAC）和有损压缩（MP3、AAC、OGG、WMA）两种。</p>

<p>目前最为常用的音频格式是MP3，MP3是一种有损压缩的音频格式，设计这种格式的目的就是为了大幅度的减小音频的数据量，它舍弃PCM音频数据中人类听觉不敏感的部分，从下面的比较图我们可以明显的看到MP3数据相比PCM数据明显矮了一截（图片引自<a href="http://bbs.imp3.net/thread-243641-1-1.html">imp3论坛</a>）。</p>

<p><img src="/images/iOS-audio/pcm.jpg" alt="上图为pcm数据" />
<img src="/images/iOS-audio/mp3.jpg" alt="上图为mp3数据" /></p>

<p>MP3格式中的码率（BitRate）代表了MP3数据的压缩质量，现在常用的码率有128kbit/s、160kbit/s、320kbit/s等等，这个值越高声音质量也就越高。MP3编码方式常用的有两种<a href="http://zh.wikipedia.org/wiki/%E5%9B%BA%E5%AE%9A%E7%A0%81%E7%8E%87">固定码率</a>(Constant bitrate，CBR)和<a href="http://zh.wikipedia.org/wiki/%E5%8F%AF%E5%8F%98%E7%A0%81%E7%8E%87">可变码率</a>(Variable bitrate，VBR)。</p>

<p>MP3格式中的数据通常由两部分组成，一部分为<a href="http://zh.wikipedia.org/zh/ID3">ID3</a>用来存储歌名、演唱者、专辑、音轨数等信息，另一部分为音频数据。音频数据部分以帧(frame)为单位存储，每个音频都有自己的帧头，如图所示就是一个MP3文件帧结构图（图片同样来自互联网）。MP3中的每一个帧都有自己的帧头，其中存储了码率、采样率等解码必须的信息，所以每一个帧都可以独立于文件存在和播放，这个特性加上高压缩比使得MP3文件成为了音频流播放的主流格式。帧头之后存储着音频数据，这些音频数据是若干个PCM数据帧经过压缩算法压缩得到的，对CBR的MP3数据来说每个帧中包含的PCM数据帧是固定的，而VBR是可变的。</p>

<p><img src="/images/iOS-audio/mp3frame.jpg" alt="" /></p>

<hr />

<h1>iOS音频播放概述</h1>

<p>了解了基础概念之后我们就可以列出一个经典的音频播放流程（以MP3为例）：</p>

<ol>
<li>读取MP3文件</li>
<li>解析采样率、码率、时长等信息，分离MP3中的音频帧</li>
<li>对分离出来的音频帧解码得到PCM数据</li>
<li>对PCM数据进行音效处理（均衡器、混响器等，非必须）</li>
<li>把PCM数据解码成音频信号</li>
<li>把音频信号交给硬件播放</li>
<li>重复1-6步直到播放完成</li>
</ol>


<p>在iOS系统中apple对上述的流程进行了封装并提供了不同层次的接口（图片引自<a href="https://developer.apple.com/library/ios/documentation/MusicAudio/Conceptual/CoreAudioOverview/CoreAudioEssentials/CoreAudioEssentials.html#//apple_ref/doc/uid/TP40003577-CH10-SW1">官方文档</a>）。</p>

<p><img src="/images/iOS-audio/api-architectural-layers.png" alt="CoreAudio的接口层次" /></p>

<p>下面对其中的中高层接口进行功能说明：</p>

<ul>
<li>Audio File Services：读写音频数据，可以完成播放流程中的第2步；</li>
<li>Audio File Stream Services：对音频进行解码，可以完成播放流程中的第2步；</li>
<li>Audio Converter services：音频数据转换，可以完成播放流程中的第3步；</li>
<li>Audio Processing Graph Services：音效处理模块，可以完成播放流程中的第4步；</li>
<li>Audio Unit Services：播放音频数据：可以完成播放流程中的第5步、第6步；</li>
<li>Extended Audio File Services：Audio File Services和Audio Converter services的结合体；</li>
<li>AVAudioPlayer/AVPlayer(AVFoundation)：高级接口，可以完成整个音频播放的过程（包括本地文件和网络流播放，第4步除外）；</li>
<li>Audio Queue Services：高级接口，可以进行录音和播放，可以完成播放流程中的第3、5、6步；</li>
<li>OpenAL：用于游戏音频播放，暂不讨论</li>
</ul>


<p>可以看到apple提供的接口类型非常丰富，可以满足各种类别类需求：</p>

<ul>
<li><p>如果你只是想实现音频的播放，没有其他需求AVFoundation会很好的满足你的需求。它的接口使用简单、不用关心其中的细节；</p></li>
<li><p>如果你的app需要对音频进行流播放并且同时存储，那么AudioFileStreamer加AudioQueue能够帮到你，你可以先把音频数据下载到本地，一边下载一边用NSFileHandler等接口读取本地音频文件并交给AudioFileStreamer或者AudioFile解析分离音频帧，分离出来的音频帧可以送给AudioQueue进行解码和播放。如果是本地文件直接读取文件解析即可。（这两个都是比较直接的做法，这类需求也可以用AVFoundation+本地server的方式实现，AVAudioPlayer会把请求发送给本地server，由本地server转发出去，获取数据后在本地server中存储并转送给AVAudioPlayer。另一个比较trick的做法是先把音频下载到文件中，在下载到一定量的数据后把文件路径给AVAudioPlayer播放，当然这种做法在音频seek后就回有问题了。）；</p></li>
<li><p>如果你正在开发一个专业的音乐播放软件，需要对音频施加音效（均衡器、混响器），那么除了数据的读取和解析以外还需要用到AudioConverter来把音频数据转换成PCM数据，再由AudioUnit+AUGraph来进行音效处理和播放（但目前多数带音效的app都是自己开发音效模块来坐PCM数据的处理，这部分功能自行开发在自定义性和扩展性上会比较强一些。PCM数据通过音效器处理完成后就可以使用AudioUnit播放了，当然AudioQueue也支持直接使对PCM数据进行播放。）。下图描述的就是使用AudioFile + AudioConverter + AudioUnit进行音频播放的流程（图片引自<a href="https://developer.apple.com/library/ios/documentation/MusicAudio/Conceptual/CoreAudioOverview/ARoadmaptoCommonTasks/ARoadmaptoCommonTasks.html#//apple_ref/doc/uid/TP40003577-CH6-SW1">官方文档</a>）。</p></li>
</ul>


<p><img src="/images/iOS-audio/audioUnitPlay.jpg" alt="" /></p>

<hr />

<h1>下篇预告</h1>

<p>下一篇将讲述iOS音频播放中必须面对的难（da）题（keng），AudioSession。</p>

<hr />

<h1>参考资料</h1>

<p><a href="http://zh.wikipedia.org/zh/%E9%9F%B3%E9%A2%91%E6%96%87%E4%BB%B6%E6%A0%BC%E5%BC%8F">音频文件格式</a></p>

<p><a href="http://zh.wikipedia.org/wiki/%E8%84%88%E8%A1%9D%E7%B7%A8%E8%99%9F%E8%AA%BF%E8%AE%8A">脉冲编码调制</a></p>

<p><a href="http://zh.wikipedia.org/zh/%E9%87%87%E6%A0%B7%E7%8E%87">采样率</a></p>

<p><a href="http://zh.wikipedia.org/wiki/%E5%A5%88%E5%A5%8E%E6%96%AF%E7%89%B9%E9%A2%91%E7%8E%87">奈奎斯特频率</a></p>

<p><a href="http://zh.wikipedia.org/wiki/MP3">MP3</a></p>

<p><a href="http://zh.wikipedia.org/zh/ID3">ID3</a></p>

<p><a href="https://developer.apple.com/library/ios/documentation/MusicAudio/Conceptual/CoreAudioOverview/CoreAudioEssentials/CoreAudioEssentials.html#//apple_ref/doc/uid/TP40003577-CH10-SW1">Core Audio Essential</a></p>

<p><a href="https://developer.apple.com/library/ios/documentation/MusicAudio/Conceptual/CoreAudioOverview/ARoadmaptoCommonTasks/ARoadmaptoCommonTasks.html#//apple_ref/doc/uid/TP40003577-CH6-SW1">Common Tasks in OS X</a></p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[奇怪的Graphics消耗（iOS 7.1.x）]]></title>
    <link href="http://msching.github.io/blog/2014/07/04/strange-high-graphics-cost-in-ios-7-dot-1-x/"/>
    <updated>2014-07-04T14:25:31+08:00</updated>
    <id>http://msching.github.io/blog/2014/07/04/strange-high-graphics-cost-in-ios-7-dot-1-x</id>
    <content type="html"><![CDATA[<p>近期有部分用户反馈我们的app在使用时有发热现象，在排查原因的过程中发现了一个奇怪的问题。在某个页面推出时<code>Instruments</code>的<code>Core Animation</code>会有帧数显示，数值在59~60，而此时并没有任何需要消耗Graphics的代码在跑，所有UI都是静止的。这个现象只会在iOS 7.1.x上出现，其他系统包括最新发布的iOS8 beta上均没有出现类似问题。</p>

<p>造成这个现象的页面比较特殊，其展现形式看上去虽然是一个UIViewController把一个UINavigationController用ModalView的形式push出来了，但实际上是把UINavigationController的.view直接add在了UIViewController的.view上，并用一个UIView动画展示页面。</p>

<p>为了弄清楚到底为什么会在UI静止的情况下产生Graphics消耗，我新建了一个空工程用相同的方法实现了一个推出页面的动画，然后profile却没有发现有问题。再次回头看app发现这个推出页面的controller是个UITabBarController，于是为测试工程加上tabBarController后再次profile，问题重现了，Core Animation在页面出现之后显示了帧数，页面隐藏之后帧数显示就消失了，从而可以推断是UITabBarController上的某个UI元素导致了这个问题。</p>

<p>接下来的步骤就是遍历UITabBarController的view上所有的subview并逐个隐藏来进行测试，幸运的是第一个就蒙对了，在我隐藏了UITabBarController的tabbar以后奇怪的帧数就不再出现了。这个现象很快让我想到了iOS 7以后苹果加入的模糊效果，这个效果在UINavigationBar、UITabBar、UIToolBar等UI控件上都有使用，下面把UITabBarController去掉，在view上直接add一个UIToolBar，发现问题同样会出现。至此基本确定是由于这个模糊效果造成的，至于为什么只会在7.1.x上出现这个问题。。可能只有苹果才知道了=。=。</p>

<p>测试程序代码点<a href="https://github.com/msching/TestGPU">这里</a></p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[iOS8beta1下WebCore可能会打断音频播放]]></title>
    <link href="http://msching.github.io/blog/2014/06/26/audio-interrput-by-webcore-in-ios-8-beta-1/"/>
    <updated>2014-06-26T16:09:56+08:00</updated>
    <id>http://msching.github.io/blog/2014/06/26/audio-interrput-by-webcore-in-ios-8-beta-1</id>
    <content type="html"><![CDATA[<h1>问题</h1>

<p>前不久在QA发现一个问题，在iOS8 beta1上使用我们的app播放歌曲时进入某些内嵌的web页面（UIWebview实现）时歌曲会暂停播放，但是界面仍然显示为正在播放状态。把真机连上Xcode6调试后发现在进入部分网页时会再console上打印如下log：</p>

<pre><code>AVAudioSession.mm:623: -[AVAudioSession setActive:withOptions:error:]: Deactivating an audio session that has running I/O. All I/O should be stopped or paused prior to deactivating the audio session.
</code></pre>

<p>bt后堆栈如下：</p>

<p>```</p>

<pre><code>frame #1: 0x299632fe libAVFAudio.dylib`-[AVAudioSession setActive:error:] + 26
frame #2: 0x3551b92e WebCore`WebCore::AudioSession::setActive(bool) + 62
frame #3: 0x35af2674 WebCore`WebCore::MediaSessionManager::updateSessionState() + 100
frame #4: 0x35af03b6 WebCore`WebCore::MediaSessionManager::addSession(WebCore::MediaSession&amp;) + 74
frame #5: 0x35af0002 WebCore`WebCore::MediaSession::MediaSession(WebCore::MediaSessionClient&amp;) + 38
frame #6: 0x35735a20 WebCore`WebCore::HTMLMediaSession::create(WebCore::MediaSessionClient&amp;) + 20
frame #7: 0x35724c68 WebCore`WebCore::HTMLMediaElement::HTMLMediaElement(WebCore::QualifiedName const&amp;, WebCore::Document&amp;, bool) + 976
frame #8: 0x3570ad24 WebCore`WebCore::HTMLAudioElement::create(WebCore::QualifiedName const&amp;, WebCore::Document&amp;, bool) + 36
frame #9: 0x35718184 WebCore`WebCore::audioConstructor(WebCore::QualifiedName const&amp;, WebCore::Document&amp;, WebCore::HTMLFormElement*, bool) + 56
frame #10: 0x3571803a WebCore`WebCore::HTMLElementFactory::createElement(WebCore::QualifiedName const&amp;, WebCore::Document&amp;, WebCore::HTMLFormElement*, bool) + 230
frame #11: 0x3533a26c WebCore`WebCore::HTMLDocument::createElement(WTF::AtomicString const&amp;, int&amp;) + 88
frame #12: 0x3533a1ae WebCore`WebCore::jsDocumentPrototypeFunctionCreateElement(JSC::ExecState*) + 242
frame #13: 0x2c1cc4d4 JavaScriptCore`llint_entry + 21380
</code></pre>

<p>```</p>

<p>发现是WebCore调用了<code>AVAudioSession</code>的setActive方法，并且把active置为了NO。这个过程其实类似于音乐在播放时被其他事件打断（例如电话、siri）一样，audio会被打断，同时会发送<code>kAudioSessionBeginInterruption</code>事件通知app音频播放已经被打断，需要修正播放器和UI状态；打断结束后回发送<code>kAudioSessionEndInterruption</code>事件通知app恢复播放状态。区别在于WebCore的打断并没有任何通知，所以就导致界面上的播放状态为播放中而实际音乐却被打断。</p>

<h1>适配</h1>

<p>接下来就要对这个问题进行适配了：</p>

<ol>
<li>首先，联系了前段组的同事对出现问题的页面进行检查，之后被告知是某个页面的js中调用了一些播放相关的代码导致了这个问题，这些js是之前版本中使用的，现在已经被废弃但没有及时的删除。在删除这些js后，问题自然就消失了。</li>
<li>客户端本身也应该做一些适配来防止下次再有页面出现类似问题，目前我能想到的办法是做一个<code>AVAudioSession</code>的category，method swizzle方法<code>setActive:withOptions:error:</code>在设置active值时发送通知来修改UI的状态。</li>
</ol>

]]></content>
  </entry>
  
</feed>
