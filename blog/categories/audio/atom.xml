<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Category: Audio | 码农人生]]></title>
  <link href="http://msching.github.io/blog/categories/audio/atom.xml" rel="self"/>
  <link href="http://msching.github.io/"/>
  <updated>2014-07-08T17:52:03+08:00</updated>
  <id>http://msching.github.io/</id>
  <author>
    <name><![CDATA[ChengYinZju]]></name>
    
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[iOS音频播放 (二)：AudioSession]]></title>
    <link href="http://msching.github.io/blog/2014/07/08/audio-in-ios-2/"/>
    <updated>2014-07-08T13:58:27+08:00</updated>
    <id>http://msching.github.io/blog/2014/07/08/audio-in-ios-2</id>
    <content type="html"><![CDATA[<h1>前言</h1>

<p>本篇为《iOS音频播放》系列的第二篇。</p>

<p>在实施<a href="blog/2014/07/07/audio-in-ios/">前一篇</a>中所述的7个步骤步之前还必须面对一个麻烦的问题，AudioSession。</p>

<hr />

<h1>AudioSession简介</h1>

<p>AudioSession这个玩意的主要功能包括以下几点（图片来自<a href="https://developer.apple.com/library/ios/documentation/Audio/Conceptual/AudioSessionProgrammingGuide/Introduction/Introduction.html#//apple_ref/doc/uid/TP40007875">官方文档</a>）：</p>

<ol>
<li>确定你的app如何使用音频（是播放？还是录音？）</li>
<li>为你的app选择合适的输入输出设备（比如输入用的麦克风，输出是耳机、手机功放或者airplay）</li>
<li>协调你的app的音频播放和系统以及其他app行为（例如有电话时需要打断，电话结束时需要恢复，按下静音按钮时是否歌曲也要静音等）</li>
</ol>


<p><img src="/images/iOS-audio/audiosession.jpg" alt="AudioSession" /></p>

<p>AudioSession相关的类有两个：</p>

<ol>
<li><code>AudioToolBox</code>中的<code>AudioSession</code></li>
<li><code>AVFoundation</code>中的<code>AVAudioSession</code></li>
</ol>


<p>其中AudioSession在SDK 7中已经被标注为depracated，而AVAudioSession这个类虽然iOS 3开始就已经存在了，但其中很多方法和变量都是在iOS 6以后甚至是iOS 7才有的。所以各位可以依照以下标准选择：</p>

<ul>
<li>如果最低版本支持iOS 5，请使用<code>AudioSession</code></li>
<li>如果最低版本支持iOS 6及以上，可以考虑使用<code>AVAudioSession</code></li>
<li>如果最低版本支持iOS 7及以上，请使用<code>AVAudioSession</code></li>
</ul>


<p>下面以<code>AudioSession</code>类为例来讲述AudioSession相关功能的使用（很不幸我需要支持iOS 5。。T-T，使用<code>AVAudioSession</code>的同学可以在其头文件中寻找对应的方法使用即可，需要注意的点我会加以说明）.</p>

<p><strong>注意：在使用AVAudioPlayer/AVPlayer时可以不用关心AudioSession的相关问题，Apple已经把AudioSession的处理过程封装了，但音乐打断后的响应还是要做的（比如打断后音乐暂停了UI状态也要变化，这个应该通过KVO就可以搞定了吧。。我没试过瞎猜的>_&lt;）。</strong></p>

<hr />

<h1>初始化AudioSession</h1>

<p>使用<code>AudioSession</code>类首先需要调用初始化方法：</p>

<p>```objc
extern OSStatus AudioSessionInitialize(CFRunLoopRef inRunLoop,</p>

<pre><code>                                     CFStringRef inRunLoopMode, 
                                     AudioSessionInterruptionListener inInterruptionListener,
                                     void *inClientData);
</code></pre>

<p>```</p>

<p>前两个参数一般填<code>NULL</code>表示AudioSession运行在主线程上（但并不代表音频的相关处理运行在主线程上，只是AudioSession），第三个参数需要传入一个一个<code>AudioSessionInterruptionListener</code>类型的方法，作为AudioSession被打断时的回调，第四个参数则是代表打断回调时需要附带的对象（即回到方法中的inClientData，如下所示，可以理解为UIView animation中的context）。</p>

<p><code>objc
typedef void (*AudioSessionInterruptionListener)(void * inClientData, UInt32 inInterruptionState);
</code></p>

<p>这才刚开始，坑就来了。这里会有两个问题：</p>

<p>第一，AudioSessionInitialize方法只能被有效运行一次，也就是说<code>AudioSessionInterruptionListener</code>只能被设置一次，这就意味着这个打断回调方法是一个静态方法，一旦初始化成功以后所有的打断都会回调到这个方法，即便下一次再次调用AudioSessionInitialize并且把另一个静态方法作为参数传入，当打断到来时还是会回调到第一次设置的方法上。</p>

<p>这种场景并不少见，例如你的app既需要播放歌曲又需要录音，当然你不可能知道用户会先调用哪个功能，所以你必须在播放和录音的模块中都调用AudioSessionInitialize注册打断方法，但最终打断回调只会作用在先注册的那个模块中，很蛋疼吧。。。所以对于AudioSession的使用最好的方法是生成一个类单独进行管理，统一接收打断回调并发送自定义的打断通知，在需要用到AudioSession的模块中接收通知并做相应的操作。</p>

<p>Apple也察觉到了这一点，所以在AVAudioSession中首先取消了Initialize方法，改为了单例方法<code>sharedInstance</code>。在iOS 6上所有的打断都需要通过设置<code>id&lt;AVAudioSessionDelegate&gt; delegate</code>并实现回调方法来实现，这同样会有上述的问题，所以在iOS6下仍然需要一个单独管理AudioSession的类存在。在iOS 7以后Apple终于把打断改成了通知的形式。。这下科学了。</p>

<p>第二，AudioSessionInitialize方法的第四个参数inClientData，也就是回调方法的第一个参数。上面已经说了打断回调是一个静态方法，而这个参数的目的是为了能让回调时拿到context（上下文信息），所以这个inClientData需要是一个有足够长生命周期的对象（当然前提是你确实需要用到这个参数），如果这个对象被dealloc了，那么回调时拿到的inClientData会是一个野指针。就这一点来说构造一个单独管理AudioSession的类也是有必要的，因为这个类的生命周期和AudioSession一样长，我们可以把context保存在这个类中。</p>

<hr />

<h1>监听RouteChange事件</h1>

<p>如果想要实现类似于“拔掉耳机就把歌曲暂停”的功能就需要监听RouteChange事件：</p>

<p>```objc
extern OSStatus AudioSessionAddPropertyListener(AudioSessionPropertyID inID,</p>

<pre><code>                                              AudioSessionPropertyListener inProc,
                                              void *inClientData);
</code></pre>

<p>typedef void (*AudioSessionPropertyListener)(void * inClientData,</p>

<pre><code>                                           AudioSessionPropertyID inID,
                                           UInt32 inDataSize,
                                           const void * inData);
</code></pre>

<p>```</p>

<p>调用上述方法，AudioSessionPropertyID参数传<code>kAudioSessionProperty_AudioRouteChange</code>，AudioSessionPropertyListener参数传对应的回调方法。inClientData参数同AudioSessionInitialize方法。</p>

<p>同样作为静态回调方法还是需要统一管理，接到回调时可以把第一个参数inData转换成<code>CFDictionaryRef</code>并从中获取kAudioSession_AudioRouteChangeKey_Reason键值对应的value（应该是一个CFNumberRef），得到这些信息后就可以发送自定义通知给其他模块进行相应操作(例如<code>kAudioSessionRouteChangeReason_OldDeviceUnavailable</code>就可以用来做“拔掉耳机就把歌曲暂停”)。</p>

<p>```objc
//AudioSession的AudioRouteChangeReason枚举
enum {</p>

<pre><code>    kAudioSessionRouteChangeReason_Unknown = 0,
    kAudioSessionRouteChangeReason_NewDeviceAvailable = 1,
    kAudioSessionRouteChangeReason_OldDeviceUnavailable = 2,
    kAudioSessionRouteChangeReason_CategoryChange = 3,
    kAudioSessionRouteChangeReason_Override = 4,
    kAudioSessionRouteChangeReason_WakeFromSleep = 6,
    kAudioSessionRouteChangeReason_NoSuitableRouteForCategory = 7,
    kAudioSessionRouteChangeReason_RouteConfigurationChange = 8
};
</code></pre>

<p>```</p>

<p>```objc
//AVAudioSession的AudioRouteChangeReason枚举
typedef NS_ENUM(NSUInteger, AVAudioSessionRouteChangeReason)
{</p>

<pre><code>AVAudioSessionRouteChangeReasonUnknown = 0,
AVAudioSessionRouteChangeReasonNewDeviceAvailable = 1,
AVAudioSessionRouteChangeReasonOldDeviceUnavailable = 2,
AVAudioSessionRouteChangeReasonCategoryChange = 3,
AVAudioSessionRouteChangeReasonOverride = 4,
AVAudioSessionRouteChangeReasonWakeFromSleep = 6,
AVAudioSessionRouteChangeReasonNoSuitableRouteForCategory = 7,
AVAudioSessionRouteChangeReasonRouteConfigurationChange NS_ENUM_AVAILABLE_IOS(7_0) = 8
</code></pre>

<p>}
```</p>

<p><strong>注意：iOS 6下如果使用了<code>AVAudioSession</code>由于<code>AVAudioSessionDelegate</code>中并没有定义相关的方法，还是需要用这个方法来实现监听。iOS 7下直接监听AVAudioSession的通知就可以了。</strong></p>

<hr />

<p>这里附带两个方法的实现，都是基于<code>AudioSession</code>类的（使用<code>AVAudioSession</code>的同学帮不到你们啦）。</p>

<p>1、判断是否插了耳机：</p>

<p>```objc
+ (BOOL)usingHeadset
{</p>

<h1>if TARGET_IPHONE_SIMULATOR</h1>

<pre><code>return NO;
</code></pre>

<h1>endif</h1>

<pre><code>CFStringRef route;
UInt32 propertySize = sizeof(CFStringRef);
AudioSessionGetProperty(kAudioSessionProperty_AudioRoute, &amp;propertySize, &amp;route);

BOOL hasHeadset = NO;
if((route == NULL) || (CFStringGetLength(route) == 0))
{
    // Silent Mode
}
else
{
    /* Known values of route:
     * "Headset"
     * "Headphone"
     * "Speaker"
     * "SpeakerAndMicrophone"
     * "HeadphonesAndMicrophone"
     * "HeadsetInOut"
     * "ReceiverAndMicrophone"
     * "Lineout"
     */
    NSString* routeStr = (__bridge NSString*)route;
    NSRange headphoneRange = [routeStr rangeOfString : @"Headphone"];
    NSRange headsetRange = [routeStr rangeOfString : @"Headset"];

    if (headphoneRange.location != NSNotFound)
    {
        hasHeadset = YES;
    }
    else if(headsetRange.location != NSNotFound)
    {
        hasHeadset = YES;
    }
}

if (route)
{
    CFRelease(route);
}

return hasHeadset;
</code></pre>

<p>}</p>

<p>```</p>

<p>2、判断是否开了Airplay(来自<a href="http://stackoverflow.com/questions/13044894/get-name-of-airplay-device-using-avplayer">StackOverflow</a>)：</p>

<p>```objc
+ (BOOL)isAirplayActived
{</p>

<pre><code>CFDictionaryRef currentRouteDescriptionDictionary = nil;
UInt32 dataSize = sizeof(currentRouteDescriptionDictionary);
AudioSessionGetProperty(kAudioSessionProperty_AudioRouteDescription, &amp;dataSize, &amp;currentRouteDescriptionDictionary);

BOOL airplayActived = NO;
if (currentRouteDescriptionDictionary)
{
    CFArrayRef outputs = CFDictionaryGetValue(currentRouteDescriptionDictionary, kAudioSession_AudioRouteKey_Outputs);
    if(outputs != NULL &amp;&amp; CFArrayGetCount(outputs) &gt; 0)
    {
        CFDictionaryRef currentOutput = CFArrayGetValueAtIndex(outputs, 0);
        //Get the output type (will show airplay / hdmi etc
        CFStringRef outputType = CFDictionaryGetValue(currentOutput, kAudioSession_AudioRouteKey_Type);

        airplayActived = (CFStringCompare(outputType, kAudioSessionOutputRoute_AirPlay, 0) == kCFCompareEqualTo);
    }
    CFRelease(currentRouteDescriptionDictionary);                
}
return airplayActived;
</code></pre>

<p>}</p>

<p>```</p>

<hr />

<h1>设置类别</h1>

<p>下一步要设置AudioSession的Category，使用<code>AudioSession</code>时调用下面的接口</p>

<p>```objc
extern OSStatus AudioSessionSetProperty(AudioSessionPropertyID inID,</p>

<pre><code>                                      UInt32 inDataSize,
                                      const void *inData);
</code></pre>

<p>```</p>

<p>如果我需要的功能是播放，执行如下代码</p>

<p>```objc
UInt32 sessionCategory = kAudioSessionCategory_MediaPlayback;
AudioSessionSetProperty (kAudioSessionProperty_AudioCategory,</p>

<pre><code>                       sizeof(sessionCategory),
                       &amp;sessionCategory);
</code></pre>

<p>```</p>

<p>使用<code>AVAudioSession</code>时调用下面的接口</p>

<p><code>objc
/* set session category */
- (BOOL)setCategory:(NSString *)category error:(NSError **)outError;
/* set session category with options */
- (BOOL)setCategory:(NSString *)category withOptions: (AVAudioSessionCategoryOptions)options error:(NSError **)outError NS_AVAILABLE_IOS(6_0);
</code></p>

<p>至于Category的类型在<a href="https://developer.apple.com/library/ios/documentation/Audio/Conceptual/AudioSessionProgrammingGuide/AudioSessionBasics/AudioSessionBasics.html#//apple_ref/doc/uid/TP40007875-CH3-SW1">官方文档</a>中都有介绍，我这里也只罗列一下具体就不赘述了，各位在使用时可以依照自己需要的功能设置Category。</p>

<p>```objc
//AudioSession的AudioSessionCategory枚举
enum {</p>

<pre><code>    kAudioSessionCategory_AmbientSound               = 'ambi',
    kAudioSessionCategory_SoloAmbientSound           = 'solo',
    kAudioSessionCategory_MediaPlayback              = 'medi',
    kAudioSessionCategory_RecordAudio                = 'reca',
    kAudioSessionCategory_PlayAndRecord              = 'plar',
    kAudioSessionCategory_AudioProcessing            = 'proc'
};
</code></pre>

<p>```</p>

<p>```objc
//AudioSession的AudioSessionCategory字符串
/<em>  Use this category for background sounds such as rain, car engine noise, etc.<br/>
 Mixes with other music. </em>/
AVF_EXPORT NSString *const AVAudioSessionCategoryAmbient;</p>

<p>/<em>  Use this category for background sounds.  Other music will stop playing. </em>/
AVF_EXPORT NSString *const AVAudioSessionCategorySoloAmbient;</p>

<p>/<em> Use this category for music tracks.</em>/
AVF_EXPORT NSString *const AVAudioSessionCategoryPlayback;</p>

<p>/<em>  Use this category when recording audio. </em>/
AVF_EXPORT NSString *const AVAudioSessionCategoryRecord;</p>

<p>/<em>  Use this category when recording and playing back audio. </em>/
AVF_EXPORT NSString *const AVAudioSessionCategoryPlayAndRecord;</p>

<p>/<em>  Use this category when using a hardware codec or signal processor while
 not playing or recording audio. </em>/
AVF_EXPORT NSString *const AVAudioSessionCategoryAudioProcessing;</p>

<p>```</p>

<hr />

<h1>启用</h1>

<p>有了Category就可以启动AudioSession了，启动方法：</p>

<p>```objc
//AudioSession的启动方法
extern OSStatus AudioSessionSetActive(Boolean active);
extern OSStatus AudioSessionSetActiveWithFlags(Boolean active, UInt32 inFlags);</p>

<p>//AVAudioSession的启动方法
&ndash; (BOOL)setActive:(BOOL)active error:(NSError <strong>)outError;
&ndash; (BOOL)setActive:(BOOL)active withOptions:(AVAudioSessionSetActiveOptions)options error:(NSError </strong>)outError NS_AVAILABLE_IOS(6_0);
```
启动方法调用后必须要判断是否启动成功，启动不成功的情况经常存在，例如一个前台的app正在播放，你的app正在后台想要启动AudioSession那就会返回失败。</p>

<p>一般情况下我们在启动和停止AudioSession调用第一个方法就可以了。但如果你正在做一个即时语音通讯app的话（类似于微信、易信）就需要注意在deactive AudioSession的时候需要使用第二个方法，inFlags参数传入<code>kAudioSessionSetActiveFlag_NotifyOthersOnDeactivation</code>（<code>AVAudioSession</code>给options参数传入<code>AVAudioSessionSetActiveOptionNotifyOthersOnDeactivation</code>）。当你的app deactive自己的AudioSession时系统会通知上一个被打断播放app打断结束（就是上面说到的打断回调），如果你的app在deactive时传入了NotifyOthersOnDeactivation参数，那么其他app在接到打断结束回调时会多得到一个参数<code>kAudioSessionInterruptionType_ShouldResume</code>否则就是ShouldNotResume（<code>AVAudioSessionInterruptionOptionShouldResume</code>），根据参数的值可以决定是否继续播放。</p>

<p>大概流程是这样的：</p>

<ol>
<li>一个音乐软件A正在播放；</li>
<li>用户打开你的软件播放对话语音，AudioSession active；</li>
<li>音乐软件A音乐被打断并收到InterruptBegin事件；</li>
<li>对话语音播放结束，AudioSession deactive并且传入NotifyOthersOnDeactivation参数；</li>
<li>音乐软件A收到InterruptEnd事件，查看Resume参数，如果是ShouldResume控制音频继续播放，如果是ShouldNotResume就维持打断状态；</li>
</ol>


<p><a href="https://developer.apple.com/library/ios/documentation/Audio/Conceptual/AudioSessionProgrammingGuide/ConfiguringanAudioSession/ConfiguringanAudioSession.html#//apple_ref/doc/uid/TP40007875-CH2-SW1">官方文档</a>中有一张很形象的图来阐述这个现象：</p>

<p><img src="/images/iOS-audio/audiosession-active.jpg" alt="" /></p>

<p>然而现在某些语音通讯软件和某些音乐软件却无视了<code>NotifyOthersOnDeactivation</code>和<code>ShouldResume</code>的正确用法，导致我们经常接到这样的用户反馈：</p>

<pre><code>你们的app在使用xx语音软件听了一段话后就不会继续播放了，但xx音乐软件可以继续播放啊。
</code></pre>

<p>好吧，上面只是吐槽一下。请无视我吧。</p>

<hr />

<h1>打断处理</h1>

<p>正常启动AudioSession之后就可以播放音频了，下面要讲的是对于打断的处理。之前我们说到打断的回调在iOS5、6下需要统一管理，在收到打断开始和结束时需要发送自定义的通知。</p>

<p>使用<code>AudioSession</code>时打断回调应该首先获取<code>kAudioSessionProperty_InterruptionType</code>，然后发送一个自定义的通知并带上对应的参数。</p>

<p>```objc
static void MyAudioSessionInterruptionListener(void *inClientData, UInt32 inInterruptionState)
{</p>

<pre><code>AudioSessionInterruptionType interruptionType = kAudioSessionInterruptionType_ShouldNotResume;
UInt32 interruptionTypeSize = sizeof(interruptionType);
AudioSessionGetProperty(kAudioSessionProperty_InterruptionType,
                        &amp;interruptionTypeSize,
                        &amp;interruptionType);

NSDictionary *userInfo = @{MyAudioInterruptionStateKey:@(inInterruptionState),
                             MyAudioInterruptionTypeKey:@(interruptionType)};

[[NSNotificationCenter defaultCenter] postNotificationName:MyAudioInterruptionNotification object:nil userInfo:userInfo];
</code></pre>

<p>}
```</p>

<p>收到通知后的处理方法如下（注意ShouldResume参数）：</p>

<p>```objc
&ndash; (void)interruptionNotificationReceived:(NSNotification *)notification
{</p>

<pre><code>UInt32 interruptionState = [notification.userInfo[MyAudioInterruptionStateKey] unsignedIntValue];
AudioSessionInterruptionType interruptionType = [notification.userInfo[MyAudioInterruptionTypeKey] unsignedIntValue];
[self handleAudioSessionInterruptionWithState:interruptionState type:interruptionType];
</code></pre>

<p>}</p>

<ul>
<li>(void)handleAudioSessionInterruptionWithState:(UInt32)interruptionState type:(AudioSessionInterruptionType)interruptionType
{
  if (interruptionState == kAudioSessionBeginInterruption)
  {
      //控制UI，暂停播放
  }
  else if (interruptionState == kAudioSessionEndInterruption)
  {
      if (interruptionType == kAudioSessionInterruptionType_ShouldResume)
      {
          OSStatus status = AudioSessionSetActive(true);
          if (status == noErr)
          {
              //控制UI，继续播放
          }
      }
  }
}
```</li>
</ul>


<hr />

<h1>小结</h1>

<p>关于AudioSession的话题到此结束（码字果然很累。。）。小结一下：</p>

<ul>
<li>如果最低版本支持iOS 5，请使用<code>AudioSession</code>，需要有一个类统一管理AudioSession的所有回调，在接到回调后发送对应的自定义通知；</li>
<li>如果最低版本支持iOS 6及以上，可以考虑使用<code>AVAudioSession</code>，仍然需要统一管理AudioSession的回调，其中打断回调可以使用<code>AVAudioSessionDelegate</code>的方法进行监听；</li>
<li>如果最低版本支持iOS 7及以上，请使用<code>AVAudioSession</code>，不用统一管理，接AVAudioSession的通知即可；</li>
<li>根据app的应用场景合理选择<code>Category</code>；</li>
<li>在deactive时需要注意app的应用场景来合理的选择是否使用<code>NotifyOthersOnDeactivation</code>参数；</li>
<li>在处理InterruptEnd事件时需要注意<code>ShouldResume</code>的值。</li>
</ul>


<hr />

<h1>下篇预告</h1>

<p>下一篇将讲述如何使用AudioFileStreamer分离音频帧，以及如何使用AudioQueue进行播放。</p>

<hr />

<h1>参考资料</h1>

<p><a href="https://developer.apple.com/library/ios/documentation/Audio/Conceptual/AudioSessionProgrammingGuide/Introduction/Introduction.html#//apple_ref/doc/uid/TP40007875">AudioSession</a></p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[iOS音频播放 (一)：概述]]></title>
    <link href="http://msching.github.io/blog/2014/07/07/audio-in-ios/"/>
    <updated>2014-07-07T14:40:42+08:00</updated>
    <id>http://msching.github.io/blog/2014/07/07/audio-in-ios</id>
    <content type="html"><![CDATA[<h1>前言</h1>

<p>从事音乐相关的app开发也已经有一段时日了，在这过程中app的播放器几经修改我也因此对于iOS下的音频播放实现有了一定的研究。写这个系列的博客目的一方面希望能够抛砖引玉，另一方面也是希望能帮助国内其他的iOS开发者和爱好者少走弯路（我自己就遇到了不少的坑=。=）。</p>

<p>本篇为《iOS音频播放》系列的第一篇，主要将对iOS下实现音频播放的方法进行概述。</p>

<hr />

<h1>基础</h1>

<p>先来简单了解一下一些基础的音频知识。</p>

<p>目前我们在计算机上进行音频播放都需要依赖于音频文件，音频文件的生成过程是将声音信息采样、量化和编码产生的数字信号的过程，人耳所能听到的声音，最低的频率是从20Hz起一直到最高频率20KHZ，因此音频文件格式的最大带宽是20KHZ。根据<a href="http://zh.wikipedia.org/wiki/%E5%A5%88%E5%A5%8E%E6%96%AF%E7%89%B9%E9%A2%91%E7%8E%87">奈奎斯特</a>的理论，只有采样频率高于声音信号最高频率的两倍时，才能把数字信号表示的声音还原成为原来的声音，所以音频文件的采样率一般在40~50KHZ，比如最常见的CD音质采样率44.1KHZ。</p>

<p>对声音进行采样、量化过程被称为<a href="http://zh.wikipedia.org/wiki/%E8%84%88%E8%A1%9D%E7%B7%A8%E8%99%9F%E8%AA%BF%E8%AE%8A">脉冲编码调制</a>（Pulse Code Modulation），简称<code>PCM</code>。PCM数据是最原始的音频数据完全无损，所以PCM数据虽然音质优秀但体积庞大，为了解决这个问题先后诞生了一系列的音频格式，这些音频格式运用不同的方法对音频数据进行压缩，其中有无损压缩（ALAC、APE、FLAC）和有损压缩（MP3、AAC、OGG、WMA）两种。</p>

<p>目前最为常用的音频格式是MP3，MP3是一种有损压缩的音频格式，设计这种格式的目的就是为了大幅度的减小音频的数据量，它舍弃PCM音频数据中人类听觉不敏感的部分，从下面的比较图我们可以明显的看到MP3数据相比PCM数据明显矮了一截（图片引自<a href="http://bbs.imp3.net/thread-243641-1-1.html">imp3论坛</a>）。</p>

<p><img src="/images/iOS-audio/pcm.jpg" alt="上图为pcm数据" />
<img src="/images/iOS-audio/mp3.jpg" alt="上图为mp3数据" /></p>

<p>MP3格式中的码率（BitRate）代表了MP3数据的压缩质量，现在常用的码率有128kbit/s、160kbit/s、320kbit/s等等，这个值越高声音质量也就越高。MP3编码方式常用的有两种<a href="http://zh.wikipedia.org/wiki/%E5%9B%BA%E5%AE%9A%E7%A0%81%E7%8E%87">固定码率</a>(Constant bitrate，CBR)和<a href="http://zh.wikipedia.org/wiki/%E5%8F%AF%E5%8F%98%E7%A0%81%E7%8E%87">可变码率</a>(Variable bitrate，VBR)。</p>

<p>MP3格式中的数据通常由两部分组成，一部分为<a href="http://zh.wikipedia.org/zh/ID3">ID3</a>用来存储歌名、演唱者、专辑、音轨数等信息，另一部分为音频数据。音频数据部分以帧(frame)为单位存储，每个音频都有自己的帧头，如图所示就是一个MP3文件帧结构图（图片同样来自互联网）。MP3中的每一个帧都有自己的帧头，其中存储了码率、采样率等解码必须的信息，所以每一个帧都可以独立于文件存在和播放，这个特性加上高压缩比使得MP3文件成为了音频流播放的主流格式。</p>

<p><img src="/images/iOS-audio/mp3frame.jpg" alt="" /></p>

<hr />

<h1>iOS音频播放概述</h1>

<p>了解了基础概念之后我们就可以列出一个经典的音频播放流程（以MP3为例）：</p>

<ol>
<li>读取MP3文件</li>
<li>分离MP3中的音频帧</li>
<li>对分离出来的音频帧解码得到PCM数据</li>
<li>对PCM数据进行音效处理（均衡器、混响器等，非必须）</li>
<li>把PCM数据解码成音频信号</li>
<li>把音频信号交给硬件播放</li>
<li>重复1-6步直到播放完成</li>
</ol>


<p>在iOS系统中apple对上述的流程进行了封装并提供了不同层次的接口（图片引自<a href="https://developer.apple.com/library/ios/documentation/MusicAudio/Conceptual/CoreAudioOverview/CoreAudioEssentials/CoreAudioEssentials.html#//apple_ref/doc/uid/TP40003577-CH10-SW1">官方文档</a>）。</p>

<p><img src="/images/iOS-audio/api-architectural-layers.png" alt="CoreAudio的接口层次" /></p>

<p>下面对其中的中高层接口进行功能说明：</p>

<ul>
<li>Audio File Services：读写音频数据，可以完成播放流程中的第1、2步；</li>
<li>Audio File Stream Services：对音频进行解码，可以完成播放流程中的第2部；</li>
<li>Audio Converter services：音频数据转换，可以完成播放流程中的第3步；</li>
<li>Audio Processing Graph Services：音效处理模块，可以完成播放流程中的第4步；</li>
<li>Audio Unit Services：播放音频数据：可以完成播放流程中的第5步、第6步；</li>
<li>Extended Audio File Services：Audio File Services和Audio Converter services的结合体；</li>
<li>AVAudioPlayer/AVPlayer(AVFoundation)：高级接口，可以完成整个音频播放的过程（包括本地文件和网络流播放，第4步除外）；</li>
<li>Audio Queue Services：高级接口，可以进行录音和播放，可以完成播放流程中的第3、5、6步；</li>
<li>OpenAL：用于游戏音频播放，暂不讨论</li>
</ul>


<p>可以看到apple提供的接口类型非常丰富，可以满足各种类别类需求：</p>

<ul>
<li><p>如果你只是想实现音频的播放，没有其他需求AVFoundation会很好的满足你的需求。它的接口使用简单、不用关心其中的细节；</p></li>
<li><p>如果你的app需要对音频进行流播放并且同时存储，那么AudioFileStreamer加AudioQueue能够帮到你，你可以把音频数据下载到本地，用NSFileHandler读取本地音频文件并交给AudioFileStreamer分离音频帧，分离出来的音频帧可以送给AudioQueue进行解码和播放。如果是本地文件也可以直接用AudioFile读取文件并分离帧。（这两个都是比较直接的做法，这类需求也可以用AVFoundation+本地server的方式实现，AVAudioPlayer会把请求发送给本地server，由本地server转发出去，获取数据后在本地server中存储并转送给AVAudioPlayer。另一个比较trick的做法是先把音频下载到文件中，在下载到一定量的数据后把文件路径给AVAudioPlayer播放，当然这种做法在音频seek后就回有问题了。）；</p></li>
<li><p>如果你正在开发一个专业的音乐播放软件，需要对音频施加音效（均衡器、混响器），那么除了数据的读取以外还需要用到AudioConverter来把音频数据转换成PCM数据，再由AudioUnit+AUGraph来进行音效处理和播放（但目前多数带音效的app都是自己开发音效模块来坐PCM数据的处理，这部分功能自行开发在自定义性和扩展性上会比较强一些。PCM数据通过音效器处理完成后就可以使用AudioUnit播放了，当然AudioQueue也支持直接使对PCM数据进行播放。）。下图描述的就是使用AudioFile + AudioConverter + AudioUnit进行音频播放的流程（图片引自<a href="https://developer.apple.com/library/ios/documentation/MusicAudio/Conceptual/CoreAudioOverview/ARoadmaptoCommonTasks/ARoadmaptoCommonTasks.html#//apple_ref/doc/uid/TP40003577-CH6-SW1">官方文档</a>）。</p></li>
</ul>


<p><img src="/images/iOS-audio/audioUnitPlay.jpg" alt="" /></p>

<hr />

<h1>下篇预告</h1>

<p>下一篇将讲述iOS音频播放中必须面对的难（da）题（keng），AudioSession。</p>

<hr />

<h1>参考资料</h1>

<p><a href="http://zh.wikipedia.org/zh/%E9%9F%B3%E9%A2%91%E6%96%87%E4%BB%B6%E6%A0%BC%E5%BC%8F">音频文件格式</a></p>

<p><a href="http://zh.wikipedia.org/wiki/%E8%84%88%E8%A1%9D%E7%B7%A8%E8%99%9F%E8%AA%BF%E8%AE%8A">脉冲编码调制</a></p>

<p><a href="http://zh.wikipedia.org/zh/%E9%87%87%E6%A0%B7%E7%8E%87">采样率</a></p>

<p><a href="http://zh.wikipedia.org/wiki/%E5%A5%88%E5%A5%8E%E6%96%AF%E7%89%B9%E9%A2%91%E7%8E%87">奈奎斯特频率</a></p>

<p><a href="http://zh.wikipedia.org/wiki/MP3">MP3</a></p>

<p><a href="http://zh.wikipedia.org/zh/ID3">ID3</a></p>

<p><a href="https://developer.apple.com/library/ios/documentation/MusicAudio/Conceptual/CoreAudioOverview/CoreAudioEssentials/CoreAudioEssentials.html#//apple_ref/doc/uid/TP40003577-CH10-SW1">Core Audio Essential</a></p>

<p><a href="https://developer.apple.com/library/ios/documentation/MusicAudio/Conceptual/CoreAudioOverview/ARoadmaptoCommonTasks/ARoadmaptoCommonTasks.html#//apple_ref/doc/uid/TP40003577-CH6-SW1">Common Tasks in OS X</a></p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[iOS8beta1下WebCore可能会打断音频播放]]></title>
    <link href="http://msching.github.io/blog/2014/06/26/audio-interrput-by-webcore-in-ios-8-beta-1/"/>
    <updated>2014-06-26T16:09:56+08:00</updated>
    <id>http://msching.github.io/blog/2014/06/26/audio-interrput-by-webcore-in-ios-8-beta-1</id>
    <content type="html"><![CDATA[<h1>问题</h1>

<p>前不久在QA发现一个问题，在iOS8 beta1上使用我们的app播放歌曲时进入某些内嵌的web页面（UIWebview实现）时歌曲会暂停播放，但是界面仍然显示为正在播放状态。把真机连上Xcode6调试后发现在进入部分网页时会再console上打印如下log：</p>

<pre><code>AVAudioSession.mm:623: -[AVAudioSession setActive:withOptions:error:]: Deactivating an audio session that has running I/O. All I/O should be stopped or paused prior to deactivating the audio session.
</code></pre>

<p>bt后堆栈如下：</p>

<p>```</p>

<pre><code>frame #1: 0x299632fe libAVFAudio.dylib`-[AVAudioSession setActive:error:] + 26
frame #2: 0x3551b92e WebCore`WebCore::AudioSession::setActive(bool) + 62
frame #3: 0x35af2674 WebCore`WebCore::MediaSessionManager::updateSessionState() + 100
frame #4: 0x35af03b6 WebCore`WebCore::MediaSessionManager::addSession(WebCore::MediaSession&amp;) + 74
frame #5: 0x35af0002 WebCore`WebCore::MediaSession::MediaSession(WebCore::MediaSessionClient&amp;) + 38
frame #6: 0x35735a20 WebCore`WebCore::HTMLMediaSession::create(WebCore::MediaSessionClient&amp;) + 20
frame #7: 0x35724c68 WebCore`WebCore::HTMLMediaElement::HTMLMediaElement(WebCore::QualifiedName const&amp;, WebCore::Document&amp;, bool) + 976
frame #8: 0x3570ad24 WebCore`WebCore::HTMLAudioElement::create(WebCore::QualifiedName const&amp;, WebCore::Document&amp;, bool) + 36
frame #9: 0x35718184 WebCore`WebCore::audioConstructor(WebCore::QualifiedName const&amp;, WebCore::Document&amp;, WebCore::HTMLFormElement*, bool) + 56
frame #10: 0x3571803a WebCore`WebCore::HTMLElementFactory::createElement(WebCore::QualifiedName const&amp;, WebCore::Document&amp;, WebCore::HTMLFormElement*, bool) + 230
frame #11: 0x3533a26c WebCore`WebCore::HTMLDocument::createElement(WTF::AtomicString const&amp;, int&amp;) + 88
frame #12: 0x3533a1ae WebCore`WebCore::jsDocumentPrototypeFunctionCreateElement(JSC::ExecState*) + 242
frame #13: 0x2c1cc4d4 JavaScriptCore`llint_entry + 21380
</code></pre>

<p>```</p>

<p>发现是WebCore调用了<code>AVAudioSession</code>的setActive方法，并且把active置为了NO。这个过程其实类似于音乐在播放时被其他事件打断（例如电话、siri）一样，audio会被打断，同时会发送<code>kAudioSessionBeginInterruption</code>事件通知app音频播放已经被打断，需要修正播放器和UI状态；打断结束后回发送<code>kAudioSessionEndInterruption</code>事件通知app恢复播放状态。区别在于WebCore的打断并没有任何通知，所以就导致界面上的播放状态为播放中而实际音乐却被打断。</p>

<h1>适配</h1>

<p>接下来就要对这个问题进行适配了：</p>

<ol>
<li>首先，联系了前段组的同事对出现问题的页面进行检查，之后被告知是某个页面的js中调用了一些播放相关的代码导致了这个问题，这些js是之前版本中使用的，现在已经被废弃但没有及时的删除。在删除这些js后，问题自然就消失了。</li>
<li>客户端本身也应该做一些适配来防止下次再有页面出现类似问题，目前我能想到的办法是做一个<code>AVAudioSession</code>的category，method swizzle方法<code>setActive:withOptions:error:</code>在设置active值时发送通知来修改UI的状态。</li>
</ol>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[初始化AudioUnit的正确姿势]]></title>
    <link href="http://msching.github.io/blog/2014/06/25/init-audiounit-when-app-is-in-background/"/>
    <updated>2014-06-25T13:56:27+08:00</updated>
    <id>http://msching.github.io/blog/2014/06/25/init-audiounit-when-app-is-in-background</id>
    <content type="html"><![CDATA[<p>在使用AudioUnit的过程中发现当app在后台时调用<code>extern OSStatus AudioUnitInitialize(AudioUnit inUnit)</code>方法返回<code>561015905</code>错误码，解析成string后是<code>!pla</code>，google错误码后毫无收获，于是只能workaround。面对这个问题我的workaround是当出现初始化失败的情况下会在程序进入前台时再尝试调用<code>AudioUnitInitialize</code>方法来初始化AudioUnit。至此问题已经在一定程度上得到了解决，只要用户进入前台就可以正确初始化AudioUnit并且播放音乐。</p>

<p>今天在应对某个用户反馈时发现该用户在使用remoteControl过程中无法启动播放的情况正是因为后台init AudioUnit会失败导致程序无法如预期工作。于是灵光一闪，觉得在初始化AudioUnit之前先调用<code>AudioSessionInitialize</code>并setActive是否就可以解决问题。尝试之后发现果然可以&hellip;（之前都在AudioUnitInitialize成功后才去init audiosession）。</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[AVAudioPlayer的1937337955错误研究]]></title>
    <link href="http://msching.github.io/blog/2014/05/04/secret-of-avaudioplayer/"/>
    <updated>2014-05-04T19:46:53+08:00</updated>
    <id>http://msching.github.io/blog/2014/05/04/secret-of-avaudioplayer</id>
    <content type="html"><![CDATA[<h1>问题</h1>

<p>前两天公司有一位同事在使用AVAudioPlayer的过程中遇到了这样一个问题：</p>

<p>他需要播放一段网络上的音频，实现策略是把音频下载到本地，然后使用AVAudioPlayer进行播放。代码大致是这样的：</p>

<p><code>objective-c
NSString *path = .../xxx.mp3; //mp3 file path
NSData *data = [NSData dataWithContentsOfURL:[NSURL fileURLWithPath:path]];
NSError *error = nil;
AVAudioPlayer *player = [[AVAudioPlayer alloc] initWithData:data error:&amp;error];
</code></p>

<p>但他在init AVAudioPlayer时遇到了下面的错误。</p>

<p><code>objective-c
Error Domain=NSOSStatusErrorDomain Code=1937337955 "The operation couldn’t be completed. (OSStatus error 1937337955.)"
</code></p>

<hr />

<h1>普遍的解决方法</h1>

<p>在google搜索之后发现1937337955这个code并不少见，在<a href="http://stackoverflow.com/questions/4901709/iphone-avaudioplayer-unsupported-file-type">StackOverflow</a>上有人提问问到这个问题，国内的一些博客中也有提到（例如<a href="http://zhu340381425.blog.163.com/blog/static/75952514201192021013852">@我的桌子</a>和<a href="http://blog.sina.com.cn/s/blog_7cb9b3b80101d8ap.html">@SkyLine</a>）。</p>

<p>其中提到的解决方法都一样，就是使用AVAudioPlayer的另一个init方法：</p>

<p><code>objective-c
- (id)initWithContentsOfURL:(NSURL *)url error:(NSError **)outError;
</code>
于是尝试修改了代码：</p>

<p><code>objective-c
NSString *path = .../xxx.mp3; //mp3 file path
NSError *error = nil;
AVAudioPlayer *player = [[AVAudioPlayer alloc] initWithContentsOfURL:[NSURL fileURLWithPath:path] error:&amp;error];
</code>
果然没有出现错误，player成功创建并且能够播放。</p>

<hr />

<h1>深究</h1>

<p>不能播放的问题到这里已经fix了，但这个问题本身还没有完结，为什么使用<code>-initWithContentsOfURL:error:</code>方法就可以播呢？这个时候也许有的人会认为这是一个apple的bug，认为<code>-initWithContentsOfURL:error:</code>方法比<code>-initWithData::error:</code>具有更好的适应性。</p>

<pre><code>Oh that's very interesting. Perhaps that should be submitted to Apple as a bug? 
In the end I opted for the saved file anyways because it fit better with what we were trying to do. 
Thanks for the Tip! –  mtmurdock Mar 19 '11 at 0:26
</code></pre>

<p>但凡事遇到错误，都应该先从自身开始找原因。经过搜索发现，1937337955这个Errorcode其实就是<code>kAudioFileUnsupportedFileTypeError</code>，一般出现在文件格式不符合规范的情况下。假设apple并没有写出bug的话，那么上述问题中的这个mp3一定在文件格式上有缺陷，最终导致了<code>-initWithData::error:</code>方法返回错误，而<code>-initWithContentsOfURL:error:</code>采用某种方式规避了这个格式缺陷。</p>

<p>回过头去查看<code>AVAudioPlayer.h</code>头文件可以看到SDK7中多了两个init方法：</p>

<p><code>objective-c
/* The file type hint is a constant defined in AVMediaFormat.h whose value is a UTI for a file format. e.g. AVFileTypeAIFF. */
/* Sometimes the type of a file cannot be determined from the data, or it is actually corrupt.
The file type hint tells the parser what kind of data to look for so that files which are not self identifying or possibly even corrupt can be successfully parsed. */
- (id)initWithContentsOfURL:(NSURL *)url fileTypeHint:(NSString*)utiString error:(NSError **)outError NS_AVAILABLE(10_9, 7_0);
- (id)initWithData:(NSData *)data fileTypeHint:(NSString*)utiString error:(NSError **)outError NS_AVAILABLE(10_9, 7_0);
</code></p>

<p>多出来的这个hint参数和<code>AudioToolbox</code>framework中<code>AudioFileStream</code>、<code>AudioFile</code>两个类的Open方法中所使用的hint参数作用一样，可以辅助系统判定当前的文件格式。</p>

<p>接下来尝试在iOS7系统下使用新的init方法生成AVAudioPlayer：</p>

<p><code>objective-c
NSString *path = .../xxx.mp3; //mp3 file path
NSData *data = [NSData dataWithContentsOfURL:[NSURL fileURLWithPath:path]];
NSError *error = nil;
AVAudioPlayer *player = [[AVAudioPlayer alloc] initWithData:data fileTypeHint:AVFileTypeMPEGLayer3 error:&amp;error];
</code></p>

<p>结果没有错误产生，没有出现错误，player成功创建并且能够播放。</p>

<p>进而进行另两个实验：</p>

<ol>
<li>把保存文件时的.mp3后缀去掉后使用<code>-initWithContentsOfURL:error:</code>生成对象，结果发现产生了错误。</li>
<li>把保存文件时的.mp3后缀改为.wav后使用<code>-initWithContentsOfURL:error:</code>生成对象，结果发现产生了错误。</li>
</ol>


<p>于是确定<code>-initWithContentsOfURL:error:</code>方法是利用后缀名作为hintType对文件的解码进行辅助，而<code>-initWithData::error:</code>方法由于没有任何hint并且文件本身格式又有缺陷导致错误的产生。</p>

<hr />

<h1>更完整的解决方法</h1>

<p>基于以上分析可以得出一个更为完整的解决方法，可以有效的规避这一类错误：</p>

<ol>
<li>对于iOS7以上的系统（含iOS7）,在确定文件格式的情况下可以使用<code>initWithData:fileTypeHint:error:</code>和<code>initWithContentsOfURL:fileTypeHint:error:</code>生成实例，或者把data保存为对应后缀名的文件后使用<code>-initWithContentsOfURL:error:</code>后再生成实例；</li>
<li>对于iOS7以下的系统，在确定文件格式的情况下，最为安全的方法是把data保存为对应后缀名的文件后使用<code>-initWithContentsOfURL:error:</code>生成实例；</li>
</ol>


<p>如果上述方法帮不了你，那么就只能去检查文件格式有没有问题或者采用其他的实现方式进行尝试了（比如<code>AVPlayer</code>和<code>AudioToolBox</code>）。不管怎么说，客户端所能做的只是尽量减少错误发生的频率，最终解决这类问题还是需要音频文件的提供者确保音频文件的格式符合标准没有错误和缺陷。</p>
]]></content>
  </entry>
  
</feed>
