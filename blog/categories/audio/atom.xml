<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Category: Audio | 码农人生]]></title>
  <link href="http://msching.github.io/blog/categories/audio/atom.xml" rel="self"/>
  <link href="http://msching.github.io/"/>
  <updated>2014-07-20T16:50:43+08:00</updated>
  <id>http://msching.github.io/</id>
  <author>
    <name><![CDATA[ChengYinZju]]></name>
    
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[iOS音频播放 (四)：AudioFile]]></title>
    <link href="http://msching.github.io/blog/2014/07/19/audio-in-ios-4/"/>
    <updated>2014-07-19T13:38:30+08:00</updated>
    <id>http://msching.github.io/blog/2014/07/19/audio-in-ios-4</id>
    <content type="html"><![CDATA[<h1>前言</h1>

<p>接着<a href="/blog/2014/07/09/audio-in-ios-3/">第三篇</a>的<code>AudioStreamFile</code>这一篇要来聊一下<code>AudioFile</code>。和<code>AudioStreamFile</code>一样<code>AudioFile</code>是<code>AudioToolBox</code> framework中的一员，它也能够完成<a href="/blog/2014/07/07/audio-in-ios/">第一篇</a>所述的第2步，读取音频格式信息和进行帧分离，但事实上它的功能远不止如此。</p>

<hr />

<h1>AudioFile介绍</h1>

<p>按照<a href="https://developer.apple.com/library/mac/documentation/musicaudio/reference/AudioFileConvertRef/Reference/reference.html#//apple_ref/c/func/AudioFileCreateWithURL">官方文档</a>的描述：</p>

<p><code>a C programming interface that enables you to read or write a wide variety of audio data to or from disk or a memory buffer.With Audio File Services you can:</code></p>

<ul>
<li>Create, initialize, open, and close audio files</li>
<li>Read and write audio files</li>
<li>Optimize audio files</li>
<li>Work with user data and global information</li>
</ul>


<p>这个类可以用来创建、初始化音频文件；读写音频数据；对音频文件进行优化；读取和写入音频格式信息等等，功能十分强大，可见它不但可以用来支持音频播放，甚至可以用来生成音频文件。当然，在本篇文章中只会涉及一些和音频播放相关的内容（打开音频文件、读取格式信息、读取音频数据，其实我也只对这些方法有一点了解，其余的功能没用过。。>_&lt;）.</p>

<hr />

<h1>AudioFile的打开“姿势”</h1>

<p><code>AudioFile</code>提供了两个打开文件的方法：</p>

<p>1、 <code>AudioFileOpenURL</code></p>

<p>```objc</p>

<p>enum {</p>

<pre><code>kAudioFileReadPermission      = 0x01,
kAudioFileWritePermission     = 0x02,
kAudioFileReadWritePermission = 0x03
</code></pre>

<p>};</p>

<p>extern OSStatus AudioFileOpenURL (CFURLRef inFileRef,</p>

<pre><code>                                SInt8 inPermissions,
                                AudioFileTypeID inFileTypeHint,
                                AudioFileID * outAudioFile);
</code></pre>

<p>```</p>

<p>从方法的定义上来看是用来读取本地文件的：</p>

<p>第一个参数，文件路径；</p>

<p>第二个参数，文件的允许使用方式，是读、写还是读写，如果打开文件后进行了允许使用方式以外的操作，就得到<code>kAudioFilePermissionsError</code>错误码（比如Open时声明是<code>kAudioFileReadPermission</code>但却调用了<code>AudioFileWriteBytes</code>）；</p>

<p>第三个参数，和<code>AudioFileStream</code>的open方法中一样是一个帮助<code>AudioFile</code>解析文件的类型提示，如果文件类型确定的话应当传入；</p>

<p>第四个参数，返回AudioFile实例对应的<code>AudioFileID</code>，这个ID需要保存起来作为后续一些方法的参数使用；</p>

<p>返回值用来判断是否成功打开文件（OSSStatus == noErr）。</p>

<hr />

<p>2、 <code>AudioFileOpenWithCallbacks</code></p>

<p>```objc
extern OSStatus AudioFileOpenWithCallbacks (void * inClientData,</p>

<pre><code>                                          AudioFile_ReadProc inReadFunc,
                                          AudioFile_WriteProc inWriteFunc,
                                          AudioFile_GetSizeProc inGetSizeFunc,
                                          AudioFile_SetSizeProc inSetSizeFunc,
                                          AudioFileTypeID inFileTypeHint,
                                          AudioFileID * outAudioFile);
</code></pre>

<p>```</p>

<p>看过第一个Open方法后，这个方法乍看上去让人有点迷茫，没有URL的参数如何告诉AudioFile该打开哪个文件？还是先来看一下参数的说明吧：</p>

<p>第一个参数，上下文信息，不再多做解释；</p>

<p>第二个参数，当<code>AudioFile</code>需要读音频数据时进行的回调（调用Open和Read方式后<code>同步</code>回调）；</p>

<p>第三个参数，当<code>AudioFile</code>需要写音频数据时进行的回调（写音频文件功能时使用，暂不讨论）；</p>

<p>第四个参数，当<code>AudioFile</code>需要用到文件的总大小时回调（调用Open和Read方式后<code>同步</code>回调）；</p>

<p>第五个参数，当<code>AudioFile</code>需要设置文件的大小时回调（写音频文件功能时使用，暂不讨论）；</p>

<p>第六、七个参数和返回值同<code>AudioFileOpenURL</code>方法；</p>

<p>这个方法的重点在于<code>AudioFile_ReadProc</code>这个回调。换一个角度理解，这个方法相比于第一个方法自由度更高，AudioFile需要的只是一个数据源，无论是磁盘上的文件、内存里的数据甚至是网络流只要能在<code>AudioFile</code>需要数据时（Open和Read时）通过<code>AudioFile_ReadProc</code>回调为AudioFile提供合适的数据就可以了，也就是说使用方法不仅仅可以读取本地文件也可以如<code>AudioFileStream</code>一样以流的形式读取数据。</p>

<hr />

<p>下面来看一下<code>AudioFile_GetSizeProc</code>和<code>AudioFile_ReadProc</code>这两个读取功能相关的回调
```objc
typedef SInt64 (*AudioFile_GetSizeProc)(void * inClientData);</p>

<p>typedef OSStatus (*AudioFile_ReadProc)(void * inClientData,</p>

<pre><code>                                     SInt64 inPosition,
                                     UInt32 requestCount,
                                     void * buffer, 
                                     UInt32 * actualCount);
</code></pre>

<p>```</p>

<p>首先是<code>AudioFile_GetSizeProc</code>回调，这个回调很好理解，返回文件总长度即可，总长度的获取途径自然是文件系统或者httpResponse等等。</p>

<p>接下来是<code>AudioFile_ReadProc</code>回调：</p>

<p>第一个参数，上下文对象，不再赘述；</p>

<p>第二个参数，需要读取第几个字节开始的数据；</p>

<p>第三个参数，需要读取的数据长度；</p>

<p>第四个参数，返回参数，是一个数据指针并且其空间已经被分配，我们需要做的是把数据memcpy到buffer中；</p>

<p>第五个参数，实际提供的数据长度，即memcpy到buffer中的数据长度；</p>

<p>返回值，如果没有任何异常产生就返回noErr，如果有异常可以根据异常类型选择需要的error常量返回（一般用不到其他返回值，返回noErr就足够了）；</p>

<p>这里需要解释一下这个回调方法的工作方式。<code>AudioFile</code>需要数据时会调用回调方法，需要数据的时间点有两个：</p>

<ol>
<li><p>Open方法调用时，由于<code>AudioFile</code>的Open方法调用过程中就会对音频格式信息进行解析，只有符合要求的音频格式才能被成功打开否则Open方法就会返回错误码（换句话说，Open方法一旦调用成功就相当于<code>AudioStreamFile</code>在Parse后返回<code>ReadyToProducePackets</code>一样，只要Open成功就可以开始读取音频数据，详见<a href="/blog/2014/07/09/audio-in-ios-3/">第三篇</a>），所以在Open方法调用的过程中就需要提供一部分音频数据来进行解析；</p></li>
<li><p>Read相关方法调用时，这个不需要多说很好理解；</p></li>
</ol>


<p>通过回调提供数据时需要注意inPosition和requestCount参数，这两个参数指明了本次回调需要提供的数据范围是从inPosition开始requestCount个字节的数据。这里又可以分为两种情况：</p>

<ol>
<li><p>有充足的数据：那么我们需要把这个范围内的数据拷贝到buffer中，并且给actualCount赋值requestCount，最后返回noError；</p></li>
<li><p>数据不足：没有充足数据的话就只能把手头有的数据拷贝到buffer中，需要注意的是这部分被拷贝的数据必须是从inPosition开始的<code>连续数据</code>，拷贝完成后给actualCount赋值实际拷贝进buffer中的数据长度后返回noErr，这个过程可以用下面的代码来表示：</p></li>
</ol>


<p>```objc
static OSStatus MyAudioFileReadCallBack(void *inClientData,</p>

<pre><code>                                    SInt64 inPosition,
                                    UInt32 requestCount,
                                    void *buffer,
                                    UInt32 *actualCount)
</code></pre>

<p>{</p>

<pre><code>__unsafe_unretained MyContext *context = (__bridge MyContext *)inClientData;

*actualCount = [context availableDataLengthAtOffset:inPosition maxLength:requestCount];
if (*actualCount &gt; 0)
{
    NSData *data = [context dataAtOffset:inPosition length:*actualCount];
    memcpy(buffer, [data bytes], [data length]);
}

return noErr;
</code></pre>

<p>}</p>

<p>```</p>

<p>说到这里又需要分两种情况：</p>

<p>2.1. Open方法调用时的回调数据不足：AudioFile的Open方法会根据文件格式类型分几步进行数据读取以解析确定是否是一个合法的文件格式，其中每一步的inPosition和requestCount都不一样，如果某一步不成功就会直接进行下一步，如果几部下来都失败了，那么Open方法就会失败。简单的说就是在调用Open之前首先需要保证音频文件的格式信息完整，这就意味着<code>AudioFile</code>并不能独立用于音频流的读取，在流播放时首先需要使用<code>AudioStreamFile</code>来得到<code>ReadyToProducePackets</code>标志位来保证信息完整；</p>

<p>2.2. Read方法调用时的回调数据不足：这种情况下inPosition和requestCount的数值与Read方法调用时传入的参数有关，数据不足对于Read方法本身没有影响，只要回调返回noErr，Read就成功，只是实际交给Read方法的调用方的数据会不足，那么就把这个问题的处理交给了Read的调用方；</p>

<hr />

<h1>读取音频格式信息</h1>

<p>成功打开音频文件后就可以读取其中的格式信息了，读取用到的方法如下：</p>

<p>```objc
extern OSStatus AudioFileGetPropertyInfo(AudioFileID inAudioFile,</p>

<pre><code>                                       AudioFilePropertyID inPropertyID,
                                       UInt32 * outDataSize,
                                       UInt32 * isWritable);
</code></pre>

<p>extern OSStatus AudioFileGetProperty(AudioFileID inAudioFile,</p>

<pre><code>                                   AudioFilePropertyID inPropertyID,
                                   UInt32 * ioDataSize,
                                   void * outPropertyData); 
</code></pre>

<p><code>``
</code>AudioFileGetPropertyInfo<code>方法用来获取某个属性对应的数据的大小（outDataSize）以及该属性是否可以被write（isWritable），而</code>AudioFileGetProperty<code>则用来获取属性对应的数据。对于一些大小可变的属性需要先使用</code>AudioFileGetPropertyInfo<code>获取数据大小才能取获取数据（例如formatList），而有些确定类型单个属性则不必先调用</code>AudioFileGetPropertyInfo<code>直接调用</code>AudioFileGetProperty`即可（比如BitRate），例子如下：</p>

<p>```objc
AudioFileID fileID; //Open方法返回的AudioFileID</p>

<p>//获取格式信息
UInt32 formatListSize = 0;
OSStatus status = AudioFileGetPropertyInfo(_fileID, kAudioFilePropertyFormatList, &amp;formatListSize, NULL);
if (status == noErr)
{</p>

<pre><code>AudioFormatListItem *formatList = (AudioFormatListItem *)malloc(formatListSize);
status = AudioFileGetProperty(fileID, kAudioFilePropertyFormatList, &amp;formatListSize, formatList);
if (status == noErr)
{
    for (int i = 0; i * sizeof(AudioFormatListItem) &lt; formatListSize; i += sizeof(AudioFormatListItem))
    {
        AudioStreamBasicDescription pasbd = formatList[i].mASBD;
        //选择需要的格式。。                             
    }
}
free(formatList);
</code></pre>

<p>}</p>

<p>//获取码率
UInt32 bitRate;
UInt32 bitRateSize = sizeof(bitRate);
status = AudioFileGetProperty(fileID, kAudioFilePropertyBitRate, &amp;size, &amp;bitRate);
if (status != noErr)
{</p>

<pre><code>//错误处理
</code></pre>

<p>}
```
可以获取的属性有下面这些，大家可以参考文档来获取自己需要的信息（注意到这里有EstimatedDuration，可以得到Duration了）：</p>

<p>```
enum
{</p>

<pre><code>kAudioFilePropertyFileFormat                =   'ffmt',
kAudioFilePropertyDataFormat                =   'dfmt',
kAudioFilePropertyIsOptimized           =   'optm',
kAudioFilePropertyMagicCookieData       =   'mgic',
kAudioFilePropertyAudioDataByteCount        =   'bcnt',
kAudioFilePropertyAudioDataPacketCount  =   'pcnt',
kAudioFilePropertyMaximumPacketSize     =   'psze',
kAudioFilePropertyDataOffset                =   'doff',
kAudioFilePropertyChannelLayout         =   'cmap',
kAudioFilePropertyDeferSizeUpdates      =   'dszu',
kAudioFilePropertyMarkerList                =   'mkls',
kAudioFilePropertyRegionList                =   'rgls',
kAudioFilePropertyChunkIDs              =   'chid',
kAudioFilePropertyInfoDictionary            =   'info',
kAudioFilePropertyPacketTableInfo       =   'pnfo',
kAudioFilePropertyFormatList                =   'flst',
kAudioFilePropertyPacketSizeUpperBound      =   'pkub',
kAudioFilePropertyReserveDuration       =   'rsrv',
kAudioFilePropertyEstimatedDuration     =   'edur',
kAudioFilePropertyBitRate               =   'brat',
kAudioFilePropertyID3Tag                    =   'id3t',
kAudioFilePropertySourceBitDepth            =   'sbtd',
kAudioFilePropertyAlbumArtwork          =   'aart',
</code></pre>

<p>  kAudioFilePropertyAudioTrackCount         =    &lsquo;atct&rsquo;,</p>

<pre><code>kAudioFilePropertyUseAudioTrack         =   'uatk'
</code></pre>

<p>};<br/>
```</p>

<hr />

<h1>读取音频数据</h1>

<p>读取音频数据的方法分为两类：</p>

<p>1、直接读取音频数据：</p>

<p>```objc
extern OSStatus AudioFileReadBytes (AudioFileID inAudioFile,</p>

<pre><code>                                  Boolean inUseCache,
                                  SInt64 inStartingByte,
                                  UInt32 * ioNumBytes,
                                  void * outBuffer);
</code></pre>

<p>```</p>

<p>第一个参数，FileID；</p>

<p>第二个参数，是否需要cache，一般来说传false；</p>

<p>第三个参数，从第几个byte开始读取数据</p>

<p>第四个参数，这个参数在调用时作为输入参数表示需要读取读取多少数据，调用完成后作为输出参数表示实际读取了多少数据（即Read回调中的requestCount和actualCount）；</p>

<p>第五个参数，buffer指针，需要事先分配好足够大的内存（ioNumBytes大，即Read回调中的buffer，所以Read回调中不需要再分配内存）；</p>

<p>返回值表示是否读取成功，EOF时会返回<code>kAudioFileEndOfFileError</code>；</p>

<p>使用这个方法得到的数据都是没有进行过帧分离的数据，如果想要用来播放或者解码还必须通过<code>AudioFileStream</code>进行帧分离；</p>

<p>2、按帧（Packet）读取音频数据：</p>

<p>```objc
extern OSStatus AudioFileReadPacketData (AudioFileID inAudioFile,</p>

<pre><code>                                       Boolean inUseCache,
                                       UInt32 * ioNumBytes,
                                       AudioStreamPacketDescription * outPacketDescriptions,
                                       SInt64 inStartingPacket,
                                       UInt32 * ioNumPackets,
                                       void * outBuffer);
</code></pre>

<p>extern OSStatus AudioFileReadPackets (AudioFileID inAudioFile,</p>

<pre><code>                                    Boolean inUseCache,
                                    UInt32 * outNumBytes,
                                    AudioStreamPacketDescription * outPacketDescriptions,
                                    SInt64 inStartingPacket, 
                                    UInt32 * ioNumPackets, 
                                    void * outBuffer);
</code></pre>

<p>```
按帧读取的方法有两个，这两个方法看上去差不多，就连参数也几乎相同，但使用场景和效率上却有所不同，<a href="https://developer.apple.com/library/mac/documentation/musicaudio/reference/AudioFileConvertRef/Reference/reference.html#//apple_ref/c/func/AudioFileCreateWithURL">官方文档</a>中如此描述这两个方法：</p>

<ul>
<li><code>AudioFileReadPacketData</code> is memory efficient when reading variable bit-rate (VBR) audio data;</li>
<li><code>AudioFileReadPacketData</code> is more efficient than <code>AudioFileReadPackets</code> when reading compressed file formats that do not have packet tables, such as MP3 or ADTS. This function is a good choice for reading either CBR (constant bit-rate) or VBR data if you do not need to read a fixed duration of audio.</li>
<li>Use <code>AudioFileReadPackets</code> only when you need to read a fixed duration of audio data, or when you are reading only uncompressed audio.</li>
</ul>


<p>只有当需要读取固定时长音频或者非压缩音频时才会用到<code>AudioFileReadPackets</code>，其余时候使用<code>AudioFileReadPacketData</code>会有更高的效率并且更省内存；</p>

<p>下面来看看这些参数：</p>

<p>第一、二个参数，同<code>AudioFileReadBytes</code>；</p>

<p>第三个参数，对于<code>AudioFileReadPacketData</code>来说ioNumBytes这个参数在输入输出时都要用到，在输入时表示outBuffer的size，输出时表示实际读取了多少size的数据。而对<code>AudioFileReadPackets</code>来说outNumBytes只在输出时使用，表示实际读取了多少size的数据；</p>

<p>第四个参数，帧信息数组指针，在输入前需要分配内存，大小必须足够存在ioNumPackets个帧信息（ioNumPackets * sizeof(AudioStreamPacketDescription)）；</p>

<p>第五个参数，在输入时表示需要读取多少个帧，在输出时表示实际读取了多少帧；</p>

<p>第六个参数，outBuffer数据指针，在输入前就需要分配好空间，这个参数看上去两个方法一样但其实并非如此。对于<code>AudioFileReadPacketData</code>来说只要分配<code>近似帧大小 * 帧数</code>的内存空间即可，方法本身会针对给定的内存空间大小来决定最后输出多少个帧，如果空间不够会适当减少出的帧数；而对于<code>AudioFileReadPackets</code>来说则需要分配<code>最大帧大小(或帧大小上界) * 帧数</code>的内存空间才行（最大帧大小和帧大小上界的区别等下会说）；这也就是为何第三个参数一个是输入输出双向使用的，而另一个只是输出时使用的原因。就这点来说两个方法中前者在使用的过程中要比后者更省内存；</p>

<p>返回值，同<code>AudioFileReadBytes</code>；</p>

<p>这两个方法读取后的数据为帧分离后的数据，可以直接用来播放或者解码。</p>

<p>下面给出两个方法的使用代码（以MP3为例）：</p>

<p>```objc
AudioFileID fileID; //Open方法返回的AudioFileID
UInt32 ioNumPackets = &hellip;; //要读取多少个packet
SInt64 inStartingPacket = &hellip;; //从第几个Packet开始读取</p>

<p>UInt32 bitRate = &hellip;; //AudioFileGetProperty读取kAudioFilePropertyBitRate
UInt32 sampleRate = &hellip;; //AudioFileGetProperty读取kAudioFilePropertyDataFormat或kAudioFilePropertyFormatList
UInt32 byteCountPerPacket = 144 * bitRate / sampleRate; //MP3数据每个Packet的近似大小</p>

<p>UInt32 descSize = sizeof(AudioStreamPacketDescription) * ioNumPackets;
AudioStreamPacketDescription * outPacketDescriptions = (AudioStreamPacketDescription *)malloc(descSize);</p>

<p>UInt32 ioNumBytes = byteCountPerPacket * ioNumPackets;
void * outBuffer = (void *)malloc(ioNumBytes);</p>

<p>OSStatus status = AudioFileReadPacketData(fileID,</p>

<pre><code>                                        false, 
                                        &amp;ioNumBytes, 
                                        outPacketDescriptions, 
                                        inStartingPacket, 
                                        &amp;ioNumPackets, 
                                        outBuffer);
</code></pre>

<p>```</p>

<p>```objc
AudioFileID fileID; //Open方法返回的AudioFileID
UInt32 ioNumPackets = &hellip;; //要读取多少个packet
SInt64 inStartingPacket = &hellip;; //从第几个Packet开始读取</p>

<p>UInt32 maxByteCountPerPacket = &hellip;; //AudioFileGetProperty读取kAudioFilePropertyMaximumPacketSize，最大的packet大小
//也可以用：
//UInt32 byteCountUpperBoundPerPacket = &hellip;; //AudioFileGetProperty读取kAudioFilePropertyPacketSizeUpperBound，当前packet大小上界（未扫描全文件的情况下）</p>

<p>UInt32 descSize = sizeof(AudioStreamPacketDescription) * ioNumPackets;
AudioStreamPacketDescription * outPacketDescriptions = (AudioStreamPacketDescription *)malloc(descSize);</p>

<p>UInt32 outNumBytes = 0；
UInt32 ioNumBytes = maxByteCountPerPacket * ioNumPackets;
void * outBuffer = (void *)malloc(ioNumBytes);</p>

<p>OSStatus status = AudioFileReadPackets(fileID,</p>

<pre><code>                                   false,
                                   &amp;outNumBytes,
                                   outPacketDescriptions,
                                   inStartingPacket,
                                   &amp;ioNumPackets,
                                   outBuffer);
</code></pre>

<p>```</p>

<hr />

<h1>Seek</h1>

<p>seek的思路和之前讲<code>AudioFileStream</code>时讲到的是一样的，区别在于AudioFile没有方法来帮助修正seek的offset和seek的时间：</p>

<ul>
<li>使用<code>AudioFileReadBytes</code>时需要计算出approximateSeekOffset</li>
<li>使用<code>AudioFileReadPacketData</code>或者<code>AudioFileReadPackets</code>时需要计算出seekToPacket</li>
</ul>


<p>approximateSeekOffset和seekToPacket的计算方法参见<a href="/blog/2014/07/09/audio-in-ios-3/">第三篇</a>。</p>

<hr />

<h1>关闭AudioFile</h1>

<p><code>AudioFile</code>使用完毕后需要调用<code>AudioFileClose</code>进行关闭，没啥特别需要注意的。</p>

<p><code>objc
extern OSStatus AudioFileClose (AudioFileID inAudioFile);   
</code></p>

<hr />

<h1>小结</h1>

<p>本篇针对<code>AudioFile</code>的音频读取功能做了介绍，小结一下：</p>

<ul>
<li><p><code>AudioFile</code>有两个Open方法，需要针对自身的使用场景选择不同的方法；</p></li>
<li><p><code>AudioFileOpenURL</code>用来读取本地文件</p></li>
<li><p><code>AudioFileOpenWithCallbacks</code>的使用场景比前者要广泛，使用时需要注意<code>AudioFile_ReadProc</code>，这个回调方法在Open方法本身和Read方法被调用时会被<code>同步</code>调用</p></li>
<li><p>必须保证音频文件格式信息可读时才能使用<code>AudioFile</code>的Open方法，AudioFile并不能独立用于音频流的读取，需要配合<code>AudioStreamFile</code>使用才能读取流（需要用<code>AudioStreamFile</code>来判断文件格式信息可读之后再调用Open方法）；</p></li>
<li><p>使用<code>AudioFileGetProperty</code>读取格式信息时需要判断所读取的信息是否需要先调用<code>AudioFileGetPropertyInfo</code>获得数据大小后再进行读取；</p></li>
<li><p>读取音频数据应该根据使用的场景选择不同的音频读取方法，对于不同的读取方法seek时需要计算的变量也不相同；</p></li>
<li><p><code>AudioFile</code>使用完毕后需要调用<code>AudioFileClose</code>进行关闭；</p></li>
</ul>


<hr />

<h1>示例代码</h1>

<p>对于本地文件用AudioFile读取比较简单就不在这里提供demo了，对于流播放中的AudioFile使用推荐大家阅读豆瓣的开源播放器代码<a href="https://github.com/douban/DOUAudioStreamer">DOUAudioStreamer</a>。</p>

<hr />

<h1>下篇预告</h1>

<p>下一篇将讲述如何使用<code>AudioQueue</code>。</p>

<hr />

<h1>参考资料</h1>

<p><a href="https://developer.apple.com/library/mac/documentation/musicaudio/reference/AudioFileConvertRef/Reference/reference.html#//apple_ref/c/func/AudioFileCreateWithURL">Audio File Services Reference</a></p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[iOS音频播放 (三)：AudioFileStream]]></title>
    <link href="http://msching.github.io/blog/2014/07/09/audio-in-ios-3/"/>
    <updated>2014-07-09T11:31:28+08:00</updated>
    <id>http://msching.github.io/blog/2014/07/09/audio-in-ios-3</id>
    <content type="html"><![CDATA[<h1>前言</h1>

<p>本来说好是要在第三篇中讲<code>AudioFileStream</code>和<code>AudioQueue</code>，但写着写着发现光<code>AudioFileStream</code>就好多内容，最后还是决定分篇介绍，这篇先来说一下<code>AudioFileStream</code>，下一篇计划说一下和<code>AudioFileStream</code>类似的<code>AudioFile</code>，下下篇再来说<code>AudioQueue</code>。</p>

<p>在本篇那种将会提到计算音频时长duration和音频seek的方法，这些方法对于CBR编码形式的音频文件可以做到比较精确而对于VBR编码形式的会存在较大的误差（关于CBR和VBR，请看本系列的<a href="blog/2014/07/07/audio-in-ios/">第一篇</a>），具体讲到duration和seek时会再进行说明。</p>

<hr />

<h1>AudioFileStream介绍</h1>

<p>在<a href="blog/2014/07/07/audio-in-ios/">第一篇</a>中说到<code>AudioFileStreamer</code>时提到它的作用是用来读取采样率、码率、时长等基本信息以及分离音频帧。那么在<a href="https://developer.apple.com/library/ios/documentation/audiovideo/conceptual/multimediapg/usingaudio/usingaudio.html#//apple_ref/doc/uid/TP40009767-CH2-SW28">官方文档</a>中Apple是这样描述的：</p>

<p><code>To play streamed audio content, such as from a network connection, use Audio File Stream Services in concert with Audio Queue Services. Audio File Stream Services parses audio packets and metadata from common audio file container formats in a network bitstream. You can also use it to parse packets and metadata from on-disk files</code></p>

<p>根据Apple的描述<code>AudioFileStreamer</code>用在流播放中，当然不仅限于网络流，本地文件同样可以用它来读取信息和分离音频帧。<code>AudioFileStreamer</code>的主要数据是文件数据而不是文件路径，所以数据的读取需要使用者自行实现，</p>

<p>支持的文件格式有：</p>

<ul>
<li>MPEG-1 Audio Layer 3, used for .mp3 files</li>
<li>MPEG-2 ADTS, used for the .aac audio data format</li>
<li>AIFC</li>
<li>AIFF</li>
<li>CAF</li>
<li>MPEG-4, used for .m4a, .mp4, and .3gp files</li>
<li>NeXT</li>
<li>WAVE</li>
</ul>


<p>上述格式是iOS、MacOSX所支持的音频格式，这类格式可以被系统提供的API解码，如果想要解码其他的音频格式（如OGG、APE、FLAC）就需要自己实现解码器了。</p>

<hr />

<h1>初始化AudioFileStream</h1>

<p>第一步，自然是要生成一个<code>AudioFileStream</code>的实例：
```objc
extern OSStatus AudioFileStreamOpen (void * inClientData,</p>

<pre><code>                                   AudioFileStream_PropertyListenerProc inPropertyListenerProc,
                                   AudioFileStream_PacketsProc inPacketsProc,
                                   AudioFileTypeID inFileTypeHint,
                                   AudioFileStreamID * outAudioFileStream);
</code></pre>

<p>```
第一个参数和之前的AudioSession的初始化方法一样是一个上下文对象；</p>

<p>第二个参数<code>AudioFileStream_PropertyListenerProc</code>是歌曲信息解析的回调，每解析出一个歌曲信息都会进行一次回调；</p>

<p>第三个参数<code>AudioFileStream_PacketsProc</code>是分离帧的回调，每解析出一部分帧就会进行一次回调；</p>

<p>第四个参数<code>AudioFileTypeID</code>是文件类型的提示，这个参数来帮助<code>AudioFileStream</code>对文件格式进行解析。这个参数在文件信息不完整（例如信息有缺陷）时尤其有用，它可以给与<code>AudioFileStream</code>一定的提示，帮助其绕过文件中的错误或者缺失从而成功解析文件。所以在确定文件类型的情况下建议各位还是填上这个参数，如果无法确定可以传入0（原理上应该和<a href="/blog/2014/05/04/secret-of-avaudioplayer/">这篇博文</a>近似）；</p>

<p>```objc
//AudioFileTypeID枚举
enum {</p>

<pre><code>    kAudioFileAIFFType              = 'AIFF',
    kAudioFileAIFCType              = 'AIFC',
    kAudioFileWAVEType              = 'WAVE',
    kAudioFileSoundDesigner2Type    = 'Sd2f',
    kAudioFileNextType              = 'NeXT',
    kAudioFileMP3Type               = 'MPG3',   // mpeg layer 3
    kAudioFileMP2Type               = 'MPG2',   // mpeg layer 2
    kAudioFileMP1Type               = 'MPG1',   // mpeg layer 1
      kAudioFileAC3Type             = 'ac-3',
    kAudioFileAAC_ADTSType          = 'adts',
    kAudioFileMPEG4Type            = 'mp4f',
    kAudioFileM4AType              = 'm4af',
    kAudioFileM4BType              = 'm4bf',
      kAudioFileCAFType             = 'caff',
      kAudioFile3GPType             = '3gpp',
      kAudioFile3GP2Type                = '3gp2',       
      kAudioFileAMRType             = 'amrf'        
</code></pre>

<p>};
```</p>

<p>第五个参数是返回的AudioFileStream实例对应的<code>AudioFileStreamID</code>，这个ID需要保存起来作为后续一些方法的参数使用；</p>

<p>返回值用来判断是否成功初始化（OSSStatus == noErr）。</p>

<hr />

<h1>解析数据</h1>

<p>在初始化完成之后，只要拿到文件数据就可以进行解析了。解析时调用方法：</p>

<p>```objc
extern OSStatus AudioFileStreamParseBytes(AudioFileStreamID inAudioFileStream,</p>

<pre><code>                                        UInt32 inDataByteSize,
                                        const void* inData,
                                        UInt32 inFlags);
</code></pre>

<p><code>``
第一个参数</code>AudioFileStreamID`，即初始化时返回的ID；</p>

<p>第二个参数inDataByteSize，本次解析的数据长度；</p>

<p>第三个参数inData，本次解析的数据；</p>

<p>第四个参数是说本次的解析和上一次解析是否是连续的关系，如果是连续的传入0，否则传入<code>kAudioFileStreamParseFlag_Discontinuity</code>。</p>

<p>这里需要插入解释一下何谓“连续”。在第一篇中我们提到过形如MP3的数据都以帧的形式存在的，解析时也需要以帧为单位解析。但在解码之前我们不可能知道每个帧的边界在第几个字节，所以就会出现这样的情况：我们传给AudioFileStreamParseBytes的数据在解析完成之后会有一部分数据余下来，这部分数据是接下去那一帧的前半部分，如果再次有数据输入需要继续解析时就必须要用到前一次解析余下来的数据才能保证帧数据完整，所以在正常播放的情况下传入0即可。目前知道的需要传入<code>kAudioFileStreamParseFlag_Discontinuity</code>的情况有两个，一个是在<strong>seek完毕之后</strong>显然seek后的数据和之前的数据完全无关；另一个是开源播放器<a href="https://github.com/mattgallagher/AudioStreamer">AudioStreamer</a>的作者@Matt Gallagher曾在自己的<a href="http://www.cocoawithlove.com/2008/09/streaming-and-playing-live-mp3-stream.html">blog</a>中提到过的：</p>

<p><code>the Audio File Stream Services hit me with a nasty bug: AudioFileStreamParseBytes will crash when trying to parse a streaming MP3.</code></p>

<p><code>In this case, if we pass the kAudioFileStreamParseFlag_Discontinuity flag to AudioFileStreamParseBytes on every invocation between receiving kAudioFileStreamProperty_ReadyToProducePackets and the first successful call to MyPacketsProc, then AudioFileStreamParseBytes will be extra cautious in its approach and won't crash.</code></p>

<p>Matt发布这篇blog是在2008年，这个Bug年代相当久远了，而且原因未知，究竟是否修复也不得而知，而且由于环境不同（比如测试用的mp3文件和所处的iOS系统）无法重现这个问题，所以我个人觉得还是按照Matt的work around在回调得到<code>kAudioFileStreamProperty_ReadyToProducePackets</code>之后，在正常解析第一帧之前都传入<code>kAudioFileStreamParseFlag_Discontinuity</code>比较好。</p>

<p>回到之前的内容，<code>AudioFileStreamParseBytes</code>方法的返回值表示当前的数据是否被正常解析，如果OSStatus的值不是noErr则表示解析不成功，其中错误码包括：</p>

<p>```objc
enum
{</p>

<pre><code>kAudioFileStreamError_UnsupportedFileType       = 'typ?',
kAudioFileStreamError_UnsupportedDataFormat     = 'fmt?',
kAudioFileStreamError_UnsupportedProperty       = 'pty?',
kAudioFileStreamError_BadPropertySize           = '!siz',
kAudioFileStreamError_NotOptimized              = 'optm',
kAudioFileStreamError_InvalidPacketOffset       = 'pck?',
kAudioFileStreamError_InvalidFile               = 'dta?',
kAudioFileStreamError_ValueUnknown              = 'unk?',
kAudioFileStreamError_DataUnavailable           = 'more',
kAudioFileStreamError_IllegalOperation          = 'nope',
kAudioFileStreamError_UnspecifiedError          = 'wht?',
kAudioFileStreamError_DiscontinuityCantRecover  = 'dsc!'
</code></pre>

<p>};</p>

<p><code>``
大多数都可以从字面上理解，需要提一下的是</code>kAudioFileStreamError_NotOptimized`，文档上是这么说的：</p>

<p><code>It is not possible to produce output packets because the file's packet table or other defining info is either not present or is after the audio data.</code></p>

<p>它的含义是说这个音频文件的文件头不存在或者说文件头可能在文件的末尾，当前无法正常Parse，换句话说就是这个文件需要全部下载完才能播放，无法流播。</p>

<p><strong>注意<code>AudioFileStreamParseBytes</code>方法每一次调用都应该注意返回值，一旦出现错误就可以不必继续Parse了。</strong></p>

<hr />

<h1>解析文件格式信息</h1>

<p>在调用<code>AudioFileStreamParseBytes</code>方法进行解析时会首先读取格式信息，并同步的进入<code>AudioFileStream_PropertyListenerProc</code>回调方法</p>

<p><img src="/images/iOS-audio/audiofilestreamParse-1.jpg" alt="" /></p>

<p>来看一下这个回调方法的定义
```objc
typedef void (*AudioFileStream_PropertyListenerProc)(void * inClientData,</p>

<pre><code>                                                   AudioFileStreamID inAudioFileStream,
                                                   AudioFileStreamPropertyID inPropertyID,
                                                   UInt32 * ioFlags);
</code></pre>

<p>```</p>

<p>回调的第一个参数是Open方法中的上下文对象；</p>

<p>第二个参数inAudioFileStream是和Open方法中第四个返回参数<code>AudioFileStreamID</code>一样，表示当前FileStream的ID；</p>

<p>第三个参数是此次回调解析的信息ID。表示当前PropertyID对应的信息已经解析完成信息（例如数据格式、音频数据的偏移量等等），使用者可以通过<code>AudioFileStreamGetProperty</code>接口获取PropertyID对应的值或者数据结构；</p>

<p>```objc
extern OSStatus AudioFileStreamGetProperty(AudioFileStreamID inAudioFileStream,</p>

<pre><code>                                         AudioFileStreamPropertyID inPropertyID,
                                         UInt32 * ioPropertyDataSize,
                                         void * outPropertyData);
</code></pre>

<p>```</p>

<p>第四个参数ioFlags是一个返回参数，表示这个property是否需要被缓存，如果需要赋值<code>kAudioFileStreamPropertyFlag_PropertyIsCached</code>否则不赋值（这个参数我也不知道应该在啥场景下使用。。一直都没去理他）；</p>

<p>这个回调会进来多次，但并不是每一次都需要进行处理，可以根据需求处理需要的PropertyID进行处理（PropertyID列表如下）。
```objc
//AudioFileStreamProperty枚举
enum
{</p>

<pre><code>kAudioFileStreamProperty_ReadyToProducePackets          =   'redy',
kAudioFileStreamProperty_FileFormat                     =   'ffmt',
kAudioFileStreamProperty_DataFormat                     =   'dfmt',
kAudioFileStreamProperty_FormatList                     =   'flst',
kAudioFileStreamProperty_MagicCookieData                    =   'mgic',
kAudioFileStreamProperty_AudioDataByteCount             =   'bcnt',
kAudioFileStreamProperty_AudioDataPacketCount           =   'pcnt',
kAudioFileStreamProperty_MaximumPacketSize              =   'psze',
kAudioFileStreamProperty_DataOffset                     =   'doff',
kAudioFileStreamProperty_ChannelLayout                  =   'cmap',
kAudioFileStreamProperty_PacketToFrame                  =   'pkfr',
kAudioFileStreamProperty_FrameToPacket                  =   'frpk',
kAudioFileStreamProperty_PacketToByte                   =   'pkby',
kAudioFileStreamProperty_ByteToPacket                   =   'bypk',
kAudioFileStreamProperty_PacketTableInfo                    =   'pnfo',
kAudioFileStreamProperty_PacketSizeUpperBound           =   'pkub',
kAudioFileStreamProperty_AverageBytesPerPacket          =   'abpp',
kAudioFileStreamProperty_BitRate                            =   'brat',
</code></pre>

<p>  kAudioFileStreamProperty_InfoDictionary                  =    &lsquo;info&rsquo;
};
```</p>

<p>这里列几个我认为比较重要的PropertyID：</p>

<p>1、<code>kAudioFileStreamProperty_BitRate</code>：</p>

<p>表示音频数据的码率，获取这个Property是为了计算音频的总时长Duration（因为AudioFileStream没有这样的接口。。）。</p>

<p>```objc
UInt32 bitRate;
UInt32 bitRateSize = sizeof(bitRate);
OSStatus status = AudioFileStreamGetProperty(inAudioFileStream, kAudioFileStreamProperty_BitRate, &amp;bitRateSize, &amp;bitRate);
if (status != noErr)
{</p>

<pre><code>//错误处理
</code></pre>

<p>}
```</p>

<p>2、<code>kAudioFileStreamProperty_DataOffset</code>：</p>

<p>表示音频数据在整个音频文件中的offset（因为大多数音频文件都会有一个文件头之后才使真正的音频数据），这个值在seek时会发挥比较大的作用，音频的seek并不是直接seek文件位置而seek时间（比如seek到2分10秒的位置），seek时会根据时间计算出音频数据的字节offset然后需要再加上音频数据的offset才能得到在文件中的真正offset。
```objc
SInt64 dataOffset;
UInt32 offsetSize = sizeof(dataOffset);
OSStatus status = AudioFileStreamGetProperty(inAudioFileStream, kAudioFileStreamProperty_DataOffset, &amp;offsetSize, &amp;dataOffset);
if (status != noErr)
{</p>

<pre><code>//错误处理
</code></pre>

<p>}
```</p>

<p>3、<code>kAudioFileStreamProperty_DataFormat</code></p>

<p>表示音频文件结构信息，是一个AudioStreamBasicDescription的结构</p>

<p>```objc
struct AudioStreamBasicDescription
{</p>

<pre><code>Float64 mSampleRate;
UInt32  mFormatID;
UInt32  mFormatFlags;
UInt32  mBytesPerPacket;
UInt32  mFramesPerPacket;
UInt32  mBytesPerFrame;
UInt32  mChannelsPerFrame;
UInt32  mBitsPerChannel;
UInt32  mReserved;
</code></pre>

<p>};</p>

<p>AudioStreamBasicDescription asbd;
UInt32 asbdSize = sizeof(asbd);
OSStatus status = AudioFileStreamGetProperty(inAudioFileStream, kAudioFileStreamProperty_DataFormat, &amp;asbdSize, &amp;asbd);
if (status != noErr)
{</p>

<pre><code>//错误处理
</code></pre>

<p>} <br/>
```</p>

<p>4、<code>kAudioFileStreamProperty_FormatList</code></p>

<p>作用和<code>kAudioFileStreamProperty_DataFormat</code>是一样的，区别在于用这个PropertyID获取到是一个AudioStreamBasicDescription的数组，这个参数是用来支持AAC SBR这样的包含多个文件类型的音频格式。由于到底有多少个format我们并不知晓，所以需要先获取一下总数据大小：</p>

<p>```objc
//获取数据大小
Boolean outWriteable;
UInt32 formatListSize;
OSStatus status = AudioFileStreamGetPropertyInfo(inAudioFileStream, kAudioFileStreamProperty_FormatList, &amp;formatListSize, &amp;outWriteable);
if (status != noErr)
{</p>

<pre><code>//错误处理
</code></pre>

<p>}</p>

<p>//获取formatlist
AudioFormatListItem *formatList = malloc(formatListSize);
OSStatus status = AudioFileStreamGetProperty(inAudioFileStream, kAudioFileStreamProperty_FormatList, &amp;formatListSize, formatList);
if (status != noErr)
{</p>

<pre><code>//错误处理
</code></pre>

<p>}</p>

<p>//选择需要的格式
for (int i = 0; i * sizeof(AudioFormatListItem) &lt; formatListSize; i += sizeof(AudioFormatListItem))
{</p>

<pre><code>AudioStreamBasicDescription pasbd = formatList[i].mASBD;
//选择需要的格式。。                             
</code></pre>

<p>}
free(formatList);
```</p>

<p>5、<code>kAudioFileStreamProperty_AudioDataByteCount</code></p>

<p>顾名思义，音频文件中音频数据的总量。这个Property的作用一是用来计算音频的总时长，二是可以在seek时用来计算时间对应的字节offset。</p>

<p>```objc
UInt64 audioDataByteCount;
UInt32 byteCountSize = sizeof(audioDataByteCount);
OSStatus status = AudioFileStreamGetProperty(inAudioFileStream, kAudioFileStreamProperty_AudioDataByteCount, &amp;byteCountSize, &amp;audioDataByteCount);
if (status != noErr)
{</p>

<pre><code>//错误处理
</code></pre>

<p>}
```</p>

<p>5、<code>kAudioFileStreamProperty_ReadyToProducePackets</code></p>

<p>这个PropertyID可以不必获取对应的值，一旦回调中这个PropertyID出现就代表解析完成，接下来可以对音频数据进行帧分离了。</p>

<hr />

<h1>计算时长Duration</h1>

<p>获取时长的最佳方法是从ID3信息中去读取，那样是最准确的。如果ID3信息中没有存，那就依赖于文件头中的信息去计算了。</p>

<p>计算duration的公式如下：</p>

<p><code>
double duration = (audioDataByteCount * 8) / bitRate
</code></p>

<p>音频数据的字节总量audioDataByteCount可以通过<code>kAudioFileStreamProperty_AudioDataByteCount</code>获取，码率bitRate可以通过<code>kAudioFileStreamProperty_BitRate</code>获取也可以通过Parse一部分数据后计算平均码率来得到。</p>

<p>对于CBR数据来说用这样的计算方法的duration会比较准确，对于VBR数据就不好说了。所以对于VBR数据来说，最好是能够从ID3信息中获取到duration，获取不到再想办法通过计算平均码率的途径来计算duration。</p>

<hr />

<h1>分离音频帧</h1>

<p>读取格式信息完成之后继续调用<code>AudioFileStreamParseBytes</code>方法可以对帧进行分离，并同步的进入<code>AudioFileStream_PacketsProc</code>回调方法。</p>

<p><img src="/images/iOS-audio/audiofilestreamParse-2.jpg" alt="" /></p>

<p>回调的定义：</p>

<p>```objc
typedef void (*AudioFileStream_PacketsProc)(void * inClientData,</p>

<pre><code>                                          UInt32 inNumberBytes,
                                          UInt32 inNumberPackets,
                                          const void * inInputData,
                                          AudioStreamPacketDescription * inPacketDescriptions);
</code></pre>

<p>```
第一个参数，一如既往的上下文对象；</p>

<p>第二个参数，本次处理的数据大小；</p>

<p>第三个参数，本次总共处理了多少帧（即代码里的Packet）；</p>

<p>第四个参数，本次处理的所有数据；</p>

<p>第五个参数，<code>AudioStreamPacketDescription</code>数组，存储了每一帧数据是从第几个字节开始的，这一帧总共多少字节。</p>

<p>```objc
//AudioStreamPacketDescription结构
//这里的mVariableFramesInPacket是指实际的数据帧只有VBR的数据才能用到（像MP3这样的压缩数据一个帧里会有好几个数据帧）
struct  AudioStreamPacketDescription
{</p>

<pre><code>SInt64  mStartOffset;
UInt32  mVariableFramesInPacket;
UInt32  mDataByteSize;
</code></pre>

<p>};
```</p>

<p>下面是我按照自己的理解实现的回调方法片段：</p>

<p>```
static void MyAudioFileStreamPacketsCallBack(void *inClientData,</p>

<pre><code>                                         UInt32 inNumberBytes,
                                         UInt32 inNumberPackets,
                                         const void *inInputData,
                                         AudioStreamPacketDescription   *inPacketDescriptions)
</code></pre>

<p>{</p>

<pre><code>//处理discontinuous..

if (numberOfBytes == 0 || numberOfPackets == 0)
{
    return;
}

BOOL deletePackDesc = NO;
if (packetDescriptioins == NULL)
{
    //如果packetDescriptioins不存在，就按照CBR处理，平均每一帧的数据后生成packetDescriptioins
    deletePackDesc = YES;
    UInt32 packetSize = numberOfBytes / numberOfPackets;
    packetDescriptioins = (AudioStreamPacketDescription *)malloc(sizeof(AudioStreamPacketDescription) * numberOfPackets);

    for (int i = 0; i &lt; numberOfPackets; i++)
    {
        UInt32 packetOffset = packetSize * i;
        descriptions[i].mStartOffset = packetOffset;
        descriptions[i].mVariableFramesInPacket = 0;
        if (i == numberOfPackets - 1)
        {
            packetDescriptioins[i].mDataByteSize = numberOfBytes - packetOffset;
        }
        else
        {
            packetDescriptioins[i].mDataByteSize = packetSize;
        }
    }
}

for (int i = 0; i &lt; numberOfPackets; ++i)
{
    SInt64 packetOffset = packetDescriptioins[i].mStartOffset;
    UInt32 packetSize   = packetDescriptioins[i].mDataByteSize;

    //把解析出来的帧数据放进自己的buffer中
    ...
}

if (deletePackDesc)
{
    free(packetDescriptioins);
}
</code></pre>

<p>}
```
inPacketDescriptions这个字段为空时需要按CBR的数据处理。但其实在解析CBR数据时inPacketDescriptions一般也会有返回，因为即使是CBR数据帧的大小也不是恒定不变的，例如CBR的MP3会在每一帧的数据后放1 byte的填充位，这个填充位也并非时时刻刻存在，所以帧的大小会有1 byte的浮动。（比如采样率44.1KHZ，码率160kbps的CBR MP3文件每一帧的大小在522字节和523字节浮动。所以不能因为有inPacketDescriptions没有返回NULL而判定音频数据就是VBR编码的）。</p>

<hr />

<h1>Seek</h1>

<p>就音频的角度来seek功能描述为“我要拖到xx分xx秒”，而实际操作时我们需要操作的是文件，所以我们需要知道的是“我要拖到xx分xx秒”这个操作对应到文件上是要从第几个字节开始读取音频数据。</p>

<p>对于原始的PCM数据来说每一个PCM帧都是固定长度的，对应的播放时长也是固定的，但一旦转换成压缩后的音频数据就会因为编码形式的不同而不同了。对于CBR而言每个帧中所包含的PCM数据帧是恒定的，所以每一帧对应的播放时长也是恒定的；而VBR则不同，为了保证数据最优并且文件大小最小，VBR的每一帧中所包含的PCM数据帧是不固定的，这就导致在流播放的情况下VBR的数据想要做seek并不容易。这里我们也只讨论CBR下的seek。</p>

<p>CBR数据的seek一般是这样实现的（参考并修改自<a href="http://www.cocoawithlove.com/2010/03/streaming-mp3aac-audio-again.html">matt的blog</a>）：</p>

<p>1、近似地计算应该seek到哪个字节</p>

<p>```objc
double seekToTime = &hellip;; //需要seek到哪个时间，秒为单位
UInt64 audioDataByteCount = &hellip;; //通过kAudioFileStreamProperty_AudioDataByteCount获取的值
SInt64 dataOffset = &hellip;; //通过kAudioFileStreamProperty_DataOffset获取的值
double durtion = &hellip;; //通过公式(AudioDataByteCount * 8) / BitRate计算得到的时长</p>

<p>//近似seekOffset = 数据偏移 + seekToTime对应的近似字节数
SInt64 approximateSeekOffset = dataOffset + (seekToTime / duration) * audioDataByteCount;
```</p>

<p>2、计算seekToTime对应的是第几个帧（Packet）</p>

<p>我们可以利用之前Parse得到的音频格式信息来计算PacketDuration。<em>audioItem.fileFormat.mFramesPerPacket / </em>audioItem.fileFormat.mSampleRate;</p>

<p>```objc
//首先需要计算每个packet对应的时长
AudioStreamBasicDescription asbd = &hellip;; ////通过kAudioFileStreamProperty_DataFormat或者kAudioFileStreamProperty_FormatList获取的值
double packetDuration = asbd.mFramesPerPacket / asbd.mSampleRate</p>

<p>//然后计算packet位置
SInt64 seekToPacket = floor(seekToTime / packetDuration);
```</p>

<p>3、使用<code>AudioFileStreamSeek</code>计算精确的字节偏移和时间</p>

<p><code>AudioFileStreamSeek</code>可以用来寻找某一个帧（Packet）对应的字节偏移（byte offset）：</p>

<ul>
<li>如果找到了就会把ioFlags加上kAudioFileStreamSeekFlag_OffsetIsEstimated，并且给outDataByteOffset赋值，outDataByteOffset就是输入的seekToPacket对应的字节偏移量，我们可以根据outDataByteOffset来计算出精确的seekOffset和seekToTime；</li>
<li>如果没找到那么还是应该用第1步计算出来的approximateSeekOffset来做seek；</li>
</ul>


<p>```
SInt64 seekByteOffset;
UInt32 ioFlags = 0;
SInt64 outDataByteOffset;
OSStatus status = AudioFileStreamSeek(audioFileStreamID, seekToPacket, &amp;outDataByteOffset, &amp;ioFlags);
if (status == noErr &amp;&amp; !(ioFlags &amp; kAudioFileStreamSeekFlag_OffsetIsEstimated))
{</p>

<pre><code>//如果AudioFileStreamSeek方法找到了帧的字节偏移，需要修正一下时间
seekToTime -= ((seekByteOffset - dataOffset) - outDataByteOffset) * 8.0 / bitRate;
seekByteOffset = outDataByteOffset + dataOffset;
</code></pre>

<p>}
else
{</p>

<pre><code>seekByteOffset = approximateSeekOffset;
</code></pre>

<p>}
<code>``
4、按照seekByteOffset读取对应的数据继续使用</code>AudioFileStreamParseByte`进行解析</p>

<p>如果是网络流可以通过设置range头来获取字节，本地文件的话直接seek就好了。调用<code>AudioFileStreamParseByte</code>时注意刚seek完第一次Parse数据需要加参数<code>kAudioFileStreamParseFlag_Discontinuity</code>。</p>

<hr />

<h1>关闭AudioFileStream</h1>

<p><code>AudioFileStream</code>使用完毕后需要调用<code>AudioFileStreamClose</code>进行关闭，没啥特别需要注意的。</p>

<p><code>objc
extern OSStatus AudioFileStreamClose(AudioFileStreamID inAudioFileStream);  
</code></p>

<hr />

<h1>小结</h1>

<p>本篇关于<code>AudioFileStream</code>做了详细介绍，小结一下：</p>

<ul>
<li><p>使用<code>AudioFileStream</code>首先需要调用<code>AudioFileStreamOpen</code>，需要注意的是尽量提供inFileTypeHint参数帮助<code>AudioFileStream</code>解析数据，调用完成后记录<code>AudioFileStreamID</code>；</p></li>
<li><p>当有数据时调用<code>AudioFileStreamParseBytes</code>进行解析，每一次解析都需要注意返回值，返回值一旦出现noErr以外的值就代表Parse出错，其中<code>kAudioFileStreamError_NotOptimized</code>代表该文件缺少头信息或者其头信息在文件尾部不适合流播放；</p></li>
<li><p>使用<code>AudioFileStreamParseBytes</code>需要注意第四个参数在需要合适的时候传入<code>kAudioFileStreamParseFlag_Discontinuity</code>；</p></li>
<li><p>调用<code>AudioFileStreamParseBytes</code>后会首先同步进入<code>AudioFileStream_PropertyListenerProc</code>回调来解析文件格式信息，如果回调得到<code>kAudioFileStreamProperty_ReadyToProducePackets</code>表示解析格式信息完成；</p></li>
<li><p>解析格式信息完成后继续调用<code>AudioFileStreamParseBytes</code>会进入<code>MyAudioFileStreamPacketsCallBack</code>回调来分离音频帧，在回调中应该将分离出来的帧信息保存到自己的buffer中</p></li>
<li><p>seek时需要先近似的计算seekTime对应的seekByteOffset，然后利用<code>AudioFileStreamSeek</code>计算精确的offset，如果能得到精确的offset就修正一下seektime，如果无法得到精确的offset就用之前的近似结果</p></li>
<li><p><code>AudioFileStream</code>使用完毕后需要调用<code>AudioFileStreamClose</code>进行关闭；</p></li>
</ul>


<hr />

<h1>示例代码</h1>

<p><a href="https://github.com/mattgallagher/AudioStreamer">AudioStreamer</a>和<a href="https://github.com/muhku/FreeStreamer">FreeStreamer</a>这两个优秀的开源播放器都用到<code>AudioFileStream</code>大家可以借鉴。我自己也写了一个<a href="https://github.com/msching/MCAudioFileStream">简单的AudioFileStream封装</a>。</p>

<hr />

<h1>下篇预告</h1>

<p>下一篇将讲述如何使用<code>AudioFile</code>。</p>

<hr />

<h1>参考资料</h1>

<p><a href="https://developer.apple.com/library/ios/documentation/audiovideo/conceptual/multimediapg/usingaudio/usingaudio.html#//apple_ref/doc/uid/TP40009767-CH2-SW28">Using Audio</a></p>

<p><a href="http://www.cocoawithlove.com/2008/09/streaming-and-playing-live-mp3-stream.html">Streaming and playing an MP3 stream</a></p>

<p><a href="http://www.cocoawithlove.com/2010/03/streaming-mp3aac-audio-again.html">Streaming MP3/AAC audio again</a></p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[iOS音频播放 (二)：AudioSession]]></title>
    <link href="http://msching.github.io/blog/2014/07/08/audio-in-ios-2/"/>
    <updated>2014-07-08T13:58:27+08:00</updated>
    <id>http://msching.github.io/blog/2014/07/08/audio-in-ios-2</id>
    <content type="html"><![CDATA[<h1>前言</h1>

<p>本篇为《iOS音频播放》系列的第二篇。</p>

<p>在实施<a href="blog/2014/07/07/audio-in-ios/">前一篇</a>中所述的7个步骤步之前还必须面对一个麻烦的问题，AudioSession。</p>

<hr />

<h1>AudioSession简介</h1>

<p>AudioSession这个玩意的主要功能包括以下几点（图片来自<a href="https://developer.apple.com/library/ios/documentation/Audio/Conceptual/AudioSessionProgrammingGuide/Introduction/Introduction.html#//apple_ref/doc/uid/TP40007875">官方文档</a>）：</p>

<ol>
<li>确定你的app如何使用音频（是播放？还是录音？）</li>
<li>为你的app选择合适的输入输出设备（比如输入用的麦克风，输出是耳机、手机功放或者airplay）</li>
<li>协调你的app的音频播放和系统以及其他app行为（例如有电话时需要打断，电话结束时需要恢复，按下静音按钮时是否歌曲也要静音等）</li>
</ol>


<p><img src="/images/iOS-audio/audiosession.jpg" alt="AudioSession" /></p>

<p>AudioSession相关的类有两个：</p>

<ol>
<li><code>AudioToolBox</code>中的<code>AudioSession</code></li>
<li><code>AVFoundation</code>中的<code>AVAudioSession</code></li>
</ol>


<p>其中AudioSession在SDK 7中已经被标注为depracated，而AVAudioSession这个类虽然iOS 3开始就已经存在了，但其中很多方法和变量都是在iOS 6以后甚至是iOS 7才有的。所以各位可以依照以下标准选择：</p>

<ul>
<li>如果最低版本支持iOS 5，可以使用<code>AudioSession</code>，也可以使用<code>AVAudioSession</code>；</li>
<li>如果最低版本支持iOS 6及以上，请使用<code>AVAudioSession</code></li>
</ul>


<p>下面以<code>AudioSession</code>类为例来讲述AudioSession相关功能的使用（很不幸我需要支持iOS 5。。T-T，使用<code>AVAudioSession</code>的同学可以在其头文件中寻找对应的方法使用即可，需要注意的点我会加以说明）.</p>

<p><strong>注意：在使用AVAudioPlayer/AVPlayer时可以不用关心AudioSession的相关问题，Apple已经把AudioSession的处理过程封装了，但音乐打断后的响应还是要做的（比如打断后音乐暂停了UI状态也要变化，这个应该通过KVO就可以搞定了吧。。我没试过瞎猜的>_&lt;）。</strong></p>

<hr />

<h1>初始化AudioSession</h1>

<p>使用<code>AudioSession</code>类首先需要调用初始化方法：</p>

<p>```objc
extern OSStatus AudioSessionInitialize(CFRunLoopRef inRunLoop,</p>

<pre><code>                                     CFStringRef inRunLoopMode, 
                                     AudioSessionInterruptionListener inInterruptionListener,
                                     void *inClientData);
</code></pre>

<p>```</p>

<p>前两个参数一般填<code>NULL</code>表示AudioSession运行在主线程上（但并不代表音频的相关处理运行在主线程上，只是AudioSession），第三个参数需要传入一个一个<code>AudioSessionInterruptionListener</code>类型的方法，作为AudioSession被打断时的回调，第四个参数则是代表打断回调时需要附带的对象（即回到方法中的inClientData，如下所示，可以理解为UIView animation中的context）。</p>

<p><code>objc
typedef void (*AudioSessionInterruptionListener)(void * inClientData, UInt32 inInterruptionState);
</code></p>

<p>这才刚开始，坑就来了。这里会有两个问题：</p>

<p>第一，AudioSessionInitialize可以被多次执行，但<code>AudioSessionInterruptionListener</code>只能被设置一次，这就意味着这个打断回调方法是一个静态方法，一旦初始化成功以后所有的打断都会回调到这个方法，即便下一次再次调用AudioSessionInitialize并且把另一个静态方法作为参数传入，当打断到来时还是会回调到第一次设置的方法上。</p>

<p>这种场景并不少见，例如你的app既需要播放歌曲又需要录音，当然你不可能知道用户会先调用哪个功能，所以你必须在播放和录音的模块中都调用AudioSessionInitialize注册打断方法，但最终打断回调只会作用在先注册的那个模块中，很蛋疼吧。。。所以对于AudioSession的使用最好的方法是生成一个类单独进行管理，统一接收打断回调并发送自定义的打断通知，在需要用到AudioSession的模块中接收通知并做相应的操作。</p>

<p>Apple也察觉到了这一点，所以在AVAudioSession中首先取消了Initialize方法，改为了单例方法<code>sharedInstance</code>。在iOS 5上所有的打断都需要通过设置<code>id&lt;AVAudioSessionDelegate&gt; delegate</code>并实现回调方法来实现，这同样会有上述的问题，所以在iOS 5使用AVAudioSession下仍然需要一个单独管理AudioSession的类存在。在iOS 6以后Apple终于把打断改成了通知的形式。。这下科学了。</p>

<p>第二，AudioSessionInitialize方法的第四个参数inClientData，也就是回调方法的第一个参数。上面已经说了打断回调是一个静态方法，而这个参数的目的是为了能让回调时拿到context（上下文信息），所以这个inClientData需要是一个有足够长生命周期的对象（当然前提是你确实需要用到这个参数），如果这个对象被dealloc了，那么回调时拿到的inClientData会是一个野指针。就这一点来说构造一个单独管理AudioSession的类也是有必要的，因为这个类的生命周期和AudioSession一样长，我们可以把context保存在这个类中。</p>

<hr />

<h1>监听RouteChange事件</h1>

<p>如果想要实现类似于“拔掉耳机就把歌曲暂停”的功能就需要监听RouteChange事件：</p>

<p>```objc
extern OSStatus AudioSessionAddPropertyListener(AudioSessionPropertyID inID,</p>

<pre><code>                                              AudioSessionPropertyListener inProc,
                                              void *inClientData);
</code></pre>

<p>typedef void (*AudioSessionPropertyListener)(void * inClientData,</p>

<pre><code>                                           AudioSessionPropertyID inID,
                                           UInt32 inDataSize,
                                           const void * inData);
</code></pre>

<p>```</p>

<p>调用上述方法，AudioSessionPropertyID参数传<code>kAudioSessionProperty_AudioRouteChange</code>，AudioSessionPropertyListener参数传对应的回调方法。inClientData参数同AudioSessionInitialize方法。</p>

<p>同样作为静态回调方法还是需要统一管理，接到回调时可以把第一个参数inData转换成<code>CFDictionaryRef</code>并从中获取kAudioSession_AudioRouteChangeKey_Reason键值对应的value（应该是一个CFNumberRef），得到这些信息后就可以发送自定义通知给其他模块进行相应操作(例如<code>kAudioSessionRouteChangeReason_OldDeviceUnavailable</code>就可以用来做“拔掉耳机就把歌曲暂停”)。</p>

<p>```objc
//AudioSession的AudioRouteChangeReason枚举
enum {</p>

<pre><code>    kAudioSessionRouteChangeReason_Unknown = 0,
    kAudioSessionRouteChangeReason_NewDeviceAvailable = 1,
    kAudioSessionRouteChangeReason_OldDeviceUnavailable = 2,
    kAudioSessionRouteChangeReason_CategoryChange = 3,
    kAudioSessionRouteChangeReason_Override = 4,
    kAudioSessionRouteChangeReason_WakeFromSleep = 6,
    kAudioSessionRouteChangeReason_NoSuitableRouteForCategory = 7,
    kAudioSessionRouteChangeReason_RouteConfigurationChange = 8
};
</code></pre>

<p>```</p>

<p>```objc
//AVAudioSession的AudioRouteChangeReason枚举
typedef NS_ENUM(NSUInteger, AVAudioSessionRouteChangeReason)
{</p>

<pre><code>AVAudioSessionRouteChangeReasonUnknown = 0,
AVAudioSessionRouteChangeReasonNewDeviceAvailable = 1,
AVAudioSessionRouteChangeReasonOldDeviceUnavailable = 2,
AVAudioSessionRouteChangeReasonCategoryChange = 3,
AVAudioSessionRouteChangeReasonOverride = 4,
AVAudioSessionRouteChangeReasonWakeFromSleep = 6,
AVAudioSessionRouteChangeReasonNoSuitableRouteForCategory = 7,
AVAudioSessionRouteChangeReasonRouteConfigurationChange NS_ENUM_AVAILABLE_IOS(7_0) = 8
</code></pre>

<p>}
```</p>

<p><strong>注意：iOS 5下如果使用了<code>AVAudioSession</code>由于<code>AVAudioSessionDelegate</code>中并没有定义相关的方法，还是需要用这个方法来实现监听。iOS 6下直接监听AVAudioSession的通知就可以了。</strong></p>

<hr />

<p>这里附带两个方法的实现，都是基于<code>AudioSession</code>类的（使用<code>AVAudioSession</code>的同学帮不到你们啦）。</p>

<p>1、判断是否插了耳机：</p>

<p>```objc
+ (BOOL)usingHeadset
{</p>

<h1>if TARGET_IPHONE_SIMULATOR</h1>

<pre><code>return NO;
</code></pre>

<h1>endif</h1>

<pre><code>CFStringRef route;
UInt32 propertySize = sizeof(CFStringRef);
AudioSessionGetProperty(kAudioSessionProperty_AudioRoute, &amp;propertySize, &amp;route);

BOOL hasHeadset = NO;
if((route == NULL) || (CFStringGetLength(route) == 0))
{
    // Silent Mode
}
else
{
    /* Known values of route:
     * "Headset"
     * "Headphone"
     * "Speaker"
     * "SpeakerAndMicrophone"
     * "HeadphonesAndMicrophone"
     * "HeadsetInOut"
     * "ReceiverAndMicrophone"
     * "Lineout"
     */
    NSString* routeStr = (__bridge NSString*)route;
    NSRange headphoneRange = [routeStr rangeOfString : @"Headphone"];
    NSRange headsetRange = [routeStr rangeOfString : @"Headset"];

    if (headphoneRange.location != NSNotFound)
    {
        hasHeadset = YES;
    }
    else if(headsetRange.location != NSNotFound)
    {
        hasHeadset = YES;
    }
}

if (route)
{
    CFRelease(route);
}

return hasHeadset;
</code></pre>

<p>}</p>

<p>```</p>

<p>2、判断是否开了Airplay(来自<a href="http://stackoverflow.com/questions/13044894/get-name-of-airplay-device-using-avplayer">StackOverflow</a>)：</p>

<p>```objc
+ (BOOL)isAirplayActived
{</p>

<pre><code>CFDictionaryRef currentRouteDescriptionDictionary = nil;
UInt32 dataSize = sizeof(currentRouteDescriptionDictionary);
AudioSessionGetProperty(kAudioSessionProperty_AudioRouteDescription, &amp;dataSize, &amp;currentRouteDescriptionDictionary);

BOOL airplayActived = NO;
if (currentRouteDescriptionDictionary)
{
    CFArrayRef outputs = CFDictionaryGetValue(currentRouteDescriptionDictionary, kAudioSession_AudioRouteKey_Outputs);
    if(outputs != NULL &amp;&amp; CFArrayGetCount(outputs) &gt; 0)
    {
        CFDictionaryRef currentOutput = CFArrayGetValueAtIndex(outputs, 0);
        //Get the output type (will show airplay / hdmi etc
        CFStringRef outputType = CFDictionaryGetValue(currentOutput, kAudioSession_AudioRouteKey_Type);

        airplayActived = (CFStringCompare(outputType, kAudioSessionOutputRoute_AirPlay, 0) == kCFCompareEqualTo);
    }
    CFRelease(currentRouteDescriptionDictionary);                
}
return airplayActived;
</code></pre>

<p>}</p>

<p>```</p>

<hr />

<h1>设置类别</h1>

<p>下一步要设置AudioSession的Category，使用<code>AudioSession</code>时调用下面的接口</p>

<p>```objc
extern OSStatus AudioSessionSetProperty(AudioSessionPropertyID inID,</p>

<pre><code>                                      UInt32 inDataSize,
                                      const void *inData);
</code></pre>

<p>```</p>

<p>如果我需要的功能是播放，执行如下代码</p>

<p>```objc
UInt32 sessionCategory = kAudioSessionCategory_MediaPlayback;
AudioSessionSetProperty (kAudioSessionProperty_AudioCategory,</p>

<pre><code>                       sizeof(sessionCategory),
                       &amp;sessionCategory);
</code></pre>

<p>```</p>

<p>使用<code>AVAudioSession</code>时调用下面的接口</p>

<p><code>objc
/* set session category */
- (BOOL)setCategory:(NSString *)category error:(NSError **)outError;
/* set session category with options */
- (BOOL)setCategory:(NSString *)category withOptions: (AVAudioSessionCategoryOptions)options error:(NSError **)outError NS_AVAILABLE_IOS(6_0);
</code></p>

<p>至于Category的类型在<a href="https://developer.apple.com/library/ios/documentation/Audio/Conceptual/AudioSessionProgrammingGuide/AudioSessionBasics/AudioSessionBasics.html#//apple_ref/doc/uid/TP40007875-CH3-SW1">官方文档</a>中都有介绍，我这里也只罗列一下具体就不赘述了，各位在使用时可以依照自己需要的功能设置Category。</p>

<p>```objc
//AudioSession的AudioSessionCategory枚举
enum {</p>

<pre><code>    kAudioSessionCategory_AmbientSound               = 'ambi',
    kAudioSessionCategory_SoloAmbientSound           = 'solo',
    kAudioSessionCategory_MediaPlayback              = 'medi',
    kAudioSessionCategory_RecordAudio                = 'reca',
    kAudioSessionCategory_PlayAndRecord              = 'plar',
    kAudioSessionCategory_AudioProcessing            = 'proc'
};
</code></pre>

<p>```</p>

<p>```objc
//AudioSession的AudioSessionCategory字符串
/<em>  Use this category for background sounds such as rain, car engine noise, etc.<br/>
 Mixes with other music. </em>/
AVF_EXPORT NSString *const AVAudioSessionCategoryAmbient;</p>

<p>/<em>  Use this category for background sounds.  Other music will stop playing. </em>/
AVF_EXPORT NSString *const AVAudioSessionCategorySoloAmbient;</p>

<p>/<em> Use this category for music tracks.</em>/
AVF_EXPORT NSString *const AVAudioSessionCategoryPlayback;</p>

<p>/<em>  Use this category when recording audio. </em>/
AVF_EXPORT NSString *const AVAudioSessionCategoryRecord;</p>

<p>/<em>  Use this category when recording and playing back audio. </em>/
AVF_EXPORT NSString *const AVAudioSessionCategoryPlayAndRecord;</p>

<p>/<em>  Use this category when using a hardware codec or signal processor while
 not playing or recording audio. </em>/
AVF_EXPORT NSString *const AVAudioSessionCategoryAudioProcessing;</p>

<p>```</p>

<hr />

<h1>启用</h1>

<p>有了Category就可以启动AudioSession了，启动方法：</p>

<p>```objc
//AudioSession的启动方法
extern OSStatus AudioSessionSetActive(Boolean active);
extern OSStatus AudioSessionSetActiveWithFlags(Boolean active, UInt32 inFlags);</p>

<p>//AVAudioSession的启动方法
&ndash; (BOOL)setActive:(BOOL)active error:(NSError <strong>)outError;
&ndash; (BOOL)setActive:(BOOL)active withFlags:(NSInteger)flags error:(NSError </strong>)outError NS_DEPRECATED_IOS(4_0, 6_0);
&ndash; (BOOL)setActive:(BOOL)active withOptions:(AVAudioSessionSetActiveOptions)options error:(NSError **)outError NS_AVAILABLE_IOS(6_0);
```
启动方法调用后必须要判断是否启动成功，启动不成功的情况经常存在，例如一个前台的app正在播放，你的app正在后台想要启动AudioSession那就会返回失败。</p>

<p>一般情况下我们在启动和停止AudioSession调用第一个方法就可以了。但如果你正在做一个即时语音通讯app的话（类似于微信、易信）就需要注意在deactive AudioSession的时候需要使用第二个方法，inFlags参数传入<code>kAudioSessionSetActiveFlag_NotifyOthersOnDeactivation</code>（<code>AVAudioSession</code>给options参数传入<code>AVAudioSessionSetActiveOptionNotifyOthersOnDeactivation</code>）。当你的app deactive自己的AudioSession时系统会通知上一个被打断播放app打断结束（就是上面说到的打断回调），如果你的app在deactive时传入了NotifyOthersOnDeactivation参数，那么其他app在接到打断结束回调时会多得到一个参数<code>kAudioSessionInterruptionType_ShouldResume</code>否则就是ShouldNotResume（<code>AVAudioSessionInterruptionOptionShouldResume</code>），根据参数的值可以决定是否继续播放。</p>

<p>大概流程是这样的：</p>

<ol>
<li>一个音乐软件A正在播放；</li>
<li>用户打开你的软件播放对话语音，AudioSession active；</li>
<li>音乐软件A音乐被打断并收到InterruptBegin事件；</li>
<li>对话语音播放结束，AudioSession deactive并且传入NotifyOthersOnDeactivation参数；</li>
<li>音乐软件A收到InterruptEnd事件，查看Resume参数，如果是ShouldResume控制音频继续播放，如果是ShouldNotResume就维持打断状态；</li>
</ol>


<p><a href="https://developer.apple.com/library/ios/documentation/Audio/Conceptual/AudioSessionProgrammingGuide/ConfiguringanAudioSession/ConfiguringanAudioSession.html#//apple_ref/doc/uid/TP40007875-CH2-SW1">官方文档</a>中有一张很形象的图来阐述这个现象：</p>

<p><img src="/images/iOS-audio/audiosession-active.jpg" alt="" /></p>

<p>然而现在某些语音通讯软件和某些音乐软件却无视了<code>NotifyOthersOnDeactivation</code>和<code>ShouldResume</code>的正确用法，导致我们经常接到这样的用户反馈：</p>

<pre><code>你们的app在使用xx语音软件听了一段话后就不会继续播放了，但xx音乐软件可以继续播放啊。
</code></pre>

<p>好吧，上面只是吐槽一下。请无视我吧。</p>

<p><strong>2014.7.14补充，7.19更新：</strong></p>

<p>发现即使之前已经调用过<code>AudioSessionInitialize</code>方法，在某些情况下被打断之后可能出现AudioSession失效的情况，需要再次调用<code>AudioSessionInitialize</code>方法来重新生成AudioSession。否则调用<code>AudioSessionSetActive</code>会返回560557673（其他AudioSession方法也雷同，所有方法调用前必须首先初始化AudioSession），转换成string后为"!ini"即<code>kAudioSessionNotInitialized</code>，这个情况在iOS 5.1.x上比较容易发生，iOS 6.x 和 7.x也偶有发生（<del>具体的原因还不知晓</del>好像和打断时直接调用<code>AudioOutputUnitStop</code>有关，又是个坑啊）。</p>

<p>所以每次在调用<code>AudioSessionSetActive</code>时应该判断一下错误码，如果是上述的错误码需要重新初始化一下AudioSession。</p>

<p>附上OSStatus转成string的方法：</p>

<p>```objc</p>

<h1>import &lt;Endian.h></h1>

<p>NSString * OSStatusToString(OSStatus status)
{</p>

<pre><code>size_t len = sizeof(UInt32);
long addr = (unsigned long)&amp;status;
char cstring[5];

len = (status &gt;&gt; 24) == 0 ? len - 1 : len;
len = (status &gt;&gt; 16) == 0 ? len - 1 : len;
len = (status &gt;&gt;  8) == 0 ? len - 1 : len;
len = (status &gt;&gt;  0) == 0 ? len - 1 : len;

addr += (4 - len);

status = EndianU32_NtoB(status);        // strings are big endian

strncpy(cstring, (char *)addr, len);
cstring[len] = 0;

return [NSString stringWithCString:(char *)cstring encoding:NSMacOSRomanStringEncoding];
</code></pre>

<p>}
```</p>

<hr />

<h1>打断处理</h1>

<p>正常启动AudioSession之后就可以播放音频了，下面要讲的是对于打断的处理。之前我们说到打断的回调在iOS 5下需要统一管理，在收到打断开始和结束时需要发送自定义的通知。</p>

<p>使用<code>AudioSession</code>时打断回调应该首先获取<code>kAudioSessionProperty_InterruptionType</code>，然后发送一个自定义的通知并带上对应的参数。</p>

<p>```objc
static void MyAudioSessionInterruptionListener(void *inClientData, UInt32 inInterruptionState)
{</p>

<pre><code>AudioSessionInterruptionType interruptionType = kAudioSessionInterruptionType_ShouldNotResume;
UInt32 interruptionTypeSize = sizeof(interruptionType);
AudioSessionGetProperty(kAudioSessionProperty_InterruptionType,
                        &amp;interruptionTypeSize,
                        &amp;interruptionType);

NSDictionary *userInfo = @{MyAudioInterruptionStateKey:@(inInterruptionState),
                             MyAudioInterruptionTypeKey:@(interruptionType)};

[[NSNotificationCenter defaultCenter] postNotificationName:MyAudioInterruptionNotification object:nil userInfo:userInfo];
</code></pre>

<p>}
```</p>

<p>收到通知后的处理方法如下（注意ShouldResume参数）：</p>

<p>```objc
&ndash; (void)interruptionNotificationReceived:(NSNotification *)notification
{</p>

<pre><code>UInt32 interruptionState = [notification.userInfo[MyAudioInterruptionStateKey] unsignedIntValue];
AudioSessionInterruptionType interruptionType = [notification.userInfo[MyAudioInterruptionTypeKey] unsignedIntValue];
[self handleAudioSessionInterruptionWithState:interruptionState type:interruptionType];
</code></pre>

<p>}</p>

<ul>
<li>(void)handleAudioSessionInterruptionWithState:(UInt32)interruptionState type:(AudioSessionInterruptionType)interruptionType
{
  if (interruptionState == kAudioSessionBeginInterruption)
  {
      //控制UI，暂停播放
  }
  else if (interruptionState == kAudioSessionEndInterruption)
  {
      if (interruptionType == kAudioSessionInterruptionType_ShouldResume)
      {
          OSStatus status = AudioSessionSetActive(true);
          if (status == noErr)
          {
              //控制UI，继续播放
          }
      }
  }
}
```</li>
</ul>


<hr />

<h1>小结</h1>

<p>关于AudioSession的话题到此结束（码字果然很累。。）。小结一下：</p>

<ul>
<li>如果最低版本支持iOS 5，可以使用<code>AudioSession</code>也可以考虑使用<code>AVAudioSession</code>，需要有一个类统一管理AudioSession的所有回调，在接到回调后发送对应的自定义通知；</li>
<li>如果最低版本支持iOS 6及以上，请使用<code>AVAudioSession</code>，不用统一管理，接AVAudioSession的通知即可；</li>
<li>根据app的应用场景合理选择<code>Category</code>；</li>
<li>在deactive时需要注意app的应用场景来合理的选择是否使用<code>NotifyOthersOnDeactivation</code>参数；</li>
<li>在处理InterruptEnd事件时需要注意<code>ShouldResume</code>的值。</li>
</ul>


<hr />

<h1>示例代码</h1>

<p><a href="https://github.com/msching/MCAudioSession">这里</a>有我自己写的<code>AudioSession</code>的封装，如果各位需要支持iOS 5的话可以使用一下。</p>

<hr />

<h1>下篇预告</h1>

<p><del>下一篇将讲述如何使用<code>AudioFileStreamer</code>分离音频帧，以及如何使用<code>AudioQueue</code>进行播放。</del></p>

<p>下一篇将讲述如何使用<code>AudioFileStreamer</code>提取音频文件格式信息和分离音频帧。</p>

<hr />

<h1>参考资料</h1>

<p><a href="https://developer.apple.com/library/ios/documentation/Audio/Conceptual/AudioSessionProgrammingGuide/Introduction/Introduction.html#//apple_ref/doc/uid/TP40007875">AudioSession</a></p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[iOS音频播放 (一)：概述]]></title>
    <link href="http://msching.github.io/blog/2014/07/07/audio-in-ios/"/>
    <updated>2014-07-07T14:40:42+08:00</updated>
    <id>http://msching.github.io/blog/2014/07/07/audio-in-ios</id>
    <content type="html"><![CDATA[<h1>前言</h1>

<p>从事音乐相关的app开发也已经有一段时日了，在这过程中app的播放器几经修改我也因此对于iOS下的音频播放实现有了一定的研究。写这个系列的博客目的一方面希望能够抛砖引玉，另一方面也是希望能帮助国内其他的iOS开发者和爱好者少走弯路（我自己就遇到了不少的坑=。=）。</p>

<p>本篇为《iOS音频播放》系列的第一篇，主要将对iOS下实现音频播放的方法进行概述。</p>

<hr />

<h1>基础</h1>

<p>先来简单了解一下一些基础的音频知识。</p>

<p>目前我们在计算机上进行音频播放都需要依赖于音频文件，音频文件的生成过程是将声音信息采样、量化和编码产生的数字信号的过程，人耳所能听到的声音，最低的频率是从20Hz起一直到最高频率20KHZ，因此音频文件格式的最大带宽是20KHZ。根据<a href="http://zh.wikipedia.org/wiki/%E5%A5%88%E5%A5%8E%E6%96%AF%E7%89%B9%E9%A2%91%E7%8E%87">奈奎斯特</a>的理论，只有采样频率高于声音信号最高频率的两倍时，才能把数字信号表示的声音还原成为原来的声音，所以音频文件的采样率一般在40~50KHZ，比如最常见的CD音质采样率44.1KHZ。</p>

<p>对声音进行采样、量化过程被称为<a href="http://zh.wikipedia.org/wiki/%E8%84%88%E8%A1%9D%E7%B7%A8%E8%99%9F%E8%AA%BF%E8%AE%8A">脉冲编码调制</a>（Pulse Code Modulation），简称<code>PCM</code>。PCM数据是最原始的音频数据完全无损，所以PCM数据虽然音质优秀但体积庞大，为了解决这个问题先后诞生了一系列的音频格式，这些音频格式运用不同的方法对音频数据进行压缩，其中有无损压缩（ALAC、APE、FLAC）和有损压缩（MP3、AAC、OGG、WMA）两种。</p>

<p>目前最为常用的音频格式是MP3，MP3是一种有损压缩的音频格式，设计这种格式的目的就是为了大幅度的减小音频的数据量，它舍弃PCM音频数据中人类听觉不敏感的部分，从下面的比较图我们可以明显的看到MP3数据相比PCM数据明显矮了一截（图片引自<a href="http://bbs.imp3.net/thread-243641-1-1.html">imp3论坛</a>）。</p>

<p><img src="/images/iOS-audio/pcm.jpg" alt="上图为pcm数据" />
<img src="/images/iOS-audio/mp3.jpg" alt="上图为mp3数据" /></p>

<p>MP3格式中的码率（BitRate）代表了MP3数据的压缩质量，现在常用的码率有128kbit/s、160kbit/s、320kbit/s等等，这个值越高声音质量也就越高。MP3编码方式常用的有两种<a href="http://zh.wikipedia.org/wiki/%E5%9B%BA%E5%AE%9A%E7%A0%81%E7%8E%87">固定码率</a>(Constant bitrate，CBR)和<a href="http://zh.wikipedia.org/wiki/%E5%8F%AF%E5%8F%98%E7%A0%81%E7%8E%87">可变码率</a>(Variable bitrate，VBR)。</p>

<p>MP3格式中的数据通常由两部分组成，一部分为<a href="http://zh.wikipedia.org/zh/ID3">ID3</a>用来存储歌名、演唱者、专辑、音轨数等信息，另一部分为音频数据。音频数据部分以帧(frame)为单位存储，每个音频都有自己的帧头，如图所示就是一个MP3文件帧结构图（图片同样来自互联网）。MP3中的每一个帧都有自己的帧头，其中存储了码率、采样率等解码必须的信息，所以每一个帧都可以独立于文件存在和播放，这个特性加上高压缩比使得MP3文件成为了音频流播放的主流格式。帧头之后存储着音频数据，这些音频数据是若干个PCM数据帧经过压缩算法压缩得到的，对CBR的MP3数据来说每个帧中包含的PCM数据帧是固定的，而VBR是可变的。</p>

<p><img src="/images/iOS-audio/mp3frame.jpg" alt="" /></p>

<hr />

<h1>iOS音频播放概述</h1>

<p>了解了基础概念之后我们就可以列出一个经典的音频播放流程（以MP3为例）：</p>

<ol>
<li>读取MP3文件</li>
<li>解析采样率、码率、时长等信息，分离MP3中的音频帧</li>
<li>对分离出来的音频帧解码得到PCM数据</li>
<li>对PCM数据进行音效处理（均衡器、混响器等，非必须）</li>
<li>把PCM数据解码成音频信号</li>
<li>把音频信号交给硬件播放</li>
<li>重复1-6步直到播放完成</li>
</ol>


<p>在iOS系统中apple对上述的流程进行了封装并提供了不同层次的接口（图片引自<a href="https://developer.apple.com/library/ios/documentation/MusicAudio/Conceptual/CoreAudioOverview/CoreAudioEssentials/CoreAudioEssentials.html#//apple_ref/doc/uid/TP40003577-CH10-SW1">官方文档</a>）。</p>

<p><img src="/images/iOS-audio/api-architectural-layers.png" alt="CoreAudio的接口层次" /></p>

<p>下面对其中的中高层接口进行功能说明：</p>

<ul>
<li>Audio File Services：读写音频数据，可以完成播放流程中的第2步；</li>
<li>Audio File Stream Services：对音频进行解码，可以完成播放流程中的第2步；</li>
<li>Audio Converter services：音频数据转换，可以完成播放流程中的第3步；</li>
<li>Audio Processing Graph Services：音效处理模块，可以完成播放流程中的第4步；</li>
<li>Audio Unit Services：播放音频数据：可以完成播放流程中的第5步、第6步；</li>
<li>Extended Audio File Services：Audio File Services和Audio Converter services的结合体；</li>
<li>AVAudioPlayer/AVPlayer(AVFoundation)：高级接口，可以完成整个音频播放的过程（包括本地文件和网络流播放，第4步除外）；</li>
<li>Audio Queue Services：高级接口，可以进行录音和播放，可以完成播放流程中的第3、5、6步；</li>
<li>OpenAL：用于游戏音频播放，暂不讨论</li>
</ul>


<p>可以看到apple提供的接口类型非常丰富，可以满足各种类别类需求：</p>

<ul>
<li><p>如果你只是想实现音频的播放，没有其他需求AVFoundation会很好的满足你的需求。它的接口使用简单、不用关心其中的细节；</p></li>
<li><p>如果你的app需要对音频进行流播放并且同时存储，那么AudioFileStreamer加AudioQueue能够帮到你，你可以先把音频数据下载到本地，一边下载一边用NSFileHandler等接口读取本地音频文件并交给AudioFileStreamer或者AudioFile解析分离音频帧，分离出来的音频帧可以送给AudioQueue进行解码和播放。如果是本地文件直接读取文件解析即可。（这两个都是比较直接的做法，这类需求也可以用AVFoundation+本地server的方式实现，AVAudioPlayer会把请求发送给本地server，由本地server转发出去，获取数据后在本地server中存储并转送给AVAudioPlayer。另一个比较trick的做法是先把音频下载到文件中，在下载到一定量的数据后把文件路径给AVAudioPlayer播放，当然这种做法在音频seek后就回有问题了。）；</p></li>
<li><p>如果你正在开发一个专业的音乐播放软件，需要对音频施加音效（均衡器、混响器），那么除了数据的读取和解析以外还需要用到AudioConverter来把音频数据转换成PCM数据，再由AudioUnit+AUGraph来进行音效处理和播放（但目前多数带音效的app都是自己开发音效模块来坐PCM数据的处理，这部分功能自行开发在自定义性和扩展性上会比较强一些。PCM数据通过音效器处理完成后就可以使用AudioUnit播放了，当然AudioQueue也支持直接使对PCM数据进行播放。）。下图描述的就是使用AudioFile + AudioConverter + AudioUnit进行音频播放的流程（图片引自<a href="https://developer.apple.com/library/ios/documentation/MusicAudio/Conceptual/CoreAudioOverview/ARoadmaptoCommonTasks/ARoadmaptoCommonTasks.html#//apple_ref/doc/uid/TP40003577-CH6-SW1">官方文档</a>）。</p></li>
</ul>


<p><img src="/images/iOS-audio/audioUnitPlay.jpg" alt="" /></p>

<hr />

<h1>下篇预告</h1>

<p>下一篇将讲述iOS音频播放中必须面对的难（da）题（keng），AudioSession。</p>

<hr />

<h1>参考资料</h1>

<p><a href="http://zh.wikipedia.org/zh/%E9%9F%B3%E9%A2%91%E6%96%87%E4%BB%B6%E6%A0%BC%E5%BC%8F">音频文件格式</a></p>

<p><a href="http://zh.wikipedia.org/wiki/%E8%84%88%E8%A1%9D%E7%B7%A8%E8%99%9F%E8%AA%BF%E8%AE%8A">脉冲编码调制</a></p>

<p><a href="http://zh.wikipedia.org/zh/%E9%87%87%E6%A0%B7%E7%8E%87">采样率</a></p>

<p><a href="http://zh.wikipedia.org/wiki/%E5%A5%88%E5%A5%8E%E6%96%AF%E7%89%B9%E9%A2%91%E7%8E%87">奈奎斯特频率</a></p>

<p><a href="http://zh.wikipedia.org/wiki/MP3">MP3</a></p>

<p><a href="http://zh.wikipedia.org/zh/ID3">ID3</a></p>

<p><a href="https://developer.apple.com/library/ios/documentation/MusicAudio/Conceptual/CoreAudioOverview/CoreAudioEssentials/CoreAudioEssentials.html#//apple_ref/doc/uid/TP40003577-CH10-SW1">Core Audio Essential</a></p>

<p><a href="https://developer.apple.com/library/ios/documentation/MusicAudio/Conceptual/CoreAudioOverview/ARoadmaptoCommonTasks/ARoadmaptoCommonTasks.html#//apple_ref/doc/uid/TP40003577-CH6-SW1">Common Tasks in OS X</a></p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[iOS8beta1下WebCore可能会打断音频播放]]></title>
    <link href="http://msching.github.io/blog/2014/06/26/audio-interrput-by-webcore-in-ios-8-beta-1/"/>
    <updated>2014-06-26T16:09:56+08:00</updated>
    <id>http://msching.github.io/blog/2014/06/26/audio-interrput-by-webcore-in-ios-8-beta-1</id>
    <content type="html"><![CDATA[<h1>问题</h1>

<p>前不久在QA发现一个问题，在iOS8 beta1上使用我们的app播放歌曲时进入某些内嵌的web页面（UIWebview实现）时歌曲会暂停播放，但是界面仍然显示为正在播放状态。把真机连上Xcode6调试后发现在进入部分网页时会再console上打印如下log：</p>

<pre><code>AVAudioSession.mm:623: -[AVAudioSession setActive:withOptions:error:]: Deactivating an audio session that has running I/O. All I/O should be stopped or paused prior to deactivating the audio session.
</code></pre>

<p>bt后堆栈如下：</p>

<p>```</p>

<pre><code>frame #1: 0x299632fe libAVFAudio.dylib`-[AVAudioSession setActive:error:] + 26
frame #2: 0x3551b92e WebCore`WebCore::AudioSession::setActive(bool) + 62
frame #3: 0x35af2674 WebCore`WebCore::MediaSessionManager::updateSessionState() + 100
frame #4: 0x35af03b6 WebCore`WebCore::MediaSessionManager::addSession(WebCore::MediaSession&amp;) + 74
frame #5: 0x35af0002 WebCore`WebCore::MediaSession::MediaSession(WebCore::MediaSessionClient&amp;) + 38
frame #6: 0x35735a20 WebCore`WebCore::HTMLMediaSession::create(WebCore::MediaSessionClient&amp;) + 20
frame #7: 0x35724c68 WebCore`WebCore::HTMLMediaElement::HTMLMediaElement(WebCore::QualifiedName const&amp;, WebCore::Document&amp;, bool) + 976
frame #8: 0x3570ad24 WebCore`WebCore::HTMLAudioElement::create(WebCore::QualifiedName const&amp;, WebCore::Document&amp;, bool) + 36
frame #9: 0x35718184 WebCore`WebCore::audioConstructor(WebCore::QualifiedName const&amp;, WebCore::Document&amp;, WebCore::HTMLFormElement*, bool) + 56
frame #10: 0x3571803a WebCore`WebCore::HTMLElementFactory::createElement(WebCore::QualifiedName const&amp;, WebCore::Document&amp;, WebCore::HTMLFormElement*, bool) + 230
frame #11: 0x3533a26c WebCore`WebCore::HTMLDocument::createElement(WTF::AtomicString const&amp;, int&amp;) + 88
frame #12: 0x3533a1ae WebCore`WebCore::jsDocumentPrototypeFunctionCreateElement(JSC::ExecState*) + 242
frame #13: 0x2c1cc4d4 JavaScriptCore`llint_entry + 21380
</code></pre>

<p>```</p>

<p>发现是WebCore调用了<code>AVAudioSession</code>的setActive方法，并且把active置为了NO。这个过程其实类似于音乐在播放时被其他事件打断（例如电话、siri）一样，audio会被打断，同时会发送<code>kAudioSessionBeginInterruption</code>事件通知app音频播放已经被打断，需要修正播放器和UI状态；打断结束后回发送<code>kAudioSessionEndInterruption</code>事件通知app恢复播放状态。区别在于WebCore的打断并没有任何通知，所以就导致界面上的播放状态为播放中而实际音乐却被打断。</p>

<h1>适配</h1>

<p>接下来就要对这个问题进行适配了：</p>

<ol>
<li>首先，联系了前段组的同事对出现问题的页面进行检查，之后被告知是某个页面的js中调用了一些播放相关的代码导致了这个问题，这些js是之前版本中使用的，现在已经被废弃但没有及时的删除。在删除这些js后，问题自然就消失了。</li>
<li>客户端本身也应该做一些适配来防止下次再有页面出现类似问题，目前我能想到的办法是做一个<code>AVAudioSession</code>的category，method swizzle方法<code>setActive:withOptions:error:</code>在设置active值时发送通知来修改UI的状态。</li>
</ol>

]]></content>
  </entry>
  
</feed>
